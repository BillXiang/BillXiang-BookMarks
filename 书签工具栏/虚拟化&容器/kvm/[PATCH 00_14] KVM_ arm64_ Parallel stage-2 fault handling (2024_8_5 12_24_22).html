<html style><!--
 Page saved with SingleFile 
 url: https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/t/ 
 saved date: Mon Aug 05 2024 12:24:22 GMT+0800 (中国标准时间)
--><meta charset=utf-8><title>[PATCH 00/14] KVM: arm64: Parallel stage-2 fault handling</title><link rel=alternate title="Atom feed" href=https://lore.kernel.org/all/new.atom type=application/atom+xml><style>pre{white-space:pre-wrap}*{font-size:100%;font-family:monospace}</style><style media=screen,print>*{font-size:100%;font-family:monospace;background:#fff;color:#003}pre{white-space:pre-wrap}a:link{color:#00f;text-decoration:none}a:visited{color:#808}*.q{color:#006}*.add{color:#060}*.del{color:#900}*.head{color:#000}*.hunk{color:#960}</style><style media="screen and (prefers-color-scheme:dark)">*{font-size:100%;font-family:monospace;background:#000;color:#ccc}pre{white-space:pre-wrap}a:link{color:#69f;text-decoration:none}a:visited{color:#96f}*.q{color:#09f}*.add{color:#0ff}*.del{color:#f0f}*.head{color:#fff}*.hunk{color:#c93}</style><meta name=referrer content=no-referrer><style>.sf-hidden{display:none!important}</style><link rel=canonical href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/t/><meta http-equiv=content-security-policy content="default-src 'none'; font-src 'self' data:; img-src 'self' data:; style-src 'unsafe-inline'; media-src 'self' data:; script-src 'unsafe-inline' data:; object-src 'self' data:; frame-src 'self' data:;"><style>img[src="data:,"],source[src="data:,"]{display:none!important}</style><body><form action=../../><pre><a href="https://lore.kernel.org/all/?t=20221010040009"><b>All of lore.kernel.org</b></a>
<input name=q type=text value><input type=submit value=search> <a href=https://lore.kernel.org/all/_/text/help/>help</a> / <a href=https://lore.kernel.org/all/_/text/color/>color</a> / <a id=mirror href=https://lore.kernel.org/all/_/text/mirror/>mirror</a> / <a href=https://lore.kernel.org/all/new.atom>Atom feed</a></pre></form><pre><a href=#ef4079d7dc9325e98db218fc0ba0c80e4866a66fb id=mf4079d7dc9325e98db218fc0ba0c80e4866a66fb>*</a> <b>[PATCH 00/14] KVM: arm64: Parallel stage-2 fault handling</b>
<b>@ 2022-08-30 19:41 ` Oliver Upton</b>
  <a href=#rf4079d7dc9325e98db218fc0ba0c80e4866a66fb>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson
  Cc: linux-arm-kernel, kvmarm, kvm, Oliver Upton

Presently KVM only takes a read lock for stage 2 faults if it believes
the fault can be fixed by relaxing permissions on a PTE (write unprotect
for dirty logging). Otherwise, stage 2 faults grab the write lock, which
predictably can pile up all the vCPUs in a sufficiently large VM.

Like the TDP MMU for x86, this series loosens the locking around
manipulations of the stage 2 page tables to allow parallel faults. RCU
and atomics are exploited to safely build/destroy the stage 2 page
tables in light of multiple software observers.

Patches 1-2 are a cleanup to the way we collapse page tables, with the
added benefit of narrowing the window of time a range of memory is
unmapped.

Patches 3-7 are minor cleanups and refactorings to the way KVM reads
PTEs and traverses the stage 2 page tables to make it amenable to
concurrent modification.

Patches 8-9 use RCU to punt page table cleanup out of the vCPU fault
path, which should also improve fault latency a bit.

Patches 10-14 implement the meat of this series, extending the
'break-before-make' sequence with atomics to realize locking on PTEs.
Effectively a cmpxchg() is used to 'break' a PTE, thereby serializing
changes to a given PTE.

Finally, patch 15 flips the switch on all the new code and starts
grabbing the read side of the MMU lock for stage 2 faults.

Applies to 6.0-rc3. Tested with KVM selftests and benchmarked with
dirty_log_perf_test, scaling from 1 to 48 vCPUs with 4GB of memory per
vCPU backed by THP.

  ./dirty_log_perf_test -s anonymous_thp -m 2 -b 4G -v ${NR_VCPUS}

Time to dirty memory:

        +-------+---------+------------------+
        | vCPUs | 6.0-rc3 | 6.0-rc3 + series |
        +-------+---------+------------------+
        |     1 | 0.89s   | 0.92s            |
        |     2 | 1.13s   | 1.18s            |
        |     4 | 2.42s   | 1.25s            |
        |     8 | 5.03s   | 1.36s            |
        |    16 | 8.84s   | 2.09s            |
        |    32 | 19.60s  | 4.47s            |
        |    48 | 31.39s  | 6.22s            |
        +-------+---------+------------------+

It is also worth mentioning that the time to populate memory has
improved:

        +-------+---------+------------------+
        | vCPUs | 6.0-rc3 | 6.0-rc3 + series |
        +-------+---------+------------------+
        |     1 | 0.19s   | 0.18s            |
        |     2 | 0.25s   | 0.21s            |
        |     4 | 0.38s   | 0.32s            |
        |     8 | 0.64s   | 0.40s            |
        |    16 | 1.22s   | 0.54s            |
        |    32 | 2.50s   | 1.03s            |
        |    48 | 3.88s   | 1.52s            |
        +-------+---------+------------------+

RFC: <a href=https://lore.kernel.org/kvmarm/20220415215901.1737897-1-oupton@google.com/>https://lore.kernel.org/kvmarm/20220415215901.1737897-1-oupton@google.com/</a>

RFC -&gt; v1:
 - Factored out page table teardown from kvm_pgtable_stage2_map()
 - Use the RCU callback to tear down a subtree, instead of scheduling a
   callback for every individual table page.
 - Reorganized series to (hopefully) avoid intermediate breakage.
 - Dropped the use of page headers, instead stuffing KVM metadata into
   page::private directly

Oliver Upton (14):
  KVM: arm64: Add a helper to tear down unlinked stage-2 subtrees
  KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make
  KVM: arm64: Directly read owner id field in stage2_pte_is_counted()
  KVM: arm64: Read the PTE once per visit
  KVM: arm64: Split init and set for table PTE
  KVM: arm64: Return next table from map callbacks
  KVM: arm64: Document behavior of pgtable visitor callback
  KVM: arm64: Protect page table traversal with RCU
  KVM: arm64: Free removed stage-2 tables in RCU callback
  KVM: arm64: Atomically update stage 2 leaf attributes in parallel
    walks
  KVM: arm64: Make changes block-&gt;table to leaf PTEs parallel-aware
  KVM: arm64: Make leaf-&gt;leaf PTE changes parallel-aware
  KVM: arm64: Make table-&gt;block changes parallel-aware
  KVM: arm64: Handle stage-2 faults in parallel

 arch/arm64/include/asm/kvm_pgtable.h  |  59 ++++-
 arch/arm64/kvm/hyp/nvhe/mem_protect.c |   7 +-
 arch/arm64/kvm/hyp/nvhe/setup.c       |   4 +-
 arch/arm64/kvm/hyp/pgtable.c          | 360 ++++++++++++++++----------
 arch/arm64/kvm/mmu.c                  |  65 +++--
 5 files changed, 325 insertions(+), 170 deletions(-)


base-commit: b90cb1053190353cc30f0fef0ef1f378ccc063c5
-- 
2.37.2.672.g94769d06f0-goog


<a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb id=ef4079d7dc9325e98db218fc0ba0c80e4866a66fb>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/#R>reply</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#rf4079d7dc9325e98db218fc0ba0c80e4866a66fb>96+ messages in thread</a></pre><ul><li><pre><a href=#ef4079d7dc9325e98db218fc0ba0c80e4866a66fb id=mf4079d7dc9325e98db218fc0ba0c80e4866a66fb>*</a> <b>[PATCH 00/14] KVM: arm64: Parallel stage-2 fault handling</b>
<b>@ 2022-08-30 19:41 ` Oliver Upton</b>
  <a href=#rf4079d7dc9325e98db218fc0ba0c80e4866a66fb>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson
  Cc: linux-arm-kernel, kvmarm, kvm, Oliver Upton

Presently KVM only takes a read lock for stage 2 faults if it believes
the fault can be fixed by relaxing permissions on a PTE (write unprotect
for dirty logging). Otherwise, stage 2 faults grab the write lock, which
predictably can pile up all the vCPUs in a sufficiently large VM.

Like the TDP MMU for x86, this series loosens the locking around
manipulations of the stage 2 page tables to allow parallel faults. RCU
and atomics are exploited to safely build/destroy the stage 2 page
tables in light of multiple software observers.

Patches 1-2 are a cleanup to the way we collapse page tables, with the
added benefit of narrowing the window of time a range of memory is
unmapped.

Patches 3-7 are minor cleanups and refactorings to the way KVM reads
PTEs and traverses the stage 2 page tables to make it amenable to
concurrent modification.

Patches 8-9 use RCU to punt page table cleanup out of the vCPU fault
path, which should also improve fault latency a bit.

Patches 10-14 implement the meat of this series, extending the
'break-before-make' sequence with atomics to realize locking on PTEs.
Effectively a cmpxchg() is used to 'break' a PTE, thereby serializing
changes to a given PTE.

Finally, patch 15 flips the switch on all the new code and starts
grabbing the read side of the MMU lock for stage 2 faults.

Applies to 6.0-rc3. Tested with KVM selftests and benchmarked with
dirty_log_perf_test, scaling from 1 to 48 vCPUs with 4GB of memory per
vCPU backed by THP.

  ./dirty_log_perf_test -s anonymous_thp -m 2 -b 4G -v ${NR_VCPUS}

Time to dirty memory:

        +-------+---------+------------------+
        | vCPUs | 6.0-rc3 | 6.0-rc3 + series |
        +-------+---------+------------------+
        |     1 | 0.89s   | 0.92s            |
        |     2 | 1.13s   | 1.18s            |
        |     4 | 2.42s   | 1.25s            |
        |     8 | 5.03s   | 1.36s            |
        |    16 | 8.84s   | 2.09s            |
        |    32 | 19.60s  | 4.47s            |
        |    48 | 31.39s  | 6.22s            |
        +-------+---------+------------------+

It is also worth mentioning that the time to populate memory has
improved:

        +-------+---------+------------------+
        | vCPUs | 6.0-rc3 | 6.0-rc3 + series |
        +-------+---------+------------------+
        |     1 | 0.19s   | 0.18s            |
        |     2 | 0.25s   | 0.21s            |
        |     4 | 0.38s   | 0.32s            |
        |     8 | 0.64s   | 0.40s            |
        |    16 | 1.22s   | 0.54s            |
        |    32 | 2.50s   | 1.03s            |
        |    48 | 3.88s   | 1.52s            |
        +-------+---------+------------------+

RFC: <a href=https://lore.kernel.org/kvmarm/20220415215901.1737897-1-oupton@google.com/>https://lore.kernel.org/kvmarm/20220415215901.1737897-1-oupton@google.com/</a>

RFC -&gt; v1:
 - Factored out page table teardown from kvm_pgtable_stage2_map()
 - Use the RCU callback to tear down a subtree, instead of scheduling a
   callback for every individual table page.
 - Reorganized series to (hopefully) avoid intermediate breakage.
 - Dropped the use of page headers, instead stuffing KVM metadata into
   page::private directly

Oliver Upton (14):
  KVM: arm64: Add a helper to tear down unlinked stage-2 subtrees
  KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make
  KVM: arm64: Directly read owner id field in stage2_pte_is_counted()
  KVM: arm64: Read the PTE once per visit
  KVM: arm64: Split init and set for table PTE
  KVM: arm64: Return next table from map callbacks
  KVM: arm64: Document behavior of pgtable visitor callback
  KVM: arm64: Protect page table traversal with RCU
  KVM: arm64: Free removed stage-2 tables in RCU callback
  KVM: arm64: Atomically update stage 2 leaf attributes in parallel
    walks
  KVM: arm64: Make changes block-&gt;table to leaf PTEs parallel-aware
  KVM: arm64: Make leaf-&gt;leaf PTE changes parallel-aware
  KVM: arm64: Make table-&gt;block changes parallel-aware
  KVM: arm64: Handle stage-2 faults in parallel

 arch/arm64/include/asm/kvm_pgtable.h  |  59 ++++-
 arch/arm64/kvm/hyp/nvhe/mem_protect.c |   7 +-
 arch/arm64/kvm/hyp/nvhe/setup.c       |   4 +-
 arch/arm64/kvm/hyp/pgtable.c          | 360 ++++++++++++++++----------
 arch/arm64/kvm/mmu.c                  |  65 +++--
 5 files changed, 325 insertions(+), 170 deletions(-)


base-commit: b90cb1053190353cc30f0fef0ef1f378ccc063c5
-- 
2.37.2.672.g94769d06f0-goog


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb id=ef4079d7dc9325e98db218fc0ba0c80e4866a66fb>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/#R>reply</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#rf4079d7dc9325e98db218fc0ba0c80e4866a66fb>96+ messages in thread</a></pre><li><pre><a href=#ef4079d7dc9325e98db218fc0ba0c80e4866a66fb id=mf4079d7dc9325e98db218fc0ba0c80e4866a66fb>*</a> <b>[PATCH 00/14] KVM: arm64: Parallel stage-2 fault handling</b>
<b>@ 2022-08-30 19:41 ` Oliver Upton</b>
  <a href=#rf4079d7dc9325e98db218fc0ba0c80e4866a66fb>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson
  Cc: kvmarm, linux-arm-kernel, kvm

Presently KVM only takes a read lock for stage 2 faults if it believes
the fault can be fixed by relaxing permissions on a PTE (write unprotect
for dirty logging). Otherwise, stage 2 faults grab the write lock, which
predictably can pile up all the vCPUs in a sufficiently large VM.

Like the TDP MMU for x86, this series loosens the locking around
manipulations of the stage 2 page tables to allow parallel faults. RCU
and atomics are exploited to safely build/destroy the stage 2 page
tables in light of multiple software observers.

Patches 1-2 are a cleanup to the way we collapse page tables, with the
added benefit of narrowing the window of time a range of memory is
unmapped.

Patches 3-7 are minor cleanups and refactorings to the way KVM reads
PTEs and traverses the stage 2 page tables to make it amenable to
concurrent modification.

Patches 8-9 use RCU to punt page table cleanup out of the vCPU fault
path, which should also improve fault latency a bit.

Patches 10-14 implement the meat of this series, extending the
'break-before-make' sequence with atomics to realize locking on PTEs.
Effectively a cmpxchg() is used to 'break' a PTE, thereby serializing
changes to a given PTE.

Finally, patch 15 flips the switch on all the new code and starts
grabbing the read side of the MMU lock for stage 2 faults.

Applies to 6.0-rc3. Tested with KVM selftests and benchmarked with
dirty_log_perf_test, scaling from 1 to 48 vCPUs with 4GB of memory per
vCPU backed by THP.

  ./dirty_log_perf_test -s anonymous_thp -m 2 -b 4G -v ${NR_VCPUS}

Time to dirty memory:

        +-------+---------+------------------+
        | vCPUs | 6.0-rc3 | 6.0-rc3 + series |
        +-------+---------+------------------+
        |     1 | 0.89s   | 0.92s            |
        |     2 | 1.13s   | 1.18s            |
        |     4 | 2.42s   | 1.25s            |
        |     8 | 5.03s   | 1.36s            |
        |    16 | 8.84s   | 2.09s            |
        |    32 | 19.60s  | 4.47s            |
        |    48 | 31.39s  | 6.22s            |
        +-------+---------+------------------+

It is also worth mentioning that the time to populate memory has
improved:

        +-------+---------+------------------+
        | vCPUs | 6.0-rc3 | 6.0-rc3 + series |
        +-------+---------+------------------+
        |     1 | 0.19s   | 0.18s            |
        |     2 | 0.25s   | 0.21s            |
        |     4 | 0.38s   | 0.32s            |
        |     8 | 0.64s   | 0.40s            |
        |    16 | 1.22s   | 0.54s            |
        |    32 | 2.50s   | 1.03s            |
        |    48 | 3.88s   | 1.52s            |
        +-------+---------+------------------+

RFC: <a href=https://lore.kernel.org/kvmarm/20220415215901.1737897-1-oupton@google.com/>https://lore.kernel.org/kvmarm/20220415215901.1737897-1-oupton@google.com/</a>

RFC -&gt; v1:
 - Factored out page table teardown from kvm_pgtable_stage2_map()
 - Use the RCU callback to tear down a subtree, instead of scheduling a
   callback for every individual table page.
 - Reorganized series to (hopefully) avoid intermediate breakage.
 - Dropped the use of page headers, instead stuffing KVM metadata into
   page::private directly

Oliver Upton (14):
  KVM: arm64: Add a helper to tear down unlinked stage-2 subtrees
  KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make
  KVM: arm64: Directly read owner id field in stage2_pte_is_counted()
  KVM: arm64: Read the PTE once per visit
  KVM: arm64: Split init and set for table PTE
  KVM: arm64: Return next table from map callbacks
  KVM: arm64: Document behavior of pgtable visitor callback
  KVM: arm64: Protect page table traversal with RCU
  KVM: arm64: Free removed stage-2 tables in RCU callback
  KVM: arm64: Atomically update stage 2 leaf attributes in parallel
    walks
  KVM: arm64: Make changes block-&gt;table to leaf PTEs parallel-aware
  KVM: arm64: Make leaf-&gt;leaf PTE changes parallel-aware
  KVM: arm64: Make table-&gt;block changes parallel-aware
  KVM: arm64: Handle stage-2 faults in parallel

 arch/arm64/include/asm/kvm_pgtable.h  |  59 ++++-
 arch/arm64/kvm/hyp/nvhe/mem_protect.c |   7 +-
 arch/arm64/kvm/hyp/nvhe/setup.c       |   4 +-
 arch/arm64/kvm/hyp/pgtable.c          | 360 ++++++++++++++++----------
 arch/arm64/kvm/mmu.c                  |  65 +++--
 5 files changed, 325 insertions(+), 170 deletions(-)


base-commit: b90cb1053190353cc30f0fef0ef1f378ccc063c5
-- 
2.37.2.672.g94769d06f0-goog

_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb id=ef4079d7dc9325e98db218fc0ba0c80e4866a66fb>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/#R>reply</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-1-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#rf4079d7dc9325e98db218fc0ba0c80e4866a66fb>96+ messages in thread</a></pre><li><pre><a href=#e1892e0cea5df00fa40615e71920dd8d9ef48a4ad id=m1892e0cea5df00fa40615e71920dd8d9ef48a4ad>*</a> <b>[PATCH 01/14] KVM: arm64: Add a helper to tear down unlinked stage-2 subtrees</b>
  2022-08-30 19:41 ` <a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb>Oliver Upton</a>
  (?)
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r1892e0cea5df00fa40615e71920dd8d9ef48a4ad>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-arm-kernel, kvmarm, kvm, linux-kernel

A subsequent change to KVM will move the tear down of an unlinked
stage-2 subtree out of the critical path of the break-before-make
sequence.

Introduce a new helper for tearing down unlinked stage-2 subtrees.
Leverage the existing stage-2 free walkers to do so, with a deep call
into __kvm_pgtable_walk() as the subtree is no longer reachable from the
root.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h href=#Z2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>arch/arm64/include/asm/kvm_pgtable.h</a> | 11 +++++++++++
 <a id=iZ2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a>         | 26 ++++++++++++++++++++++++++
 2 files <a href=#e1892e0cea5df00fa40615e71920dd8d9ef48a4ad>changed</a>, 37 insertions(+)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h id=Z2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>diff</a> --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index 9f339dffbc1a..d71fb92dc913 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -316,6 +316,17 @@ int __kvm_pgtable_stage2_init(struct kvm_pgtable *pgt, struct kvm_s2_mmu *mmu,
</span>  */
 void kvm_pgtable_stage2_destroy(struct kvm_pgtable *pgt);
 
<span class=add>+/**
+ * kvm_pgtable_stage2_free_removed() - Free a removed stage-2 paging structure.
+ * @pgtable:	Unlinked stage-2 paging structure to be freed.
+ * @level:	Level of the stage-2 paging structure to be freed.
+ * @arg:	Page-table structure initialised by kvm_pgtable_stage2_init*()
+ *
+ * The page-table is assumed to be unreachable by any hardware walkers prior to
+ * freeing and therefore no TLB invalidation is performed.
+ */
+void kvm_pgtable_stage2_free_removed(void *pgtable, u32 level, void *arg);
+
</span> /**
  * kvm_pgtable_stage2_map() - Install a mapping in a guest stage-2 page-table.
  * @pgt:	Page-table structure initialised by kvm_pgtable_stage2_init*().
<span class=head><a href=#iZ2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 2cb3867eb7c2..d8127c25424c 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -1233,3 +1233,29 @@ void kvm_pgtable_stage2_destroy(struct kvm_pgtable *pgt)
</span> 	pgt-&gt;mm_ops-&gt;free_pages_exact(pgt-&gt;pgd, pgd_sz);
 	pgt-&gt;pgd = NULL;
 }
<span class=add>+
+void kvm_pgtable_stage2_free_removed(void *pgtable, u32 level, void *arg)
+{
+	struct kvm_pgtable *pgt = (struct kvm_pgtable *)arg;
+	kvm_pte_t *ptep = (kvm_pte_t *)pgtable;
+	struct kvm_pgtable_walker walker = {
+		.cb	= stage2_free_walker,
+		.flags	= KVM_PGTABLE_WALK_LEAF |
+			  KVM_PGTABLE_WALK_TABLE_POST,
+		.arg	= pgt-&gt;mm_ops,
+	};
+	struct kvm_pgtable_walk_data data = {
+		.pgt	= pgt,
+		.walker	= &amp;walker,
+
+		/*
+		 * At this point the IPA really doesn't matter, as the page
+		 * table being traversed has already been removed from the stage
+		 * 2. Set an appropriate range to cover the entire page table.
+		 */
+		.addr	= 0,
+		.end	= kvm_granule_size(level),
+	};
+
+	WARN_ON(__kvm_pgtable_walk(&amp;data, ptep, level));
+}
</span>-- 
2.37.2.672.g94769d06f0-goog


<a href=#m1892e0cea5df00fa40615e71920dd8d9ef48a4ad id=e1892e0cea5df00fa40615e71920dd8d9ef48a4ad>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r1892e0cea5df00fa40615e71920dd8d9ef48a4ad>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e1892e0cea5df00fa40615e71920dd8d9ef48a4ad id=m1892e0cea5df00fa40615e71920dd8d9ef48a4ad>*</a> <b>[PATCH 01/14] KVM: arm64: Add a helper to tear down unlinked stage-2 subtrees</b>
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r1892e0cea5df00fa40615e71920dd8d9ef48a4ad>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-arm-kernel, kvmarm, kvm, linux-kernel

A subsequent change to KVM will move the tear down of an unlinked
stage-2 subtree out of the critical path of the break-before-make
sequence.

Introduce a new helper for tearing down unlinked stage-2 subtrees.
Leverage the existing stage-2 free walkers to do so, with a deep call
into __kvm_pgtable_walk() as the subtree is no longer reachable from the
root.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h href=#Z2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>arch/arm64/include/asm/kvm_pgtable.h</a> | 11 +++++++++++
 <a id=iZ2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a>         | 26 ++++++++++++++++++++++++++
 2 files <a href=#e1892e0cea5df00fa40615e71920dd8d9ef48a4ad>changed</a>, 37 insertions(+)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h id=Z2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>diff</a> --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index 9f339dffbc1a..d71fb92dc913 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -316,6 +316,17 @@ int __kvm_pgtable_stage2_init(struct kvm_pgtable *pgt, struct kvm_s2_mmu *mmu,
</span>  */
 void kvm_pgtable_stage2_destroy(struct kvm_pgtable *pgt);
 
<span class=add>+/**
+ * kvm_pgtable_stage2_free_removed() - Free a removed stage-2 paging structure.
+ * @pgtable:	Unlinked stage-2 paging structure to be freed.
+ * @level:	Level of the stage-2 paging structure to be freed.
+ * @arg:	Page-table structure initialised by kvm_pgtable_stage2_init*()
+ *
+ * The page-table is assumed to be unreachable by any hardware walkers prior to
+ * freeing and therefore no TLB invalidation is performed.
+ */
+void kvm_pgtable_stage2_free_removed(void *pgtable, u32 level, void *arg);
+
</span> /**
  * kvm_pgtable_stage2_map() - Install a mapping in a guest stage-2 page-table.
  * @pgt:	Page-table structure initialised by kvm_pgtable_stage2_init*().
<span class=head><a href=#iZ2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 2cb3867eb7c2..d8127c25424c 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -1233,3 +1233,29 @@ void kvm_pgtable_stage2_destroy(struct kvm_pgtable *pgt)
</span> 	pgt-&gt;mm_ops-&gt;free_pages_exact(pgt-&gt;pgd, pgd_sz);
 	pgt-&gt;pgd = NULL;
 }
<span class=add>+
+void kvm_pgtable_stage2_free_removed(void *pgtable, u32 level, void *arg)
+{
+	struct kvm_pgtable *pgt = (struct kvm_pgtable *)arg;
+	kvm_pte_t *ptep = (kvm_pte_t *)pgtable;
+	struct kvm_pgtable_walker walker = {
+		.cb	= stage2_free_walker,
+		.flags	= KVM_PGTABLE_WALK_LEAF |
+			  KVM_PGTABLE_WALK_TABLE_POST,
+		.arg	= pgt-&gt;mm_ops,
+	};
+	struct kvm_pgtable_walk_data data = {
+		.pgt	= pgt,
+		.walker	= &amp;walker,
+
+		/*
+		 * At this point the IPA really doesn't matter, as the page
+		 * table being traversed has already been removed from the stage
+		 * 2. Set an appropriate range to cover the entire page table.
+		 */
+		.addr	= 0,
+		.end	= kvm_granule_size(level),
+	};
+
+	WARN_ON(__kvm_pgtable_walk(&amp;data, ptep, level));
+}
</span>-- 
2.37.2.672.g94769d06f0-goog


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m1892e0cea5df00fa40615e71920dd8d9ef48a4ad id=e1892e0cea5df00fa40615e71920dd8d9ef48a4ad>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r1892e0cea5df00fa40615e71920dd8d9ef48a4ad>96+ messages in thread</a></pre><li><pre><a href=#e1892e0cea5df00fa40615e71920dd8d9ef48a4ad id=m1892e0cea5df00fa40615e71920dd8d9ef48a4ad>*</a> <b>[PATCH 01/14] KVM: arm64: Add a helper to tear down unlinked stage-2 subtrees</b>
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r1892e0cea5df00fa40615e71920dd8d9ef48a4ad>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-kernel, kvmarm, linux-arm-kernel, kvm

A subsequent change to KVM will move the tear down of an unlinked
stage-2 subtree out of the critical path of the break-before-make
sequence.

Introduce a new helper for tearing down unlinked stage-2 subtrees.
Leverage the existing stage-2 free walkers to do so, with a deep call
into __kvm_pgtable_walk() as the subtree is no longer reachable from the
root.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h href=#Z2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>arch/arm64/include/asm/kvm_pgtable.h</a> | 11 +++++++++++
 <a id=iZ2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a>         | 26 ++++++++++++++++++++++++++
 2 files <a href=#e1892e0cea5df00fa40615e71920dd8d9ef48a4ad>changed</a>, 37 insertions(+)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h id=Z2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>diff</a> --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index 9f339dffbc1a..d71fb92dc913 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -316,6 +316,17 @@ int __kvm_pgtable_stage2_init(struct kvm_pgtable *pgt, struct kvm_s2_mmu *mmu,
</span>  */
 void kvm_pgtable_stage2_destroy(struct kvm_pgtable *pgt);
 
<span class=add>+/**
+ * kvm_pgtable_stage2_free_removed() - Free a removed stage-2 paging structure.
+ * @pgtable:	Unlinked stage-2 paging structure to be freed.
+ * @level:	Level of the stage-2 paging structure to be freed.
+ * @arg:	Page-table structure initialised by kvm_pgtable_stage2_init*()
+ *
+ * The page-table is assumed to be unreachable by any hardware walkers prior to
+ * freeing and therefore no TLB invalidation is performed.
+ */
+void kvm_pgtable_stage2_free_removed(void *pgtable, u32 level, void *arg);
+
</span> /**
  * kvm_pgtable_stage2_map() - Install a mapping in a guest stage-2 page-table.
  * @pgt:	Page-table structure initialised by kvm_pgtable_stage2_init*().
<span class=head><a href=#iZ2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-2-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 2cb3867eb7c2..d8127c25424c 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -1233,3 +1233,29 @@ void kvm_pgtable_stage2_destroy(struct kvm_pgtable *pgt)
</span> 	pgt-&gt;mm_ops-&gt;free_pages_exact(pgt-&gt;pgd, pgd_sz);
 	pgt-&gt;pgd = NULL;
 }
<span class=add>+
+void kvm_pgtable_stage2_free_removed(void *pgtable, u32 level, void *arg)
+{
+	struct kvm_pgtable *pgt = (struct kvm_pgtable *)arg;
+	kvm_pte_t *ptep = (kvm_pte_t *)pgtable;
+	struct kvm_pgtable_walker walker = {
+		.cb	= stage2_free_walker,
+		.flags	= KVM_PGTABLE_WALK_LEAF |
+			  KVM_PGTABLE_WALK_TABLE_POST,
+		.arg	= pgt-&gt;mm_ops,
+	};
+	struct kvm_pgtable_walk_data data = {
+		.pgt	= pgt,
+		.walker	= &amp;walker,
+
+		/*
+		 * At this point the IPA really doesn't matter, as the page
+		 * table being traversed has already been removed from the stage
+		 * 2. Set an appropriate range to cover the entire page table.
+		 */
+		.addr	= 0,
+		.end	= kvm_granule_size(level),
+	};
+
+	WARN_ON(__kvm_pgtable_walk(&amp;data, ptep, level));
+}
</span>-- 
2.37.2.672.g94769d06f0-goog

_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m1892e0cea5df00fa40615e71920dd8d9ef48a4ad id=e1892e0cea5df00fa40615e71920dd8d9ef48a4ad>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-2-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r1892e0cea5df00fa40615e71920dd8d9ef48a4ad>96+ messages in thread</a></pre></ul><li><pre><a href=#ebc09837aeeb8bda33d35c82fe34163b1b3d4fc52 id=mbc09837aeeb8bda33d35c82fe34163b1b3d4fc52>*</a> <b>[PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
  2022-08-30 19:41 ` <a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb>Oliver Upton</a>
  (?)
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#rbc09837aeeb8bda33d35c82fe34163b1b3d4fc52>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-arm-kernel, kvmarm, kvm, linux-kernel

The break-before-make sequence is a bit annoying as it opens a window
wherein memory is unmapped from the guest. KVM should replace the PTE
as quickly as possible and avoid unnecessary work in between.

Presently, the stage-2 map walker tears down a removed table before
installing a block mapping when coalescing a table into a block. As the
removed table is no longer visible to hardware walkers after the
DSB+TLBI, it is possible to move the remaining cleanup to happen after
installing the new PTE.

Reshuffle the stage-2 map walker to install the new block entry in
the pre-order callback. Unwire all of the teardown logic and replace
it with a call to kvm_pgtable_stage2_free_removed() after fixing
the PTE. The post-order visitor is now completely unnecessary, so drop
it. Finally, touch up the comments to better represent the now
simplified map walker.

Note that the call to tear down the unlinked stage-2 is indirected
as a subsequent change will use an RCU callback to trigger tear down.
RCU is not available to pKVM, so there is a need to use different
implementations on pKVM and non-pKVM VMs.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h href=#Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>arch/arm64/include/asm/kvm_pgtable.h</a>  |  3 +
 <a id=iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c href=#Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c>arch/arm64/kvm/hyp/nvhe/mem_protect.c</a> |  1 +
 <a id=iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a>          | 83 ++++++++-------------------
 <a id=iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c href=#Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c>arch/arm64/kvm/mmu.c</a>                  |  1 +
 4 files <a href=#ebc09837aeeb8bda33d35c82fe34163b1b3d4fc52>changed</a>, 28 insertions(+), 60 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h id=Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>diff</a> --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index d71fb92dc913..c25633f53b2b 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -77,6 +77,8 @@ static inline bool kvm_level_supports_block_mapping(u32 level)
</span>  *				allocation is physically contiguous.
  * @free_pages_exact:		Free an exact number of memory pages previously
  *				allocated by zalloc_pages_exact.
<span class=add>+ * @free_removed_table:		Free a removed paging structure by unlinking and
+ *				dropping references.
</span>  * @get_page:			Increment the refcount on a page.
  * @put_page:			Decrement the refcount on a page. When the
  *				refcount reaches 0 the page is automatically
<span class=hunk>@@ -95,6 +97,7 @@ struct kvm_pgtable_mm_ops {
</span> 	void*		(*zalloc_page)(void *arg);
 	void*		(*zalloc_pages_exact)(size_t size);
 	void		(*free_pages_exact)(void *addr, size_t size);
<span class=add>+	void		(*free_removed_table)(void *addr, u32 level, void *arg);
</span> 	void		(*get_page)(void *addr);
 	void		(*put_page)(void *addr);
 	int		(*page_count)(void *addr);
<span class=head><a href=#iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c id=Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c>diff</a> --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
index 1e78acf9662e..a930fdee6fce 100644
--- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c
+++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
</span><span class=hunk>@@ -93,6 +93,7 @@ static int prepare_s2_pool(void *pgt_pool_base)
</span> 	host_kvm.mm_ops = (struct kvm_pgtable_mm_ops) {
 		.zalloc_pages_exact = host_s2_zalloc_pages_exact,
 		.zalloc_page = host_s2_zalloc_page,
<span class=add>+		.free_removed_table = kvm_pgtable_stage2_free_removed,
</span> 		.phys_to_virt = hyp_phys_to_virt,
 		.virt_to_phys = hyp_virt_to_phys,
 		.page_count = hyp_page_count,
<span class=head><a href=#iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index d8127c25424c..5c0c8028d71c 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -763,17 +763,21 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
</span> 	return 0;
 }
 
<span class=add>+static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
+				struct stage2_map_data *data);
+
</span> static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
 				     kvm_pte_t *ptep,
 				     struct stage2_map_data *data)
 {
<span class=del>-	if (data-&gt;anchor)
-		return 0;
</span><span class=add>+	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
+	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
+	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
+	int ret;
</span> 
 	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
 		return 0;
 
<span class=del>-	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
</span> 	kvm_clear_pte(ptep);
 
 	/*
<span class=hunk>@@ -782,8 +786,13 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
</span> 	 * individually.
 	 */
 	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
<span class=del>-	data-&gt;anchor = ptep;
-	return 0;
</span><span class=add>+
+	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
+
+	mm_ops-&gt;put_page(ptep);
+	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
+
+	return ret;
</span> }
 
 static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=hunk>@@ -793,13 +802,6 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 	kvm_pte_t *childp, pte = *ptep;
 	int ret;
 
<span class=del>-	if (data-&gt;anchor) {
-		if (stage2_pte_is_counted(pte))
-			mm_ops-&gt;put_page(ptep);
-
-		return 0;
-	}
-
</span> 	ret = stage2_map_walker_try_leaf(addr, end, level, ptep, data);
 	if (ret != -E2BIG)
 		return ret;
<span class=hunk>@@ -828,50 +830,14 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 	return 0;
 }
 
<span class=del>-static int stage2_map_walk_table_post(u64 addr, u64 end, u32 level,
-				      kvm_pte_t *ptep,
-				      struct stage2_map_data *data)
-{
-	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
-	kvm_pte_t *childp;
-	int ret = 0;
-
-	if (!data-&gt;anchor)
-		return 0;
-
-	if (data-&gt;anchor == ptep) {
-		childp = data-&gt;childp;
-		data-&gt;anchor = NULL;
-		data-&gt;childp = NULL;
-		ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
-	} else {
-		childp = kvm_pte_follow(*ptep, mm_ops);
-	}
-
-	mm_ops-&gt;put_page(childp);
-	mm_ops-&gt;put_page(ptep);
-
-	return ret;
-}
-
</span> /*
<span class=del>- * This is a little fiddly, as we use all three of the walk flags. The idea
- * is that the TABLE_PRE callback runs for table entries on the way down,
- * looking for table entries which we could conceivably replace with a
- * block entry for this mapping. If it finds one, then it sets the 'anchor'
- * field in 'struct stage2_map_data' to point at the table entry, before
- * clearing the entry to zero and descending into the now detached table.
- *
- * The behaviour of the LEAF callback then depends on whether or not the
- * anchor has been set. If not, then we're not using a block mapping higher
- * up the table and we perform the mapping at the existing leaves instead.
- * If, on the other hand, the anchor _is_ set, then we drop references to
- * all valid leaves so that the pages beneath the anchor can be freed.
</span><span class=add>+ * The TABLE_PRE callback runs for table entries on the way down, looking
+ * for table entries which we could conceivably replace with a block entry
+ * for this mapping. If it finds one it replaces the entry and calls
+ * kvm_pgtable_mm_ops::free_removed_table() to tear down the detached table.
</span>  *
<span class=del>- * Finally, the TABLE_POST callback does nothing if the anchor has not
- * been set, but otherwise frees the page-table pages while walking back up
- * the page-table, installing the block entry when it revisits the anchor
- * pointer and clearing the anchor to NULL.
</span><span class=add>+ * Otherwise, the LEAF callback performs the mapping at the existing leaves
+ * instead.
</span>  */
 static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
 			     enum kvm_pgtable_walk_flags flag, void * const arg)
<span class=hunk>@@ -883,11 +849,9 @@ static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 		return stage2_map_walk_table_pre(addr, end, level, ptep, data);
 	case KVM_PGTABLE_WALK_LEAF:
 		return stage2_map_walk_leaf(addr, end, level, ptep, data);
<span class=del>-	case KVM_PGTABLE_WALK_TABLE_POST:
-		return stage2_map_walk_table_post(addr, end, level, ptep, data);
</span><span class=add>+	default:
+		return -EINVAL;
</span> 	}
<span class=del>-
-	return -EINVAL;
</span> }
 
 int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
<span class=hunk>@@ -905,8 +869,7 @@ int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
</span> 	struct kvm_pgtable_walker walker = {
 		.cb		= stage2_map_walker,
 		.flags		= KVM_PGTABLE_WALK_TABLE_PRE |
<span class=del>-				  KVM_PGTABLE_WALK_LEAF |
-				  KVM_PGTABLE_WALK_TABLE_POST,
</span><span class=add>+				  KVM_PGTABLE_WALK_LEAF,
</span> 		.arg		= &amp;map_data,
 	};
 
<span class=head><a href=#iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c id=Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c>diff</a> --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index c9a13e487187..91521f4aab97 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
</span><span class=hunk>@@ -627,6 +627,7 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
</span> 	.zalloc_page		= stage2_memcache_zalloc_page,
 	.zalloc_pages_exact	= kvm_host_zalloc_pages_exact,
 	.free_pages_exact	= free_pages_exact,
<span class=add>+	.free_removed_table	= kvm_pgtable_stage2_free_removed,
</span> 	.get_page		= kvm_host_get_page,
 	.put_page		= kvm_host_put_page,
 	.page_count		= kvm_host_page_count,
-- 
2.37.2.672.g94769d06f0-goog


<a href=#mbc09837aeeb8bda33d35c82fe34163b1b3d4fc52 id=ebc09837aeeb8bda33d35c82fe34163b1b3d4fc52>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#rbc09837aeeb8bda33d35c82fe34163b1b3d4fc52>96+ messages in thread</a></pre><li><ul><li><pre><a href=#ebc09837aeeb8bda33d35c82fe34163b1b3d4fc52 id=mbc09837aeeb8bda33d35c82fe34163b1b3d4fc52>*</a> <b>[PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#rbc09837aeeb8bda33d35c82fe34163b1b3d4fc52>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-arm-kernel, kvmarm, kvm, linux-kernel

The break-before-make sequence is a bit annoying as it opens a window
wherein memory is unmapped from the guest. KVM should replace the PTE
as quickly as possible and avoid unnecessary work in between.

Presently, the stage-2 map walker tears down a removed table before
installing a block mapping when coalescing a table into a block. As the
removed table is no longer visible to hardware walkers after the
DSB+TLBI, it is possible to move the remaining cleanup to happen after
installing the new PTE.

Reshuffle the stage-2 map walker to install the new block entry in
the pre-order callback. Unwire all of the teardown logic and replace
it with a call to kvm_pgtable_stage2_free_removed() after fixing
the PTE. The post-order visitor is now completely unnecessary, so drop
it. Finally, touch up the comments to better represent the now
simplified map walker.

Note that the call to tear down the unlinked stage-2 is indirected
as a subsequent change will use an RCU callback to trigger tear down.
RCU is not available to pKVM, so there is a need to use different
implementations on pKVM and non-pKVM VMs.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h href=#Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>arch/arm64/include/asm/kvm_pgtable.h</a>  |  3 +
 <a id=iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c href=#Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c>arch/arm64/kvm/hyp/nvhe/mem_protect.c</a> |  1 +
 <a id=iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a>          | 83 ++++++++-------------------
 <a id=iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c href=#Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c>arch/arm64/kvm/mmu.c</a>                  |  1 +
 4 files <a href=#ebc09837aeeb8bda33d35c82fe34163b1b3d4fc52>changed</a>, 28 insertions(+), 60 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h id=Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>diff</a> --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index d71fb92dc913..c25633f53b2b 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -77,6 +77,8 @@ static inline bool kvm_level_supports_block_mapping(u32 level)
</span>  *				allocation is physically contiguous.
  * @free_pages_exact:		Free an exact number of memory pages previously
  *				allocated by zalloc_pages_exact.
<span class=add>+ * @free_removed_table:		Free a removed paging structure by unlinking and
+ *				dropping references.
</span>  * @get_page:			Increment the refcount on a page.
  * @put_page:			Decrement the refcount on a page. When the
  *				refcount reaches 0 the page is automatically
<span class=hunk>@@ -95,6 +97,7 @@ struct kvm_pgtable_mm_ops {
</span> 	void*		(*zalloc_page)(void *arg);
 	void*		(*zalloc_pages_exact)(size_t size);
 	void		(*free_pages_exact)(void *addr, size_t size);
<span class=add>+	void		(*free_removed_table)(void *addr, u32 level, void *arg);
</span> 	void		(*get_page)(void *addr);
 	void		(*put_page)(void *addr);
 	int		(*page_count)(void *addr);
<span class=head><a href=#iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c id=Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c>diff</a> --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
index 1e78acf9662e..a930fdee6fce 100644
--- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c
+++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
</span><span class=hunk>@@ -93,6 +93,7 @@ static int prepare_s2_pool(void *pgt_pool_base)
</span> 	host_kvm.mm_ops = (struct kvm_pgtable_mm_ops) {
 		.zalloc_pages_exact = host_s2_zalloc_pages_exact,
 		.zalloc_page = host_s2_zalloc_page,
<span class=add>+		.free_removed_table = kvm_pgtable_stage2_free_removed,
</span> 		.phys_to_virt = hyp_phys_to_virt,
 		.virt_to_phys = hyp_virt_to_phys,
 		.page_count = hyp_page_count,
<span class=head><a href=#iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index d8127c25424c..5c0c8028d71c 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -763,17 +763,21 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
</span> 	return 0;
 }
 
<span class=add>+static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
+				struct stage2_map_data *data);
+
</span> static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
 				     kvm_pte_t *ptep,
 				     struct stage2_map_data *data)
 {
<span class=del>-	if (data-&gt;anchor)
-		return 0;
</span><span class=add>+	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
+	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
+	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
+	int ret;
</span> 
 	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
 		return 0;
 
<span class=del>-	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
</span> 	kvm_clear_pte(ptep);
 
 	/*
<span class=hunk>@@ -782,8 +786,13 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
</span> 	 * individually.
 	 */
 	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
<span class=del>-	data-&gt;anchor = ptep;
-	return 0;
</span><span class=add>+
+	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
+
+	mm_ops-&gt;put_page(ptep);
+	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
+
+	return ret;
</span> }
 
 static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=hunk>@@ -793,13 +802,6 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 	kvm_pte_t *childp, pte = *ptep;
 	int ret;
 
<span class=del>-	if (data-&gt;anchor) {
-		if (stage2_pte_is_counted(pte))
-			mm_ops-&gt;put_page(ptep);
-
-		return 0;
-	}
-
</span> 	ret = stage2_map_walker_try_leaf(addr, end, level, ptep, data);
 	if (ret != -E2BIG)
 		return ret;
<span class=hunk>@@ -828,50 +830,14 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 	return 0;
 }
 
<span class=del>-static int stage2_map_walk_table_post(u64 addr, u64 end, u32 level,
-				      kvm_pte_t *ptep,
-				      struct stage2_map_data *data)
-{
-	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
-	kvm_pte_t *childp;
-	int ret = 0;
-
-	if (!data-&gt;anchor)
-		return 0;
-
-	if (data-&gt;anchor == ptep) {
-		childp = data-&gt;childp;
-		data-&gt;anchor = NULL;
-		data-&gt;childp = NULL;
-		ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
-	} else {
-		childp = kvm_pte_follow(*ptep, mm_ops);
-	}
-
-	mm_ops-&gt;put_page(childp);
-	mm_ops-&gt;put_page(ptep);
-
-	return ret;
-}
-
</span> /*
<span class=del>- * This is a little fiddly, as we use all three of the walk flags. The idea
- * is that the TABLE_PRE callback runs for table entries on the way down,
- * looking for table entries which we could conceivably replace with a
- * block entry for this mapping. If it finds one, then it sets the 'anchor'
- * field in 'struct stage2_map_data' to point at the table entry, before
- * clearing the entry to zero and descending into the now detached table.
- *
- * The behaviour of the LEAF callback then depends on whether or not the
- * anchor has been set. If not, then we're not using a block mapping higher
- * up the table and we perform the mapping at the existing leaves instead.
- * If, on the other hand, the anchor _is_ set, then we drop references to
- * all valid leaves so that the pages beneath the anchor can be freed.
</span><span class=add>+ * The TABLE_PRE callback runs for table entries on the way down, looking
+ * for table entries which we could conceivably replace with a block entry
+ * for this mapping. If it finds one it replaces the entry and calls
+ * kvm_pgtable_mm_ops::free_removed_table() to tear down the detached table.
</span>  *
<span class=del>- * Finally, the TABLE_POST callback does nothing if the anchor has not
- * been set, but otherwise frees the page-table pages while walking back up
- * the page-table, installing the block entry when it revisits the anchor
- * pointer and clearing the anchor to NULL.
</span><span class=add>+ * Otherwise, the LEAF callback performs the mapping at the existing leaves
+ * instead.
</span>  */
 static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
 			     enum kvm_pgtable_walk_flags flag, void * const arg)
<span class=hunk>@@ -883,11 +849,9 @@ static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 		return stage2_map_walk_table_pre(addr, end, level, ptep, data);
 	case KVM_PGTABLE_WALK_LEAF:
 		return stage2_map_walk_leaf(addr, end, level, ptep, data);
<span class=del>-	case KVM_PGTABLE_WALK_TABLE_POST:
-		return stage2_map_walk_table_post(addr, end, level, ptep, data);
</span><span class=add>+	default:
+		return -EINVAL;
</span> 	}
<span class=del>-
-	return -EINVAL;
</span> }
 
 int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
<span class=hunk>@@ -905,8 +869,7 @@ int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
</span> 	struct kvm_pgtable_walker walker = {
 		.cb		= stage2_map_walker,
 		.flags		= KVM_PGTABLE_WALK_TABLE_PRE |
<span class=del>-				  KVM_PGTABLE_WALK_LEAF |
-				  KVM_PGTABLE_WALK_TABLE_POST,
</span><span class=add>+				  KVM_PGTABLE_WALK_LEAF,
</span> 		.arg		= &amp;map_data,
 	};
 
<span class=head><a href=#iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c id=Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c>diff</a> --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index c9a13e487187..91521f4aab97 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
</span><span class=hunk>@@ -627,6 +627,7 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
</span> 	.zalloc_page		= stage2_memcache_zalloc_page,
 	.zalloc_pages_exact	= kvm_host_zalloc_pages_exact,
 	.free_pages_exact	= free_pages_exact,
<span class=add>+	.free_removed_table	= kvm_pgtable_stage2_free_removed,
</span> 	.get_page		= kvm_host_get_page,
 	.put_page		= kvm_host_put_page,
 	.page_count		= kvm_host_page_count,
-- 
2.37.2.672.g94769d06f0-goog


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#mbc09837aeeb8bda33d35c82fe34163b1b3d4fc52 id=ebc09837aeeb8bda33d35c82fe34163b1b3d4fc52>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#rbc09837aeeb8bda33d35c82fe34163b1b3d4fc52>96+ messages in thread</a></pre><li><pre><a href=#ebc09837aeeb8bda33d35c82fe34163b1b3d4fc52 id=mbc09837aeeb8bda33d35c82fe34163b1b3d4fc52>*</a> <b>[PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#rbc09837aeeb8bda33d35c82fe34163b1b3d4fc52>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-kernel, kvmarm, linux-arm-kernel, kvm

The break-before-make sequence is a bit annoying as it opens a window
wherein memory is unmapped from the guest. KVM should replace the PTE
as quickly as possible and avoid unnecessary work in between.

Presently, the stage-2 map walker tears down a removed table before
installing a block mapping when coalescing a table into a block. As the
removed table is no longer visible to hardware walkers after the
DSB+TLBI, it is possible to move the remaining cleanup to happen after
installing the new PTE.

Reshuffle the stage-2 map walker to install the new block entry in
the pre-order callback. Unwire all of the teardown logic and replace
it with a call to kvm_pgtable_stage2_free_removed() after fixing
the PTE. The post-order visitor is now completely unnecessary, so drop
it. Finally, touch up the comments to better represent the now
simplified map walker.

Note that the call to tear down the unlinked stage-2 is indirected
as a subsequent change will use an RCU callback to trigger tear down.
RCU is not available to pKVM, so there is a need to use different
implementations on pKVM and non-pKVM VMs.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h href=#Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>arch/arm64/include/asm/kvm_pgtable.h</a>  |  3 +
 <a id=iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c href=#Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c>arch/arm64/kvm/hyp/nvhe/mem_protect.c</a> |  1 +
 <a id=iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a>          | 83 ++++++++-------------------
 <a id=iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c href=#Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c>arch/arm64/kvm/mmu.c</a>                  |  1 +
 4 files <a href=#ebc09837aeeb8bda33d35c82fe34163b1b3d4fc52>changed</a>, 28 insertions(+), 60 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h id=Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>diff</a> --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index d71fb92dc913..c25633f53b2b 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -77,6 +77,8 @@ static inline bool kvm_level_supports_block_mapping(u32 level)
</span>  *				allocation is physically contiguous.
  * @free_pages_exact:		Free an exact number of memory pages previously
  *				allocated by zalloc_pages_exact.
<span class=add>+ * @free_removed_table:		Free a removed paging structure by unlinking and
+ *				dropping references.
</span>  * @get_page:			Increment the refcount on a page.
  * @put_page:			Decrement the refcount on a page. When the
  *				refcount reaches 0 the page is automatically
<span class=hunk>@@ -95,6 +97,7 @@ struct kvm_pgtable_mm_ops {
</span> 	void*		(*zalloc_page)(void *arg);
 	void*		(*zalloc_pages_exact)(size_t size);
 	void		(*free_pages_exact)(void *addr, size_t size);
<span class=add>+	void		(*free_removed_table)(void *addr, u32 level, void *arg);
</span> 	void		(*get_page)(void *addr);
 	void		(*put_page)(void *addr);
 	int		(*page_count)(void *addr);
<span class=head><a href=#iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c id=Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c>diff</a> --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
index 1e78acf9662e..a930fdee6fce 100644
--- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c
+++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
</span><span class=hunk>@@ -93,6 +93,7 @@ static int prepare_s2_pool(void *pgt_pool_base)
</span> 	host_kvm.mm_ops = (struct kvm_pgtable_mm_ops) {
 		.zalloc_pages_exact = host_s2_zalloc_pages_exact,
 		.zalloc_page = host_s2_zalloc_page,
<span class=add>+		.free_removed_table = kvm_pgtable_stage2_free_removed,
</span> 		.phys_to_virt = hyp_phys_to_virt,
 		.virt_to_phys = hyp_virt_to_phys,
 		.page_count = hyp_page_count,
<span class=head><a href=#iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index d8127c25424c..5c0c8028d71c 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -763,17 +763,21 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
</span> 	return 0;
 }
 
<span class=add>+static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
+				struct stage2_map_data *data);
+
</span> static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
 				     kvm_pte_t *ptep,
 				     struct stage2_map_data *data)
 {
<span class=del>-	if (data-&gt;anchor)
-		return 0;
</span><span class=add>+	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
+	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
+	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
+	int ret;
</span> 
 	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
 		return 0;
 
<span class=del>-	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
</span> 	kvm_clear_pte(ptep);
 
 	/*
<span class=hunk>@@ -782,8 +786,13 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
</span> 	 * individually.
 	 */
 	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
<span class=del>-	data-&gt;anchor = ptep;
-	return 0;
</span><span class=add>+
+	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
+
+	mm_ops-&gt;put_page(ptep);
+	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
+
+	return ret;
</span> }
 
 static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=hunk>@@ -793,13 +802,6 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 	kvm_pte_t *childp, pte = *ptep;
 	int ret;
 
<span class=del>-	if (data-&gt;anchor) {
-		if (stage2_pte_is_counted(pte))
-			mm_ops-&gt;put_page(ptep);
-
-		return 0;
-	}
-
</span> 	ret = stage2_map_walker_try_leaf(addr, end, level, ptep, data);
 	if (ret != -E2BIG)
 		return ret;
<span class=hunk>@@ -828,50 +830,14 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 	return 0;
 }
 
<span class=del>-static int stage2_map_walk_table_post(u64 addr, u64 end, u32 level,
-				      kvm_pte_t *ptep,
-				      struct stage2_map_data *data)
-{
-	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
-	kvm_pte_t *childp;
-	int ret = 0;
-
-	if (!data-&gt;anchor)
-		return 0;
-
-	if (data-&gt;anchor == ptep) {
-		childp = data-&gt;childp;
-		data-&gt;anchor = NULL;
-		data-&gt;childp = NULL;
-		ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
-	} else {
-		childp = kvm_pte_follow(*ptep, mm_ops);
-	}
-
-	mm_ops-&gt;put_page(childp);
-	mm_ops-&gt;put_page(ptep);
-
-	return ret;
-}
-
</span> /*
<span class=del>- * This is a little fiddly, as we use all three of the walk flags. The idea
- * is that the TABLE_PRE callback runs for table entries on the way down,
- * looking for table entries which we could conceivably replace with a
- * block entry for this mapping. If it finds one, then it sets the 'anchor'
- * field in 'struct stage2_map_data' to point at the table entry, before
- * clearing the entry to zero and descending into the now detached table.
- *
- * The behaviour of the LEAF callback then depends on whether or not the
- * anchor has been set. If not, then we're not using a block mapping higher
- * up the table and we perform the mapping at the existing leaves instead.
- * If, on the other hand, the anchor _is_ set, then we drop references to
- * all valid leaves so that the pages beneath the anchor can be freed.
</span><span class=add>+ * The TABLE_PRE callback runs for table entries on the way down, looking
+ * for table entries which we could conceivably replace with a block entry
+ * for this mapping. If it finds one it replaces the entry and calls
+ * kvm_pgtable_mm_ops::free_removed_table() to tear down the detached table.
</span>  *
<span class=del>- * Finally, the TABLE_POST callback does nothing if the anchor has not
- * been set, but otherwise frees the page-table pages while walking back up
- * the page-table, installing the block entry when it revisits the anchor
- * pointer and clearing the anchor to NULL.
</span><span class=add>+ * Otherwise, the LEAF callback performs the mapping at the existing leaves
+ * instead.
</span>  */
 static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
 			     enum kvm_pgtable_walk_flags flag, void * const arg)
<span class=hunk>@@ -883,11 +849,9 @@ static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 		return stage2_map_walk_table_pre(addr, end, level, ptep, data);
 	case KVM_PGTABLE_WALK_LEAF:
 		return stage2_map_walk_leaf(addr, end, level, ptep, data);
<span class=del>-	case KVM_PGTABLE_WALK_TABLE_POST:
-		return stage2_map_walk_table_post(addr, end, level, ptep, data);
</span><span class=add>+	default:
+		return -EINVAL;
</span> 	}
<span class=del>-
-	return -EINVAL;
</span> }
 
 int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
<span class=hunk>@@ -905,8 +869,7 @@ int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
</span> 	struct kvm_pgtable_walker walker = {
 		.cb		= stage2_map_walker,
 		.flags		= KVM_PGTABLE_WALK_TABLE_PRE |
<span class=del>-				  KVM_PGTABLE_WALK_LEAF |
-				  KVM_PGTABLE_WALK_TABLE_POST,
</span><span class=add>+				  KVM_PGTABLE_WALK_LEAF,
</span> 		.arg		= &amp;map_data,
 	};
 
<span class=head><a href=#iZ2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c id=Z2e.:..:20220830194132.962932-3-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c>diff</a> --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index c9a13e487187..91521f4aab97 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
</span><span class=hunk>@@ -627,6 +627,7 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
</span> 	.zalloc_page		= stage2_memcache_zalloc_page,
 	.zalloc_pages_exact	= kvm_host_zalloc_pages_exact,
 	.free_pages_exact	= free_pages_exact,
<span class=add>+	.free_removed_table	= kvm_pgtable_stage2_free_removed,
</span> 	.get_page		= kvm_host_get_page,
 	.put_page		= kvm_host_put_page,
 	.page_count		= kvm_host_page_count,
-- 
2.37.2.672.g94769d06f0-goog

_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#mbc09837aeeb8bda33d35c82fe34163b1b3d4fc52 id=ebc09837aeeb8bda33d35c82fe34163b1b3d4fc52>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-3-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#rbc09837aeeb8bda33d35c82fe34163b1b3d4fc52>96+ messages in thread</a></pre><li><pre><a href=#e842f491058e4efc7828eeedfd560576e55b54761 id=m842f491058e4efc7828eeedfd560576e55b54761>*</a> <b>Re: [PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
  2022-08-30 19:41   ` <a href=#mbc09837aeeb8bda33d35c82fe34163b1b3d4fc52>Oliver Upton</a>
  (?)
<b>@ 2022-09-06 14:35     ` Quentin Perret</b>
  <a href=#r842f491058e4efc7828eeedfd560576e55b54761>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Quentin Perret @ 2022-09-06 14:35 UTC (<a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: kvm, Marc Zyngier, Ben Gardon, linux-kernel, Catalin Marinas,
	David Matlack, Paolo Bonzini, Will Deacon, kvmarm,
	linux-arm-kernel

Hi Oliver,

On Tuesday 30 Aug 2022 at 19:41:20 (+0000), Oliver Upton wrote:
<span class=q>&gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  				     kvm_pte_t *ptep,
&gt;  				     struct stage2_map_data *data)
&gt;  {
&gt; -	if (data-&gt;anchor)
&gt; -		return 0;
&gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; +	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
&gt; +	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
&gt; +	int ret;
&gt;  
&gt;  	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
&gt;  		return 0;
&gt;  
&gt; -	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
&gt;  	kvm_clear_pte(ptep);
&gt;  
&gt;  	/*
&gt; @@ -782,8 +786,13 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  	 * individually.
&gt;  	 */
&gt;  	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt; -	data-&gt;anchor = ptep;
&gt; -	return 0;
&gt; +
&gt; +	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; +
&gt; +	mm_ops-&gt;put_page(ptep);
&gt; +	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
</span>
By the look of it, __kvm_pgtable_visit() has saved the table PTE on the
stack prior to calling the TABLE_PRE callback, and it then uses the PTE
from its stack and does kvm_pte_follow() to find the childp, and walks
from there. Would that be a UAF now?

<span class=q>&gt; +	return ret;
&gt;  }
</span>_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m842f491058e4efc7828eeedfd560576e55b54761 id=e842f491058e4efc7828eeedfd560576e55b54761>^</a> <a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/>permalink</a> <a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/t/#u><b>nested</b></a>] <a href=#r842f491058e4efc7828eeedfd560576e55b54761>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e842f491058e4efc7828eeedfd560576e55b54761 id=m842f491058e4efc7828eeedfd560576e55b54761>*</a> <b>Re: [PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
<b>@ 2022-09-06 14:35     ` Quentin Perret</b>
  <a href=#r842f491058e4efc7828eeedfd560576e55b54761>0 siblings, 0 replies; 96+ messages in thread</a>
From: Quentin Perret @ 2022-09-06 14:35 UTC (<a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Ricardo Koller, Reiji Watanabe,
	David Matlack, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

Hi Oliver,

On Tuesday 30 Aug 2022 at 19:41:20 (+0000), Oliver Upton wrote:
<span class=q>&gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  				     kvm_pte_t *ptep,
&gt;  				     struct stage2_map_data *data)
&gt;  {
&gt; -	if (data-&gt;anchor)
&gt; -		return 0;
&gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; +	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
&gt; +	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
&gt; +	int ret;
&gt;  
&gt;  	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
&gt;  		return 0;
&gt;  
&gt; -	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
&gt;  	kvm_clear_pte(ptep);
&gt;  
&gt;  	/*
&gt; @@ -782,8 +786,13 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  	 * individually.
&gt;  	 */
&gt;  	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt; -	data-&gt;anchor = ptep;
&gt; -	return 0;
&gt; +
&gt; +	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; +
&gt; +	mm_ops-&gt;put_page(ptep);
&gt; +	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
</span>
By the look of it, __kvm_pgtable_visit() has saved the table PTE on the
stack prior to calling the TABLE_PRE callback, and it then uses the PTE
from its stack and does kvm_pte_follow() to find the childp, and walks
from there. Would that be a UAF now?

<span class=q>&gt; +	return ret;
&gt;  }
</span>
_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m842f491058e4efc7828eeedfd560576e55b54761 id=e842f491058e4efc7828eeedfd560576e55b54761>^</a> <a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/>permalink</a> <a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/t/#u><b>nested</b></a>] <a href=#r842f491058e4efc7828eeedfd560576e55b54761>96+ messages in thread</a></pre><li><pre><a href=#e842f491058e4efc7828eeedfd560576e55b54761 id=m842f491058e4efc7828eeedfd560576e55b54761>*</a> <b>Re: [PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
<b>@ 2022-09-06 14:35     ` Quentin Perret</b>
  <a href=#r842f491058e4efc7828eeedfd560576e55b54761>0 siblings, 0 replies; 96+ messages in thread</a>
From: Quentin Perret @ 2022-09-06 14:35 UTC (<a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Ricardo Koller, Reiji Watanabe,
	David Matlack, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

Hi Oliver,

On Tuesday 30 Aug 2022 at 19:41:20 (+0000), Oliver Upton wrote:
<span class=q>&gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  				     kvm_pte_t *ptep,
&gt;  				     struct stage2_map_data *data)
&gt;  {
&gt; -	if (data-&gt;anchor)
&gt; -		return 0;
&gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; +	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
&gt; +	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
&gt; +	int ret;
&gt;  
&gt;  	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
&gt;  		return 0;
&gt;  
&gt; -	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
&gt;  	kvm_clear_pte(ptep);
&gt;  
&gt;  	/*
&gt; @@ -782,8 +786,13 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  	 * individually.
&gt;  	 */
&gt;  	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt; -	data-&gt;anchor = ptep;
&gt; -	return 0;
&gt; +
&gt; +	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; +
&gt; +	mm_ops-&gt;put_page(ptep);
&gt; +	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
</span>
By the look of it, __kvm_pgtable_visit() has saved the table PTE on the
stack prior to calling the TABLE_PRE callback, and it then uses the PTE
from its stack and does kvm_pte_follow() to find the childp, and walks
from there. Would that be a UAF now?

<span class=q>&gt; +	return ret;
&gt;  }
</span>
<a href=#m842f491058e4efc7828eeedfd560576e55b54761 id=e842f491058e4efc7828eeedfd560576e55b54761>^</a> <a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/>permalink</a> <a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/Yxdaw1qng%2FOr0LLA@google.com/t/#u><b>nested</b></a>] <a href=#r842f491058e4efc7828eeedfd560576e55b54761>96+ messages in thread</a></pre><li><pre><a href=#e7c9be6b16683874a19186f163e3ebda03660861d id=m7c9be6b16683874a19186f163e3ebda03660861d>*</a> <b>Re: [PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
  2022-09-06 14:35     ` <a href=#m842f491058e4efc7828eeedfd560576e55b54761>Quentin Perret</a>
  (?)
<b>@ 2022-09-09 10:04       ` Oliver Upton</b>
  <a href=#r7c9be6b16683874a19186f163e3ebda03660861d>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-09-09 10:04 UTC (<a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/raw>raw</a>)
  To: Quentin Perret
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Ricardo Koller, Reiji Watanabe,
	David Matlack, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

On Tue, Sep 06, 2022 at 02:35:47PM +0000, Quentin Perret wrote:
<span class=q>&gt; Hi Oliver,
&gt; 
&gt; On Tuesday 30 Aug 2022 at 19:41:20 (+0000), Oliver Upton wrote:
&gt; &gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt; &gt;  				     kvm_pte_t *ptep,
&gt; &gt;  				     struct stage2_map_data *data)
&gt; &gt;  {
&gt; &gt; -	if (data-&gt;anchor)
&gt; &gt; -		return 0;
&gt; &gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; &gt; +	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
&gt; &gt; +	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
&gt; &gt; +	int ret;
&gt; &gt;  
&gt; &gt;  	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
&gt; &gt;  		return 0;
&gt; &gt;  
&gt; &gt; -	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
&gt; &gt;  	kvm_clear_pte(ptep);
&gt; &gt;  
&gt; &gt;  	/*
&gt; &gt; @@ -782,8 +786,13 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt; &gt;  	 * individually.
&gt; &gt;  	 */
&gt; &gt;  	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt; &gt; -	data-&gt;anchor = ptep;
&gt; &gt; -	return 0;
&gt; &gt; +
&gt; &gt; +	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; &gt; +
&gt; &gt; +	mm_ops-&gt;put_page(ptep);
&gt; &gt; +	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
&gt; 
&gt; By the look of it, __kvm_pgtable_visit() has saved the table PTE on the
&gt; stack prior to calling the TABLE_PRE callback, and it then uses the PTE
&gt; from its stack and does kvm_pte_follow() to find the childp, and walks
&gt; from there. Would that be a UAF now?
</span>
Sure would, I suppose the actual UAF is hidden by the use of RCU later
in the series. Nonetheless, I'm going to adopt David's suggestion of
just rereading the PTE which should tidy this up.

Thanks for catching this.

--
Best,
Oliver

<a href=#m7c9be6b16683874a19186f163e3ebda03660861d id=e7c9be6b16683874a19186f163e3ebda03660861d>^</a> <a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/t/#u><b>nested</b></a>] <a href=#r7c9be6b16683874a19186f163e3ebda03660861d>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e7c9be6b16683874a19186f163e3ebda03660861d id=m7c9be6b16683874a19186f163e3ebda03660861d>*</a> <b>Re: [PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
<b>@ 2022-09-09 10:04       ` Oliver Upton</b>
  <a href=#r7c9be6b16683874a19186f163e3ebda03660861d>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-09-09 10:04 UTC (<a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/raw>raw</a>)
  To: Quentin Perret
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Ricardo Koller, Reiji Watanabe,
	David Matlack, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

On Tue, Sep 06, 2022 at 02:35:47PM +0000, Quentin Perret wrote:
<span class=q>&gt; Hi Oliver,
&gt; 
&gt; On Tuesday 30 Aug 2022 at 19:41:20 (+0000), Oliver Upton wrote:
&gt; &gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt; &gt;  				     kvm_pte_t *ptep,
&gt; &gt;  				     struct stage2_map_data *data)
&gt; &gt;  {
&gt; &gt; -	if (data-&gt;anchor)
&gt; &gt; -		return 0;
&gt; &gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; &gt; +	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
&gt; &gt; +	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
&gt; &gt; +	int ret;
&gt; &gt;  
&gt; &gt;  	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
&gt; &gt;  		return 0;
&gt; &gt;  
&gt; &gt; -	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
&gt; &gt;  	kvm_clear_pte(ptep);
&gt; &gt;  
&gt; &gt;  	/*
&gt; &gt; @@ -782,8 +786,13 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt; &gt;  	 * individually.
&gt; &gt;  	 */
&gt; &gt;  	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt; &gt; -	data-&gt;anchor = ptep;
&gt; &gt; -	return 0;
&gt; &gt; +
&gt; &gt; +	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; &gt; +
&gt; &gt; +	mm_ops-&gt;put_page(ptep);
&gt; &gt; +	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
&gt; 
&gt; By the look of it, __kvm_pgtable_visit() has saved the table PTE on the
&gt; stack prior to calling the TABLE_PRE callback, and it then uses the PTE
&gt; from its stack and does kvm_pte_follow() to find the childp, and walks
&gt; from there. Would that be a UAF now?
</span>
Sure would, I suppose the actual UAF is hidden by the use of RCU later
in the series. Nonetheless, I'm going to adopt David's suggestion of
just rereading the PTE which should tidy this up.

Thanks for catching this.

--
Best,
Oliver

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m7c9be6b16683874a19186f163e3ebda03660861d id=e7c9be6b16683874a19186f163e3ebda03660861d>^</a> <a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/t/#u><b>nested</b></a>] <a href=#r7c9be6b16683874a19186f163e3ebda03660861d>96+ messages in thread</a></pre><li><pre><a href=#e7c9be6b16683874a19186f163e3ebda03660861d id=m7c9be6b16683874a19186f163e3ebda03660861d>*</a> <b>Re: [PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
<b>@ 2022-09-09 10:04       ` Oliver Upton</b>
  <a href=#r7c9be6b16683874a19186f163e3ebda03660861d>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-09-09 10:04 UTC (<a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/raw>raw</a>)
  To: Quentin Perret
  Cc: kvm, Marc Zyngier, Ben Gardon, linux-kernel, Catalin Marinas,
	David Matlack, Paolo Bonzini, Will Deacon, kvmarm,
	linux-arm-kernel

On Tue, Sep 06, 2022 at 02:35:47PM +0000, Quentin Perret wrote:
<span class=q>&gt; Hi Oliver,
&gt; 
&gt; On Tuesday 30 Aug 2022 at 19:41:20 (+0000), Oliver Upton wrote:
&gt; &gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt; &gt;  				     kvm_pte_t *ptep,
&gt; &gt;  				     struct stage2_map_data *data)
&gt; &gt;  {
&gt; &gt; -	if (data-&gt;anchor)
&gt; &gt; -		return 0;
&gt; &gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; &gt; +	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
&gt; &gt; +	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
&gt; &gt; +	int ret;
&gt; &gt;  
&gt; &gt;  	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
&gt; &gt;  		return 0;
&gt; &gt;  
&gt; &gt; -	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
&gt; &gt;  	kvm_clear_pte(ptep);
&gt; &gt;  
&gt; &gt;  	/*
&gt; &gt; @@ -782,8 +786,13 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt; &gt;  	 * individually.
&gt; &gt;  	 */
&gt; &gt;  	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt; &gt; -	data-&gt;anchor = ptep;
&gt; &gt; -	return 0;
&gt; &gt; +
&gt; &gt; +	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; &gt; +
&gt; &gt; +	mm_ops-&gt;put_page(ptep);
&gt; &gt; +	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
&gt; 
&gt; By the look of it, __kvm_pgtable_visit() has saved the table PTE on the
&gt; stack prior to calling the TABLE_PRE callback, and it then uses the PTE
&gt; from its stack and does kvm_pte_follow() to find the childp, and walks
&gt; from there. Would that be a UAF now?
</span>
Sure would, I suppose the actual UAF is hidden by the use of RCU later
in the series. Nonetheless, I'm going to adopt David's suggestion of
just rereading the PTE which should tidy this up.

Thanks for catching this.

--
Best,
Oliver
_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m7c9be6b16683874a19186f163e3ebda03660861d id=e7c9be6b16683874a19186f163e3ebda03660861d>^</a> <a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxsPtT6DXxl2q%2FOG@google.com/t/#u><b>nested</b></a>] <a href=#r7c9be6b16683874a19186f163e3ebda03660861d>96+ messages in thread</a></pre></ul></ul><li><pre><a href=#edeb735cd82e69f17c67ed4ddc2564466844b1de1 id=mdeb735cd82e69f17c67ed4ddc2564466844b1de1>*</a> <b>Re: [PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
  2022-08-30 19:41   ` <a href=#mbc09837aeeb8bda33d35c82fe34163b1b3d4fc52>Oliver Upton</a>
  (?)
<b>@ 2022-09-07 20:57     ` David Matlack</b>
  <a href=#rdeb735cd82e69f17c67ed4ddc2564466844b1de1>-1 siblings, 0 replies; 96+ messages in thread</a>
From: David Matlack @ 2022-09-07 20:57 UTC (<a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

On Tue, Aug 30, 2022 at 07:41:20PM +0000, Oliver Upton wrote:
[...]
<span class=q>&gt;  
&gt; +static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; +				struct stage2_map_data *data);
&gt; +
&gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  				     kvm_pte_t *ptep,
&gt;  				     struct stage2_map_data *data)
&gt;  {
&gt; -	if (data-&gt;anchor)
</span>
Should @anchor and @childp be removed from struct stage2_map_data? This
commit removes the only remaining references to them.

<span class=q>&gt; -		return 0;
&gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; +	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
&gt; +	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
&gt; +	int ret;
&gt;  
&gt;  	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
&gt;  		return 0;
&gt;  
&gt; -	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
&gt;  	kvm_clear_pte(ptep);
&gt;  
&gt;  	/*
</span>[...]
<span class=q>&gt;  static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  			     enum kvm_pgtable_walk_flags flag, void * const arg)
&gt; @@ -883,11 +849,9 @@ static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  		return stage2_map_walk_table_pre(addr, end, level, ptep, data);
&gt;  	case KVM_PGTABLE_WALK_LEAF:
&gt;  		return stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; -	case KVM_PGTABLE_WALK_TABLE_POST:
&gt; -		return stage2_map_walk_table_post(addr, end, level, ptep, data);
</span>
kvm_pgtable_stage2_set_owner() still uses stage2_map_walker() with
KVM_PGTABLE_WALK_TABLE_POST.

<a href=#mdeb735cd82e69f17c67ed4ddc2564466844b1de1 id=edeb735cd82e69f17c67ed4ddc2564466844b1de1>^</a> <a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/t/#u><b>nested</b></a>] <a href=#rdeb735cd82e69f17c67ed4ddc2564466844b1de1>96+ messages in thread</a></pre><li><ul><li><pre><a href=#edeb735cd82e69f17c67ed4ddc2564466844b1de1 id=mdeb735cd82e69f17c67ed4ddc2564466844b1de1>*</a> <b>Re: [PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
<b>@ 2022-09-07 20:57     ` David Matlack</b>
  <a href=#rdeb735cd82e69f17c67ed4ddc2564466844b1de1>0 siblings, 0 replies; 96+ messages in thread</a>
From: David Matlack @ 2022-09-07 20:57 UTC (<a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: kvm, Marc Zyngier, linux-kernel, Catalin Marinas, Ben Gardon,
	Paolo Bonzini, Will Deacon, kvmarm, linux-arm-kernel

On Tue, Aug 30, 2022 at 07:41:20PM +0000, Oliver Upton wrote:
[...]
<span class=q>&gt;  
&gt; +static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; +				struct stage2_map_data *data);
&gt; +
&gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  				     kvm_pte_t *ptep,
&gt;  				     struct stage2_map_data *data)
&gt;  {
&gt; -	if (data-&gt;anchor)
</span>
Should @anchor and @childp be removed from struct stage2_map_data? This
commit removes the only remaining references to them.

<span class=q>&gt; -		return 0;
&gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; +	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
&gt; +	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
&gt; +	int ret;
&gt;  
&gt;  	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
&gt;  		return 0;
&gt;  
&gt; -	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
&gt;  	kvm_clear_pte(ptep);
&gt;  
&gt;  	/*
</span>[...]
<span class=q>&gt;  static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  			     enum kvm_pgtable_walk_flags flag, void * const arg)
&gt; @@ -883,11 +849,9 @@ static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  		return stage2_map_walk_table_pre(addr, end, level, ptep, data);
&gt;  	case KVM_PGTABLE_WALK_LEAF:
&gt;  		return stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; -	case KVM_PGTABLE_WALK_TABLE_POST:
&gt; -		return stage2_map_walk_table_post(addr, end, level, ptep, data);
</span>
kvm_pgtable_stage2_set_owner() still uses stage2_map_walker() with
KVM_PGTABLE_WALK_TABLE_POST.
_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#mdeb735cd82e69f17c67ed4ddc2564466844b1de1 id=edeb735cd82e69f17c67ed4ddc2564466844b1de1>^</a> <a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/t/#u><b>nested</b></a>] <a href=#rdeb735cd82e69f17c67ed4ddc2564466844b1de1>96+ messages in thread</a></pre><li><pre><a href=#edeb735cd82e69f17c67ed4ddc2564466844b1de1 id=mdeb735cd82e69f17c67ed4ddc2564466844b1de1>*</a> <b>Re: [PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
<b>@ 2022-09-07 20:57     ` David Matlack</b>
  <a href=#rdeb735cd82e69f17c67ed4ddc2564466844b1de1>0 siblings, 0 replies; 96+ messages in thread</a>
From: David Matlack @ 2022-09-07 20:57 UTC (<a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

On Tue, Aug 30, 2022 at 07:41:20PM +0000, Oliver Upton wrote:
[...]
<span class=q>&gt;  
&gt; +static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; +				struct stage2_map_data *data);
&gt; +
&gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  				     kvm_pte_t *ptep,
&gt;  				     struct stage2_map_data *data)
&gt;  {
&gt; -	if (data-&gt;anchor)
</span>
Should @anchor and @childp be removed from struct stage2_map_data? This
commit removes the only remaining references to them.

<span class=q>&gt; -		return 0;
&gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; +	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
&gt; +	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
&gt; +	int ret;
&gt;  
&gt;  	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
&gt;  		return 0;
&gt;  
&gt; -	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
&gt;  	kvm_clear_pte(ptep);
&gt;  
&gt;  	/*
</span>[...]
<span class=q>&gt;  static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  			     enum kvm_pgtable_walk_flags flag, void * const arg)
&gt; @@ -883,11 +849,9 @@ static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  		return stage2_map_walk_table_pre(addr, end, level, ptep, data);
&gt;  	case KVM_PGTABLE_WALK_LEAF:
&gt;  		return stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; -	case KVM_PGTABLE_WALK_TABLE_POST:
&gt; -		return stage2_map_walk_table_post(addr, end, level, ptep, data);
</span>
kvm_pgtable_stage2_set_owner() still uses stage2_map_walker() with
KVM_PGTABLE_WALK_TABLE_POST.

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#mdeb735cd82e69f17c67ed4ddc2564466844b1de1 id=edeb735cd82e69f17c67ed4ddc2564466844b1de1>^</a> <a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxkFrSmSKdBFEoZp@google.com/t/#u><b>nested</b></a>] <a href=#rdeb735cd82e69f17c67ed4ddc2564466844b1de1>96+ messages in thread</a></pre><li><pre><a href=#e7541819af8643527fcd1ce40f43b8e768a1d7f56 id=m7541819af8643527fcd1ce40f43b8e768a1d7f56>*</a> <b>Re: [PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
  2022-09-07 20:57     ` <a href=#mdeb735cd82e69f17c67ed4ddc2564466844b1de1>David Matlack</a>
  (?)
<b>@ 2022-09-09 10:07       ` Oliver Upton</b>
  <a href=#r7541819af8643527fcd1ce40f43b8e768a1d7f56>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-09-09 10:07 UTC (<a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/raw>raw</a>)
  To: David Matlack
  Cc: kvm, Marc Zyngier, linux-kernel, Catalin Marinas, Ben Gardon,
	Paolo Bonzini, Will Deacon, kvmarm, linux-arm-kernel

On Wed, Sep 07, 2022 at 01:57:17PM -0700, David Matlack wrote:
<span class=q>&gt; On Tue, Aug 30, 2022 at 07:41:20PM +0000, Oliver Upton wrote:
&gt; [...]
&gt; &gt;  
&gt; &gt; +static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; &gt; +				struct stage2_map_data *data);
&gt; &gt; +
&gt; &gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt; &gt;  				     kvm_pte_t *ptep,
&gt; &gt;  				     struct stage2_map_data *data)
&gt; &gt;  {
&gt; &gt; -	if (data-&gt;anchor)
&gt; 
&gt; Should @anchor and @childp be removed from struct stage2_map_data? This
&gt; commit removes the only remaining references to them.
</span>
Yup, I'll toss those in the next spin.

<span class=q>&gt; &gt; -		return 0;
&gt; &gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; &gt; +	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
&gt; &gt; +	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
&gt; &gt; +	int ret;
&gt; &gt;  
&gt; &gt;  	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
&gt; &gt;  		return 0;
&gt; &gt;  
&gt; &gt; -	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
&gt; &gt;  	kvm_clear_pte(ptep);
&gt; &gt;  
&gt; &gt;  	/*
&gt; [...]
&gt; &gt;  static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; &gt;  			     enum kvm_pgtable_walk_flags flag, void * const arg)
&gt; &gt; @@ -883,11 +849,9 @@ static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; &gt;  		return stage2_map_walk_table_pre(addr, end, level, ptep, data);
&gt; &gt;  	case KVM_PGTABLE_WALK_LEAF:
&gt; &gt;  		return stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; &gt; -	case KVM_PGTABLE_WALK_TABLE_POST:
&gt; &gt; -		return stage2_map_walk_table_post(addr, end, level, ptep, data);
&gt; 
&gt; kvm_pgtable_stage2_set_owner() still uses stage2_map_walker() with
&gt; KVM_PGTABLE_WALK_TABLE_POST.
</span>
Good catch, I'll drop the TABLE_POST flag there as well.

Appreciate the reviews on the series.

--
Thanks,
Oliver
_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m7541819af8643527fcd1ce40f43b8e768a1d7f56 id=e7541819af8643527fcd1ce40f43b8e768a1d7f56>^</a> <a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/t/#u><b>nested</b></a>] <a href=#r7541819af8643527fcd1ce40f43b8e768a1d7f56>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e7541819af8643527fcd1ce40f43b8e768a1d7f56 id=m7541819af8643527fcd1ce40f43b8e768a1d7f56>*</a> <b>Re: [PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
<b>@ 2022-09-09 10:07       ` Oliver Upton</b>
  <a href=#r7541819af8643527fcd1ce40f43b8e768a1d7f56>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-09-09 10:07 UTC (<a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/raw>raw</a>)
  To: David Matlack
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

On Wed, Sep 07, 2022 at 01:57:17PM -0700, David Matlack wrote:
<span class=q>&gt; On Tue, Aug 30, 2022 at 07:41:20PM +0000, Oliver Upton wrote:
&gt; [...]
&gt; &gt;  
&gt; &gt; +static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; &gt; +				struct stage2_map_data *data);
&gt; &gt; +
&gt; &gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt; &gt;  				     kvm_pte_t *ptep,
&gt; &gt;  				     struct stage2_map_data *data)
&gt; &gt;  {
&gt; &gt; -	if (data-&gt;anchor)
&gt; 
&gt; Should @anchor and @childp be removed from struct stage2_map_data? This
&gt; commit removes the only remaining references to them.
</span>
Yup, I'll toss those in the next spin.

<span class=q>&gt; &gt; -		return 0;
&gt; &gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; &gt; +	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
&gt; &gt; +	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
&gt; &gt; +	int ret;
&gt; &gt;  
&gt; &gt;  	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
&gt; &gt;  		return 0;
&gt; &gt;  
&gt; &gt; -	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
&gt; &gt;  	kvm_clear_pte(ptep);
&gt; &gt;  
&gt; &gt;  	/*
&gt; [...]
&gt; &gt;  static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; &gt;  			     enum kvm_pgtable_walk_flags flag, void * const arg)
&gt; &gt; @@ -883,11 +849,9 @@ static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; &gt;  		return stage2_map_walk_table_pre(addr, end, level, ptep, data);
&gt; &gt;  	case KVM_PGTABLE_WALK_LEAF:
&gt; &gt;  		return stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; &gt; -	case KVM_PGTABLE_WALK_TABLE_POST:
&gt; &gt; -		return stage2_map_walk_table_post(addr, end, level, ptep, data);
&gt; 
&gt; kvm_pgtable_stage2_set_owner() still uses stage2_map_walker() with
&gt; KVM_PGTABLE_WALK_TABLE_POST.
</span>
Good catch, I'll drop the TABLE_POST flag there as well.

Appreciate the reviews on the series.

--
Thanks,
Oliver

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m7541819af8643527fcd1ce40f43b8e768a1d7f56 id=e7541819af8643527fcd1ce40f43b8e768a1d7f56>^</a> <a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/t/#u><b>nested</b></a>] <a href=#r7541819af8643527fcd1ce40f43b8e768a1d7f56>96+ messages in thread</a></pre><li><pre><a href=#e7541819af8643527fcd1ce40f43b8e768a1d7f56 id=m7541819af8643527fcd1ce40f43b8e768a1d7f56>*</a> <b>Re: [PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
<b>@ 2022-09-09 10:07       ` Oliver Upton</b>
  <a href=#r7541819af8643527fcd1ce40f43b8e768a1d7f56>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-09-09 10:07 UTC (<a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/raw>raw</a>)
  To: David Matlack
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

On Wed, Sep 07, 2022 at 01:57:17PM -0700, David Matlack wrote:
<span class=q>&gt; On Tue, Aug 30, 2022 at 07:41:20PM +0000, Oliver Upton wrote:
&gt; [...]
&gt; &gt;  
&gt; &gt; +static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; &gt; +				struct stage2_map_data *data);
&gt; &gt; +
&gt; &gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt; &gt;  				     kvm_pte_t *ptep,
&gt; &gt;  				     struct stage2_map_data *data)
&gt; &gt;  {
&gt; &gt; -	if (data-&gt;anchor)
&gt; 
&gt; Should @anchor and @childp be removed from struct stage2_map_data? This
&gt; commit removes the only remaining references to them.
</span>
Yup, I'll toss those in the next spin.

<span class=q>&gt; &gt; -		return 0;
&gt; &gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; &gt; +	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
&gt; &gt; +	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
&gt; &gt; +	int ret;
&gt; &gt;  
&gt; &gt;  	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
&gt; &gt;  		return 0;
&gt; &gt;  
&gt; &gt; -	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
&gt; &gt;  	kvm_clear_pte(ptep);
&gt; &gt;  
&gt; &gt;  	/*
&gt; [...]
&gt; &gt;  static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; &gt;  			     enum kvm_pgtable_walk_flags flag, void * const arg)
&gt; &gt; @@ -883,11 +849,9 @@ static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; &gt;  		return stage2_map_walk_table_pre(addr, end, level, ptep, data);
&gt; &gt;  	case KVM_PGTABLE_WALK_LEAF:
&gt; &gt;  		return stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; &gt; -	case KVM_PGTABLE_WALK_TABLE_POST:
&gt; &gt; -		return stage2_map_walk_table_post(addr, end, level, ptep, data);
&gt; 
&gt; kvm_pgtable_stage2_set_owner() still uses stage2_map_walker() with
&gt; KVM_PGTABLE_WALK_TABLE_POST.
</span>
Good catch, I'll drop the TABLE_POST flag there as well.

Appreciate the reviews on the series.

--
Thanks,
Oliver

<a href=#m7541819af8643527fcd1ce40f43b8e768a1d7f56 id=e7541819af8643527fcd1ce40f43b8e768a1d7f56>^</a> <a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxsQfwJ++izBQuEi@google.com/t/#u><b>nested</b></a>] <a href=#r7541819af8643527fcd1ce40f43b8e768a1d7f56>96+ messages in thread</a></pre></ul></ul><li><pre><a href=#e9afe0d55f1b5ee88addd9c7a3785153e6b774154 id=m9afe0d55f1b5ee88addd9c7a3785153e6b774154>*</a> <b>Re: [PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
  2022-08-30 19:41   ` <a href=#mbc09837aeeb8bda33d35c82fe34163b1b3d4fc52>Oliver Upton</a>
  (?)
<b>@ 2022-09-14  0:20     ` Ricardo Koller</b>
  <a href=#r9afe0d55f1b5ee88addd9c7a3785153e6b774154>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Ricardo Koller @ 2022-09-14  0:20 UTC (<a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Reiji Watanabe,
	David Matlack, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

Hi Oliver,

On Tue, Aug 30, 2022 at 07:41:20PM +0000, Oliver Upton wrote:
<span class=q>&gt; The break-before-make sequence is a bit annoying as it opens a window
&gt; wherein memory is unmapped from the guest. KVM should replace the PTE
&gt; as quickly as possible and avoid unnecessary work in between.
&gt; 
&gt; Presently, the stage-2 map walker tears down a removed table before
&gt; installing a block mapping when coalescing a table into a block. As the
&gt; removed table is no longer visible to hardware walkers after the
&gt; DSB+TLBI, it is possible to move the remaining cleanup to happen after
&gt; installing the new PTE.
&gt; 
&gt; Reshuffle the stage-2 map walker to install the new block entry in
&gt; the pre-order callback. Unwire all of the teardown logic and replace
&gt; it with a call to kvm_pgtable_stage2_free_removed() after fixing
&gt; the PTE. The post-order visitor is now completely unnecessary, so drop
&gt; it. Finally, touch up the comments to better represent the now
&gt; simplified map walker.
&gt; 
&gt; Note that the call to tear down the unlinked stage-2 is indirected
&gt; as a subsequent change will use an RCU callback to trigger tear down.
&gt; RCU is not available to pKVM, so there is a need to use different
&gt; implementations on pKVM and non-pKVM VMs.
&gt; 
&gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; ---
&gt;  arch/arm64/include/asm/kvm_pgtable.h  |  3 +
&gt;  arch/arm64/kvm/hyp/nvhe/mem_protect.c |  1 +
&gt;  arch/arm64/kvm/hyp/pgtable.c          | 83 ++++++++-------------------
&gt;  arch/arm64/kvm/mmu.c                  |  1 +
&gt;  4 files changed, 28 insertions(+), 60 deletions(-)
&gt; 
&gt; diff --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
&gt; index d71fb92dc913..c25633f53b2b 100644
&gt; --- a/arch/arm64/include/asm/kvm_pgtable.h
&gt; +++ b/arch/arm64/include/asm/kvm_pgtable.h
&gt; @@ -77,6 +77,8 @@ static inline bool kvm_level_supports_block_mapping(u32 level)
&gt;   *				allocation is physically contiguous.
&gt;   * @free_pages_exact:		Free an exact number of memory pages previously
&gt;   *				allocated by zalloc_pages_exact.
&gt; + * @free_removed_table:		Free a removed paging structure by unlinking and
&gt; + *				dropping references.
&gt;   * @get_page:			Increment the refcount on a page.
&gt;   * @put_page:			Decrement the refcount on a page. When the
&gt;   *				refcount reaches 0 the page is automatically
&gt; @@ -95,6 +97,7 @@ struct kvm_pgtable_mm_ops {
&gt;  	void*		(*zalloc_page)(void *arg);
&gt;  	void*		(*zalloc_pages_exact)(size_t size);
&gt;  	void		(*free_pages_exact)(void *addr, size_t size);
&gt; +	void		(*free_removed_table)(void *addr, u32 level, void *arg);
&gt;  	void		(*get_page)(void *addr);
&gt;  	void		(*put_page)(void *addr);
&gt;  	int		(*page_count)(void *addr);
&gt; diff --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
&gt; index 1e78acf9662e..a930fdee6fce 100644
&gt; --- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c
&gt; +++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
&gt; @@ -93,6 +93,7 @@ static int prepare_s2_pool(void *pgt_pool_base)
&gt;  	host_kvm.mm_ops = (struct kvm_pgtable_mm_ops) {
&gt;  		.zalloc_pages_exact = host_s2_zalloc_pages_exact,
&gt;  		.zalloc_page = host_s2_zalloc_page,
&gt; +		.free_removed_table = kvm_pgtable_stage2_free_removed,
&gt;  		.phys_to_virt = hyp_phys_to_virt,
&gt;  		.virt_to_phys = hyp_virt_to_phys,
&gt;  		.page_count = hyp_page_count,
&gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; index d8127c25424c..5c0c8028d71c 100644
&gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; @@ -763,17 +763,21 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
&gt;  	return 0;
&gt;  }
&gt;  
&gt; +static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; +				struct stage2_map_data *data);
&gt; +
&gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  				     kvm_pte_t *ptep,
&gt;  				     struct stage2_map_data *data)
&gt;  {
&gt; -	if (data-&gt;anchor)
&gt; -		return 0;
&gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; +	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
&gt; +	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
&gt; +	int ret;
&gt;  
&gt;  	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
&gt;  		return 0;
&gt;  
&gt; -	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
&gt;  	kvm_clear_pte(ptep);
&gt;  
&gt;  	/*
&gt; @@ -782,8 +786,13 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  	 * individually.
&gt;  	 */
&gt;  	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt; -	data-&gt;anchor = ptep;
&gt; -	return 0;
&gt; +
&gt; +	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
</span>
I think this always ends up calling stage2_map_walker_try_leaf() (at
least it should). In that case, I think it might be clearer to do so, as
the intention is to just install a block.

<span class=q>&gt; +
&gt; +	mm_ops-&gt;put_page(ptep);
&gt; +	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
</span>
*old = *ptep;

<span class=q>&gt; +
&gt; +	return ret;
&gt;  }
&gt;  
&gt;  static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; @@ -793,13 +802,6 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  	kvm_pte_t *childp, pte = *ptep;
&gt;  	int ret;
&gt;  
&gt; -	if (data-&gt;anchor) {
&gt; -		if (stage2_pte_is_counted(pte))
&gt; -			mm_ops-&gt;put_page(ptep);
&gt; -
&gt; -		return 0;
&gt; -	}
&gt; -
&gt;  	ret = stage2_map_walker_try_leaf(addr, end, level, ptep, data);
&gt;  	if (ret != -E2BIG)
&gt;  		return ret;
&gt; @@ -828,50 +830,14 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  	return 0;
&gt;  }
&gt;  
&gt; -static int stage2_map_walk_table_post(u64 addr, u64 end, u32 level,
&gt; -				      kvm_pte_t *ptep,
&gt; -				      struct stage2_map_data *data)
&gt; -{
&gt; -	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; -	kvm_pte_t *childp;
&gt; -	int ret = 0;
&gt; -
&gt; -	if (!data-&gt;anchor)
&gt; -		return 0;
&gt; -
&gt; -	if (data-&gt;anchor == ptep) {
&gt; -		childp = data-&gt;childp;
&gt; -		data-&gt;anchor = NULL;
&gt; -		data-&gt;childp = NULL;
&gt; -		ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; -	} else {
&gt; -		childp = kvm_pte_follow(*ptep, mm_ops);
&gt; -	}
&gt; -
&gt; -	mm_ops-&gt;put_page(childp);
&gt; -	mm_ops-&gt;put_page(ptep);
&gt; -
&gt; -	return ret;
&gt; -}
&gt; -
&gt;  /*
&gt; - * This is a little fiddly, as we use all three of the walk flags. The idea
&gt; - * is that the TABLE_PRE callback runs for table entries on the way down,
&gt; - * looking for table entries which we could conceivably replace with a
&gt; - * block entry for this mapping. If it finds one, then it sets the 'anchor'
&gt; - * field in 'struct stage2_map_data' to point at the table entry, before
&gt; - * clearing the entry to zero and descending into the now detached table.
&gt; - *
&gt; - * The behaviour of the LEAF callback then depends on whether or not the
&gt; - * anchor has been set. If not, then we're not using a block mapping higher
&gt; - * up the table and we perform the mapping at the existing leaves instead.
&gt; - * If, on the other hand, the anchor _is_ set, then we drop references to
&gt; - * all valid leaves so that the pages beneath the anchor can be freed.
&gt; + * The TABLE_PRE callback runs for table entries on the way down, looking
&gt; + * for table entries which we could conceivably replace with a block entry
&gt; + * for this mapping. If it finds one it replaces the entry and calls
&gt; + * kvm_pgtable_mm_ops::free_removed_table() to tear down the detached table.
&gt;   *
&gt; - * Finally, the TABLE_POST callback does nothing if the anchor has not
&gt; - * been set, but otherwise frees the page-table pages while walking back up
&gt; - * the page-table, installing the block entry when it revisits the anchor
&gt; - * pointer and clearing the anchor to NULL.
&gt; + * Otherwise, the LEAF callback performs the mapping at the existing leaves
&gt; + * instead.
&gt;   */
&gt;  static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  			     enum kvm_pgtable_walk_flags flag, void * const arg)
&gt; @@ -883,11 +849,9 @@ static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  		return stage2_map_walk_table_pre(addr, end, level, ptep, data);
&gt;  	case KVM_PGTABLE_WALK_LEAF:
&gt;  		return stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; -	case KVM_PGTABLE_WALK_TABLE_POST:
&gt; -		return stage2_map_walk_table_post(addr, end, level, ptep, data);
&gt; +	default:
&gt; +		return -EINVAL;
</span>
nice!

<span class=q>&gt;  	}
&gt; -
&gt; -	return -EINVAL;
&gt;  }
&gt;  
&gt;  int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
&gt; @@ -905,8 +869,7 @@ int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
&gt;  	struct kvm_pgtable_walker walker = {
&gt;  		.cb		= stage2_map_walker,
&gt;  		.flags		= KVM_PGTABLE_WALK_TABLE_PRE |
&gt; -				  KVM_PGTABLE_WALK_LEAF |
&gt; -				  KVM_PGTABLE_WALK_TABLE_POST,
&gt; +				  KVM_PGTABLE_WALK_LEAF,
&gt;  		.arg		= &amp;map_data,
&gt;  	};
&gt;  
&gt; diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
&gt; index c9a13e487187..91521f4aab97 100644
&gt; --- a/arch/arm64/kvm/mmu.c
&gt; +++ b/arch/arm64/kvm/mmu.c
&gt; @@ -627,6 +627,7 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
&gt;  	.zalloc_page		= stage2_memcache_zalloc_page,
&gt;  	.zalloc_pages_exact	= kvm_host_zalloc_pages_exact,
&gt;  	.free_pages_exact	= free_pages_exact,
&gt; +	.free_removed_table	= kvm_pgtable_stage2_free_removed,
&gt;  	.get_page		= kvm_host_get_page,
&gt;  	.put_page		= kvm_host_put_page,
&gt;  	.page_count		= kvm_host_page_count,
&gt; -- 
&gt; 2.37.2.672.g94769d06f0-goog
&gt; 
</span>
<a href=#m9afe0d55f1b5ee88addd9c7a3785153e6b774154 id=e9afe0d55f1b5ee88addd9c7a3785153e6b774154>^</a> <a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/t/#u><b>nested</b></a>] <a href=#r9afe0d55f1b5ee88addd9c7a3785153e6b774154>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e9afe0d55f1b5ee88addd9c7a3785153e6b774154 id=m9afe0d55f1b5ee88addd9c7a3785153e6b774154>*</a> <b>Re: [PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
<b>@ 2022-09-14  0:20     ` Ricardo Koller</b>
  <a href=#r9afe0d55f1b5ee88addd9c7a3785153e6b774154>0 siblings, 0 replies; 96+ messages in thread</a>
From: Ricardo Koller @ 2022-09-14  0:20 UTC (<a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Reiji Watanabe,
	David Matlack, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

Hi Oliver,

On Tue, Aug 30, 2022 at 07:41:20PM +0000, Oliver Upton wrote:
<span class=q>&gt; The break-before-make sequence is a bit annoying as it opens a window
&gt; wherein memory is unmapped from the guest. KVM should replace the PTE
&gt; as quickly as possible and avoid unnecessary work in between.
&gt; 
&gt; Presently, the stage-2 map walker tears down a removed table before
&gt; installing a block mapping when coalescing a table into a block. As the
&gt; removed table is no longer visible to hardware walkers after the
&gt; DSB+TLBI, it is possible to move the remaining cleanup to happen after
&gt; installing the new PTE.
&gt; 
&gt; Reshuffle the stage-2 map walker to install the new block entry in
&gt; the pre-order callback. Unwire all of the teardown logic and replace
&gt; it with a call to kvm_pgtable_stage2_free_removed() after fixing
&gt; the PTE. The post-order visitor is now completely unnecessary, so drop
&gt; it. Finally, touch up the comments to better represent the now
&gt; simplified map walker.
&gt; 
&gt; Note that the call to tear down the unlinked stage-2 is indirected
&gt; as a subsequent change will use an RCU callback to trigger tear down.
&gt; RCU is not available to pKVM, so there is a need to use different
&gt; implementations on pKVM and non-pKVM VMs.
&gt; 
&gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; ---
&gt;  arch/arm64/include/asm/kvm_pgtable.h  |  3 +
&gt;  arch/arm64/kvm/hyp/nvhe/mem_protect.c |  1 +
&gt;  arch/arm64/kvm/hyp/pgtable.c          | 83 ++++++++-------------------
&gt;  arch/arm64/kvm/mmu.c                  |  1 +
&gt;  4 files changed, 28 insertions(+), 60 deletions(-)
&gt; 
&gt; diff --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
&gt; index d71fb92dc913..c25633f53b2b 100644
&gt; --- a/arch/arm64/include/asm/kvm_pgtable.h
&gt; +++ b/arch/arm64/include/asm/kvm_pgtable.h
&gt; @@ -77,6 +77,8 @@ static inline bool kvm_level_supports_block_mapping(u32 level)
&gt;   *				allocation is physically contiguous.
&gt;   * @free_pages_exact:		Free an exact number of memory pages previously
&gt;   *				allocated by zalloc_pages_exact.
&gt; + * @free_removed_table:		Free a removed paging structure by unlinking and
&gt; + *				dropping references.
&gt;   * @get_page:			Increment the refcount on a page.
&gt;   * @put_page:			Decrement the refcount on a page. When the
&gt;   *				refcount reaches 0 the page is automatically
&gt; @@ -95,6 +97,7 @@ struct kvm_pgtable_mm_ops {
&gt;  	void*		(*zalloc_page)(void *arg);
&gt;  	void*		(*zalloc_pages_exact)(size_t size);
&gt;  	void		(*free_pages_exact)(void *addr, size_t size);
&gt; +	void		(*free_removed_table)(void *addr, u32 level, void *arg);
&gt;  	void		(*get_page)(void *addr);
&gt;  	void		(*put_page)(void *addr);
&gt;  	int		(*page_count)(void *addr);
&gt; diff --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
&gt; index 1e78acf9662e..a930fdee6fce 100644
&gt; --- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c
&gt; +++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
&gt; @@ -93,6 +93,7 @@ static int prepare_s2_pool(void *pgt_pool_base)
&gt;  	host_kvm.mm_ops = (struct kvm_pgtable_mm_ops) {
&gt;  		.zalloc_pages_exact = host_s2_zalloc_pages_exact,
&gt;  		.zalloc_page = host_s2_zalloc_page,
&gt; +		.free_removed_table = kvm_pgtable_stage2_free_removed,
&gt;  		.phys_to_virt = hyp_phys_to_virt,
&gt;  		.virt_to_phys = hyp_virt_to_phys,
&gt;  		.page_count = hyp_page_count,
&gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; index d8127c25424c..5c0c8028d71c 100644
&gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; @@ -763,17 +763,21 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
&gt;  	return 0;
&gt;  }
&gt;  
&gt; +static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; +				struct stage2_map_data *data);
&gt; +
&gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  				     kvm_pte_t *ptep,
&gt;  				     struct stage2_map_data *data)
&gt;  {
&gt; -	if (data-&gt;anchor)
&gt; -		return 0;
&gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; +	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
&gt; +	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
&gt; +	int ret;
&gt;  
&gt;  	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
&gt;  		return 0;
&gt;  
&gt; -	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
&gt;  	kvm_clear_pte(ptep);
&gt;  
&gt;  	/*
&gt; @@ -782,8 +786,13 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  	 * individually.
&gt;  	 */
&gt;  	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt; -	data-&gt;anchor = ptep;
&gt; -	return 0;
&gt; +
&gt; +	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
</span>
I think this always ends up calling stage2_map_walker_try_leaf() (at
least it should). In that case, I think it might be clearer to do so, as
the intention is to just install a block.

<span class=q>&gt; +
&gt; +	mm_ops-&gt;put_page(ptep);
&gt; +	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
</span>
*old = *ptep;

<span class=q>&gt; +
&gt; +	return ret;
&gt;  }
&gt;  
&gt;  static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; @@ -793,13 +802,6 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  	kvm_pte_t *childp, pte = *ptep;
&gt;  	int ret;
&gt;  
&gt; -	if (data-&gt;anchor) {
&gt; -		if (stage2_pte_is_counted(pte))
&gt; -			mm_ops-&gt;put_page(ptep);
&gt; -
&gt; -		return 0;
&gt; -	}
&gt; -
&gt;  	ret = stage2_map_walker_try_leaf(addr, end, level, ptep, data);
&gt;  	if (ret != -E2BIG)
&gt;  		return ret;
&gt; @@ -828,50 +830,14 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  	return 0;
&gt;  }
&gt;  
&gt; -static int stage2_map_walk_table_post(u64 addr, u64 end, u32 level,
&gt; -				      kvm_pte_t *ptep,
&gt; -				      struct stage2_map_data *data)
&gt; -{
&gt; -	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; -	kvm_pte_t *childp;
&gt; -	int ret = 0;
&gt; -
&gt; -	if (!data-&gt;anchor)
&gt; -		return 0;
&gt; -
&gt; -	if (data-&gt;anchor == ptep) {
&gt; -		childp = data-&gt;childp;
&gt; -		data-&gt;anchor = NULL;
&gt; -		data-&gt;childp = NULL;
&gt; -		ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; -	} else {
&gt; -		childp = kvm_pte_follow(*ptep, mm_ops);
&gt; -	}
&gt; -
&gt; -	mm_ops-&gt;put_page(childp);
&gt; -	mm_ops-&gt;put_page(ptep);
&gt; -
&gt; -	return ret;
&gt; -}
&gt; -
&gt;  /*
&gt; - * This is a little fiddly, as we use all three of the walk flags. The idea
&gt; - * is that the TABLE_PRE callback runs for table entries on the way down,
&gt; - * looking for table entries which we could conceivably replace with a
&gt; - * block entry for this mapping. If it finds one, then it sets the 'anchor'
&gt; - * field in 'struct stage2_map_data' to point at the table entry, before
&gt; - * clearing the entry to zero and descending into the now detached table.
&gt; - *
&gt; - * The behaviour of the LEAF callback then depends on whether or not the
&gt; - * anchor has been set. If not, then we're not using a block mapping higher
&gt; - * up the table and we perform the mapping at the existing leaves instead.
&gt; - * If, on the other hand, the anchor _is_ set, then we drop references to
&gt; - * all valid leaves so that the pages beneath the anchor can be freed.
&gt; + * The TABLE_PRE callback runs for table entries on the way down, looking
&gt; + * for table entries which we could conceivably replace with a block entry
&gt; + * for this mapping. If it finds one it replaces the entry and calls
&gt; + * kvm_pgtable_mm_ops::free_removed_table() to tear down the detached table.
&gt;   *
&gt; - * Finally, the TABLE_POST callback does nothing if the anchor has not
&gt; - * been set, but otherwise frees the page-table pages while walking back up
&gt; - * the page-table, installing the block entry when it revisits the anchor
&gt; - * pointer and clearing the anchor to NULL.
&gt; + * Otherwise, the LEAF callback performs the mapping at the existing leaves
&gt; + * instead.
&gt;   */
&gt;  static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  			     enum kvm_pgtable_walk_flags flag, void * const arg)
&gt; @@ -883,11 +849,9 @@ static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  		return stage2_map_walk_table_pre(addr, end, level, ptep, data);
&gt;  	case KVM_PGTABLE_WALK_LEAF:
&gt;  		return stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; -	case KVM_PGTABLE_WALK_TABLE_POST:
&gt; -		return stage2_map_walk_table_post(addr, end, level, ptep, data);
&gt; +	default:
&gt; +		return -EINVAL;
</span>
nice!

<span class=q>&gt;  	}
&gt; -
&gt; -	return -EINVAL;
&gt;  }
&gt;  
&gt;  int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
&gt; @@ -905,8 +869,7 @@ int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
&gt;  	struct kvm_pgtable_walker walker = {
&gt;  		.cb		= stage2_map_walker,
&gt;  		.flags		= KVM_PGTABLE_WALK_TABLE_PRE |
&gt; -				  KVM_PGTABLE_WALK_LEAF |
&gt; -				  KVM_PGTABLE_WALK_TABLE_POST,
&gt; +				  KVM_PGTABLE_WALK_LEAF,
&gt;  		.arg		= &amp;map_data,
&gt;  	};
&gt;  
&gt; diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
&gt; index c9a13e487187..91521f4aab97 100644
&gt; --- a/arch/arm64/kvm/mmu.c
&gt; +++ b/arch/arm64/kvm/mmu.c
&gt; @@ -627,6 +627,7 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
&gt;  	.zalloc_page		= stage2_memcache_zalloc_page,
&gt;  	.zalloc_pages_exact	= kvm_host_zalloc_pages_exact,
&gt;  	.free_pages_exact	= free_pages_exact,
&gt; +	.free_removed_table	= kvm_pgtable_stage2_free_removed,
&gt;  	.get_page		= kvm_host_get_page,
&gt;  	.put_page		= kvm_host_put_page,
&gt;  	.page_count		= kvm_host_page_count,
&gt; -- 
&gt; 2.37.2.672.g94769d06f0-goog
&gt; 
</span>
_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m9afe0d55f1b5ee88addd9c7a3785153e6b774154 id=e9afe0d55f1b5ee88addd9c7a3785153e6b774154>^</a> <a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/t/#u><b>nested</b></a>] <a href=#r9afe0d55f1b5ee88addd9c7a3785153e6b774154>96+ messages in thread</a></pre><li><pre><a href=#e9afe0d55f1b5ee88addd9c7a3785153e6b774154 id=m9afe0d55f1b5ee88addd9c7a3785153e6b774154>*</a> <b>Re: [PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
<b>@ 2022-09-14  0:20     ` Ricardo Koller</b>
  <a href=#r9afe0d55f1b5ee88addd9c7a3785153e6b774154>0 siblings, 0 replies; 96+ messages in thread</a>
From: Ricardo Koller @ 2022-09-14  0:20 UTC (<a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: kvm, Marc Zyngier, linux-kernel, Ben Gardon, Catalin Marinas,
	David Matlack, Paolo Bonzini, Will Deacon, kvmarm,
	linux-arm-kernel

Hi Oliver,

On Tue, Aug 30, 2022 at 07:41:20PM +0000, Oliver Upton wrote:
<span class=q>&gt; The break-before-make sequence is a bit annoying as it opens a window
&gt; wherein memory is unmapped from the guest. KVM should replace the PTE
&gt; as quickly as possible and avoid unnecessary work in between.
&gt; 
&gt; Presently, the stage-2 map walker tears down a removed table before
&gt; installing a block mapping when coalescing a table into a block. As the
&gt; removed table is no longer visible to hardware walkers after the
&gt; DSB+TLBI, it is possible to move the remaining cleanup to happen after
&gt; installing the new PTE.
&gt; 
&gt; Reshuffle the stage-2 map walker to install the new block entry in
&gt; the pre-order callback. Unwire all of the teardown logic and replace
&gt; it with a call to kvm_pgtable_stage2_free_removed() after fixing
&gt; the PTE. The post-order visitor is now completely unnecessary, so drop
&gt; it. Finally, touch up the comments to better represent the now
&gt; simplified map walker.
&gt; 
&gt; Note that the call to tear down the unlinked stage-2 is indirected
&gt; as a subsequent change will use an RCU callback to trigger tear down.
&gt; RCU is not available to pKVM, so there is a need to use different
&gt; implementations on pKVM and non-pKVM VMs.
&gt; 
&gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; ---
&gt;  arch/arm64/include/asm/kvm_pgtable.h  |  3 +
&gt;  arch/arm64/kvm/hyp/nvhe/mem_protect.c |  1 +
&gt;  arch/arm64/kvm/hyp/pgtable.c          | 83 ++++++++-------------------
&gt;  arch/arm64/kvm/mmu.c                  |  1 +
&gt;  4 files changed, 28 insertions(+), 60 deletions(-)
&gt; 
&gt; diff --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
&gt; index d71fb92dc913..c25633f53b2b 100644
&gt; --- a/arch/arm64/include/asm/kvm_pgtable.h
&gt; +++ b/arch/arm64/include/asm/kvm_pgtable.h
&gt; @@ -77,6 +77,8 @@ static inline bool kvm_level_supports_block_mapping(u32 level)
&gt;   *				allocation is physically contiguous.
&gt;   * @free_pages_exact:		Free an exact number of memory pages previously
&gt;   *				allocated by zalloc_pages_exact.
&gt; + * @free_removed_table:		Free a removed paging structure by unlinking and
&gt; + *				dropping references.
&gt;   * @get_page:			Increment the refcount on a page.
&gt;   * @put_page:			Decrement the refcount on a page. When the
&gt;   *				refcount reaches 0 the page is automatically
&gt; @@ -95,6 +97,7 @@ struct kvm_pgtable_mm_ops {
&gt;  	void*		(*zalloc_page)(void *arg);
&gt;  	void*		(*zalloc_pages_exact)(size_t size);
&gt;  	void		(*free_pages_exact)(void *addr, size_t size);
&gt; +	void		(*free_removed_table)(void *addr, u32 level, void *arg);
&gt;  	void		(*get_page)(void *addr);
&gt;  	void		(*put_page)(void *addr);
&gt;  	int		(*page_count)(void *addr);
&gt; diff --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
&gt; index 1e78acf9662e..a930fdee6fce 100644
&gt; --- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c
&gt; +++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
&gt; @@ -93,6 +93,7 @@ static int prepare_s2_pool(void *pgt_pool_base)
&gt;  	host_kvm.mm_ops = (struct kvm_pgtable_mm_ops) {
&gt;  		.zalloc_pages_exact = host_s2_zalloc_pages_exact,
&gt;  		.zalloc_page = host_s2_zalloc_page,
&gt; +		.free_removed_table = kvm_pgtable_stage2_free_removed,
&gt;  		.phys_to_virt = hyp_phys_to_virt,
&gt;  		.virt_to_phys = hyp_virt_to_phys,
&gt;  		.page_count = hyp_page_count,
&gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; index d8127c25424c..5c0c8028d71c 100644
&gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; @@ -763,17 +763,21 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
&gt;  	return 0;
&gt;  }
&gt;  
&gt; +static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; +				struct stage2_map_data *data);
&gt; +
&gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  				     kvm_pte_t *ptep,
&gt;  				     struct stage2_map_data *data)
&gt;  {
&gt; -	if (data-&gt;anchor)
&gt; -		return 0;
&gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; +	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
&gt; +	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
&gt; +	int ret;
&gt;  
&gt;  	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
&gt;  		return 0;
&gt;  
&gt; -	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
&gt;  	kvm_clear_pte(ptep);
&gt;  
&gt;  	/*
&gt; @@ -782,8 +786,13 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  	 * individually.
&gt;  	 */
&gt;  	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt; -	data-&gt;anchor = ptep;
&gt; -	return 0;
&gt; +
&gt; +	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
</span>
I think this always ends up calling stage2_map_walker_try_leaf() (at
least it should). In that case, I think it might be clearer to do so, as
the intention is to just install a block.

<span class=q>&gt; +
&gt; +	mm_ops-&gt;put_page(ptep);
&gt; +	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
</span>
*old = *ptep;

<span class=q>&gt; +
&gt; +	return ret;
&gt;  }
&gt;  
&gt;  static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; @@ -793,13 +802,6 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  	kvm_pte_t *childp, pte = *ptep;
&gt;  	int ret;
&gt;  
&gt; -	if (data-&gt;anchor) {
&gt; -		if (stage2_pte_is_counted(pte))
&gt; -			mm_ops-&gt;put_page(ptep);
&gt; -
&gt; -		return 0;
&gt; -	}
&gt; -
&gt;  	ret = stage2_map_walker_try_leaf(addr, end, level, ptep, data);
&gt;  	if (ret != -E2BIG)
&gt;  		return ret;
&gt; @@ -828,50 +830,14 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  	return 0;
&gt;  }
&gt;  
&gt; -static int stage2_map_walk_table_post(u64 addr, u64 end, u32 level,
&gt; -				      kvm_pte_t *ptep,
&gt; -				      struct stage2_map_data *data)
&gt; -{
&gt; -	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; -	kvm_pte_t *childp;
&gt; -	int ret = 0;
&gt; -
&gt; -	if (!data-&gt;anchor)
&gt; -		return 0;
&gt; -
&gt; -	if (data-&gt;anchor == ptep) {
&gt; -		childp = data-&gt;childp;
&gt; -		data-&gt;anchor = NULL;
&gt; -		data-&gt;childp = NULL;
&gt; -		ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; -	} else {
&gt; -		childp = kvm_pte_follow(*ptep, mm_ops);
&gt; -	}
&gt; -
&gt; -	mm_ops-&gt;put_page(childp);
&gt; -	mm_ops-&gt;put_page(ptep);
&gt; -
&gt; -	return ret;
&gt; -}
&gt; -
&gt;  /*
&gt; - * This is a little fiddly, as we use all three of the walk flags. The idea
&gt; - * is that the TABLE_PRE callback runs for table entries on the way down,
&gt; - * looking for table entries which we could conceivably replace with a
&gt; - * block entry for this mapping. If it finds one, then it sets the 'anchor'
&gt; - * field in 'struct stage2_map_data' to point at the table entry, before
&gt; - * clearing the entry to zero and descending into the now detached table.
&gt; - *
&gt; - * The behaviour of the LEAF callback then depends on whether or not the
&gt; - * anchor has been set. If not, then we're not using a block mapping higher
&gt; - * up the table and we perform the mapping at the existing leaves instead.
&gt; - * If, on the other hand, the anchor _is_ set, then we drop references to
&gt; - * all valid leaves so that the pages beneath the anchor can be freed.
&gt; + * The TABLE_PRE callback runs for table entries on the way down, looking
&gt; + * for table entries which we could conceivably replace with a block entry
&gt; + * for this mapping. If it finds one it replaces the entry and calls
&gt; + * kvm_pgtable_mm_ops::free_removed_table() to tear down the detached table.
&gt;   *
&gt; - * Finally, the TABLE_POST callback does nothing if the anchor has not
&gt; - * been set, but otherwise frees the page-table pages while walking back up
&gt; - * the page-table, installing the block entry when it revisits the anchor
&gt; - * pointer and clearing the anchor to NULL.
&gt; + * Otherwise, the LEAF callback performs the mapping at the existing leaves
&gt; + * instead.
&gt;   */
&gt;  static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  			     enum kvm_pgtable_walk_flags flag, void * const arg)
&gt; @@ -883,11 +849,9 @@ static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  		return stage2_map_walk_table_pre(addr, end, level, ptep, data);
&gt;  	case KVM_PGTABLE_WALK_LEAF:
&gt;  		return stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; -	case KVM_PGTABLE_WALK_TABLE_POST:
&gt; -		return stage2_map_walk_table_post(addr, end, level, ptep, data);
&gt; +	default:
&gt; +		return -EINVAL;
</span>
nice!

<span class=q>&gt;  	}
&gt; -
&gt; -	return -EINVAL;
&gt;  }
&gt;  
&gt;  int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
&gt; @@ -905,8 +869,7 @@ int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
&gt;  	struct kvm_pgtable_walker walker = {
&gt;  		.cb		= stage2_map_walker,
&gt;  		.flags		= KVM_PGTABLE_WALK_TABLE_PRE |
&gt; -				  KVM_PGTABLE_WALK_LEAF |
&gt; -				  KVM_PGTABLE_WALK_TABLE_POST,
&gt; +				  KVM_PGTABLE_WALK_LEAF,
&gt;  		.arg		= &amp;map_data,
&gt;  	};
&gt;  
&gt; diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
&gt; index c9a13e487187..91521f4aab97 100644
&gt; --- a/arch/arm64/kvm/mmu.c
&gt; +++ b/arch/arm64/kvm/mmu.c
&gt; @@ -627,6 +627,7 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
&gt;  	.zalloc_page		= stage2_memcache_zalloc_page,
&gt;  	.zalloc_pages_exact	= kvm_host_zalloc_pages_exact,
&gt;  	.free_pages_exact	= free_pages_exact,
&gt; +	.free_removed_table	= kvm_pgtable_stage2_free_removed,
&gt;  	.get_page		= kvm_host_get_page,
&gt;  	.put_page		= kvm_host_put_page,
&gt;  	.page_count		= kvm_host_page_count,
&gt; -- 
&gt; 2.37.2.672.g94769d06f0-goog
&gt; 
</span>_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m9afe0d55f1b5ee88addd9c7a3785153e6b774154 id=e9afe0d55f1b5ee88addd9c7a3785153e6b774154>^</a> <a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YyEeOxDndbEVHuxE@google.com/t/#u><b>nested</b></a>] <a href=#r9afe0d55f1b5ee88addd9c7a3785153e6b774154>96+ messages in thread</a></pre><li><pre><a href=#ef2dd4c8a8568222f26d28c48d755f3b2d6a101d5 id=mf2dd4c8a8568222f26d28c48d755f3b2d6a101d5>*</a> <b>Re: [PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
  2022-09-14  0:20     ` <a href=#m9afe0d55f1b5ee88addd9c7a3785153e6b774154>Ricardo Koller</a>
  (?)
<b>@ 2022-10-10  3:58       ` Oliver Upton</b>
  <a href=#rf2dd4c8a8568222f26d28c48d755f3b2d6a101d5>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-10-10  3:58 UTC (<a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/raw>raw</a>)
  To: Ricardo Koller
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Reiji Watanabe,
	David Matlack, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

Hey Ricardo,

On Tue, Sep 13, 2022 at 05:20:11PM -0700, Ricardo Koller wrote:

[...]

<span class=q>&gt; &gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; index d8127c25424c..5c0c8028d71c 100644
&gt; &gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; @@ -763,17 +763,21 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
&gt; &gt;  	return 0;
&gt; &gt;  }
&gt; &gt;  
&gt; &gt; +static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; &gt; +				struct stage2_map_data *data);
&gt; &gt; +
&gt; &gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt; &gt;  				     kvm_pte_t *ptep,
&gt; &gt;  				     struct stage2_map_data *data)
&gt; &gt;  {
&gt; &gt; -	if (data-&gt;anchor)
&gt; &gt; -		return 0;
&gt; &gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; &gt; +	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
&gt; &gt; +	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
&gt; &gt; +	int ret;
&gt; &gt;  
&gt; &gt;  	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
&gt; &gt;  		return 0;
&gt; &gt;  
&gt; &gt; -	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
&gt; &gt;  	kvm_clear_pte(ptep);
&gt; &gt;  
&gt; &gt;  	/*
&gt; &gt; @@ -782,8 +786,13 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt; &gt;  	 * individually.
&gt; &gt;  	 */
&gt; &gt;  	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt; &gt; -	data-&gt;anchor = ptep;
&gt; &gt; -	return 0;
&gt; &gt; +
&gt; &gt; +	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; 
&gt; I think this always ends up calling stage2_map_walker_try_leaf() (at
&gt; least it should). In that case, I think it might be clearer to do so, as
&gt; the intention is to just install a block.
</span>
Yikes, I missed this in v2. I do agree with your point, it reads a bit
odd to call something that could reinstall a table.

Picked up the fix for v3. Thanks!

--
Best,
Oliver

<a href=#mf2dd4c8a8568222f26d28c48d755f3b2d6a101d5 id=ef2dd4c8a8568222f26d28c48d755f3b2d6a101d5>^</a> <a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/>permalink</a> <a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/t/#u><b>nested</b></a>] <a href=#rf2dd4c8a8568222f26d28c48d755f3b2d6a101d5>96+ messages in thread</a></pre><li><ul><li><pre><a href=#ef2dd4c8a8568222f26d28c48d755f3b2d6a101d5 id=mf2dd4c8a8568222f26d28c48d755f3b2d6a101d5>*</a> <b>Re: [PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
<b>@ 2022-10-10  3:58       ` Oliver Upton</b>
  <a href=#rf2dd4c8a8568222f26d28c48d755f3b2d6a101d5>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-10-10  3:58 UTC (<a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/raw>raw</a>)
  To: Ricardo Koller
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Reiji Watanabe,
	David Matlack, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

Hey Ricardo,

On Tue, Sep 13, 2022 at 05:20:11PM -0700, Ricardo Koller wrote:

[...]

<span class=q>&gt; &gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; index d8127c25424c..5c0c8028d71c 100644
&gt; &gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; @@ -763,17 +763,21 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
&gt; &gt;  	return 0;
&gt; &gt;  }
&gt; &gt;  
&gt; &gt; +static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; &gt; +				struct stage2_map_data *data);
&gt; &gt; +
&gt; &gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt; &gt;  				     kvm_pte_t *ptep,
&gt; &gt;  				     struct stage2_map_data *data)
&gt; &gt;  {
&gt; &gt; -	if (data-&gt;anchor)
&gt; &gt; -		return 0;
&gt; &gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; &gt; +	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
&gt; &gt; +	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
&gt; &gt; +	int ret;
&gt; &gt;  
&gt; &gt;  	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
&gt; &gt;  		return 0;
&gt; &gt;  
&gt; &gt; -	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
&gt; &gt;  	kvm_clear_pte(ptep);
&gt; &gt;  
&gt; &gt;  	/*
&gt; &gt; @@ -782,8 +786,13 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt; &gt;  	 * individually.
&gt; &gt;  	 */
&gt; &gt;  	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt; &gt; -	data-&gt;anchor = ptep;
&gt; &gt; -	return 0;
&gt; &gt; +
&gt; &gt; +	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; 
&gt; I think this always ends up calling stage2_map_walker_try_leaf() (at
&gt; least it should). In that case, I think it might be clearer to do so, as
&gt; the intention is to just install a block.
</span>
Yikes, I missed this in v2. I do agree with your point, it reads a bit
odd to call something that could reinstall a table.

Picked up the fix for v3. Thanks!

--
Best,
Oliver

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#mf2dd4c8a8568222f26d28c48d755f3b2d6a101d5 id=ef2dd4c8a8568222f26d28c48d755f3b2d6a101d5>^</a> <a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/>permalink</a> <a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/t/#u><b>nested</b></a>] <a href=#rf2dd4c8a8568222f26d28c48d755f3b2d6a101d5>96+ messages in thread</a></pre><li><pre><a href=#ef2dd4c8a8568222f26d28c48d755f3b2d6a101d5 id=mf2dd4c8a8568222f26d28c48d755f3b2d6a101d5>*</a> <b>Re: [PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</b>
<b>@ 2022-10-10  3:58       ` Oliver Upton</b>
  <a href=#rf2dd4c8a8568222f26d28c48d755f3b2d6a101d5>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-10-10  3:58 UTC (<a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/raw>raw</a>)
  To: Ricardo Koller
  Cc: kvm, Marc Zyngier, linux-kernel, Ben Gardon, Catalin Marinas,
	David Matlack, Paolo Bonzini, Will Deacon, kvmarm,
	linux-arm-kernel

Hey Ricardo,

On Tue, Sep 13, 2022 at 05:20:11PM -0700, Ricardo Koller wrote:

[...]

<span class=q>&gt; &gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; index d8127c25424c..5c0c8028d71c 100644
&gt; &gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; @@ -763,17 +763,21 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
&gt; &gt;  	return 0;
&gt; &gt;  }
&gt; &gt;  
&gt; &gt; +static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; &gt; +				struct stage2_map_data *data);
&gt; &gt; +
&gt; &gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt; &gt;  				     kvm_pte_t *ptep,
&gt; &gt;  				     struct stage2_map_data *data)
&gt; &gt;  {
&gt; &gt; -	if (data-&gt;anchor)
&gt; &gt; -		return 0;
&gt; &gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; &gt; +	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
&gt; &gt; +	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
&gt; &gt; +	int ret;
&gt; &gt;  
&gt; &gt;  	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
&gt; &gt;  		return 0;
&gt; &gt;  
&gt; &gt; -	data-&gt;childp = kvm_pte_follow(*ptep, data-&gt;mm_ops);
&gt; &gt;  	kvm_clear_pte(ptep);
&gt; &gt;  
&gt; &gt;  	/*
&gt; &gt; @@ -782,8 +786,13 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt; &gt;  	 * individually.
&gt; &gt;  	 */
&gt; &gt;  	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt; &gt; -	data-&gt;anchor = ptep;
&gt; &gt; -	return 0;
&gt; &gt; +
&gt; &gt; +	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; 
&gt; I think this always ends up calling stage2_map_walker_try_leaf() (at
&gt; least it should). In that case, I think it might be clearer to do so, as
&gt; the intention is to just install a block.
</span>
Yikes, I missed this in v2. I do agree with your point, it reads a bit
odd to call something that could reinstall a table.

Picked up the fix for v3. Thanks!

--
Best,
Oliver
_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#mf2dd4c8a8568222f26d28c48d755f3b2d6a101d5 id=ef2dd4c8a8568222f26d28c48d755f3b2d6a101d5>^</a> <a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/>permalink</a> <a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/Y0OYYZwoaW36UQK%2F@google.com/t/#u><b>nested</b></a>] <a href=#rf2dd4c8a8568222f26d28c48d755f3b2d6a101d5>96+ messages in thread</a></pre></ul></ul></ul><li><pre><a href=#e5c21368d47ad2cf5f94a28750f6278efd3a8e11a id=m5c21368d47ad2cf5f94a28750f6278efd3a8e11a>*</a> <b>[PATCH 03/14] KVM: arm64: Directly read owner id field in stage2_pte_is_counted()</b>
  2022-08-30 19:41 ` <a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb>Oliver Upton</a>
  (?)
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r5c21368d47ad2cf5f94a28750f6278efd3a8e11a>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-arm-kernel, kvmarm, kvm, linux-kernel

A subsequent change to KVM will make use of additional bits in invalid
ptes. Prepare for said change by explicitly checking the valid bit and
owner fields in stage2_pte_is_counted()

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-4-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-4-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 7 ++++++-
 1 file <a href=#e5c21368d47ad2cf5f94a28750f6278efd3a8e11a>changed</a>, 6 insertions(+), 1 deletion(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-4-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-4-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 5c0c8028d71c..b6ce786ae570 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -172,6 +172,11 @@ static kvm_pte_t kvm_init_invalid_leaf_owner(u8 owner_id)
</span> 	return FIELD_PREP(KVM_INVALID_PTE_OWNER_MASK, owner_id);
 }
 
<span class=add>+static u8 kvm_invalid_pte_owner(kvm_pte_t pte)
+{
+	return FIELD_GET(KVM_INVALID_PTE_OWNER_MASK, pte);
+}
+
</span> static int kvm_pgtable_visitor_cb(struct kvm_pgtable_walk_data *data, u64 addr,
 				  u32 level, kvm_pte_t *ptep,
 				  enum kvm_pgtable_walk_flags flag)
<span class=hunk>@@ -679,7 +684,7 @@ static bool stage2_pte_is_counted(kvm_pte_t pte)
</span> 	 * encode ownership of a page to another entity than the page-table
 	 * owner, whose id is 0.
 	 */
<span class=del>-	return !!pte;
</span><span class=add>+	return kvm_pte_valid(pte) || kvm_invalid_pte_owner(pte);
</span> }
 
 static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_s2_mmu *mmu, u64 addr,
-- 
2.37.2.672.g94769d06f0-goog


<a href=#m5c21368d47ad2cf5f94a28750f6278efd3a8e11a id=e5c21368d47ad2cf5f94a28750f6278efd3a8e11a>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r5c21368d47ad2cf5f94a28750f6278efd3a8e11a>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e5c21368d47ad2cf5f94a28750f6278efd3a8e11a id=m5c21368d47ad2cf5f94a28750f6278efd3a8e11a>*</a> <b>[PATCH 03/14] KVM: arm64: Directly read owner id field in stage2_pte_is_counted()</b>
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r5c21368d47ad2cf5f94a28750f6278efd3a8e11a>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-arm-kernel, kvmarm, kvm, linux-kernel

A subsequent change to KVM will make use of additional bits in invalid
ptes. Prepare for said change by explicitly checking the valid bit and
owner fields in stage2_pte_is_counted()

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-4-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-4-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 7 ++++++-
 1 file <a href=#e5c21368d47ad2cf5f94a28750f6278efd3a8e11a>changed</a>, 6 insertions(+), 1 deletion(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-4-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-4-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 5c0c8028d71c..b6ce786ae570 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -172,6 +172,11 @@ static kvm_pte_t kvm_init_invalid_leaf_owner(u8 owner_id)
</span> 	return FIELD_PREP(KVM_INVALID_PTE_OWNER_MASK, owner_id);
 }
 
<span class=add>+static u8 kvm_invalid_pte_owner(kvm_pte_t pte)
+{
+	return FIELD_GET(KVM_INVALID_PTE_OWNER_MASK, pte);
+}
+
</span> static int kvm_pgtable_visitor_cb(struct kvm_pgtable_walk_data *data, u64 addr,
 				  u32 level, kvm_pte_t *ptep,
 				  enum kvm_pgtable_walk_flags flag)
<span class=hunk>@@ -679,7 +684,7 @@ static bool stage2_pte_is_counted(kvm_pte_t pte)
</span> 	 * encode ownership of a page to another entity than the page-table
 	 * owner, whose id is 0.
 	 */
<span class=del>-	return !!pte;
</span><span class=add>+	return kvm_pte_valid(pte) || kvm_invalid_pte_owner(pte);
</span> }
 
 static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_s2_mmu *mmu, u64 addr,
-- 
2.37.2.672.g94769d06f0-goog


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m5c21368d47ad2cf5f94a28750f6278efd3a8e11a id=e5c21368d47ad2cf5f94a28750f6278efd3a8e11a>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r5c21368d47ad2cf5f94a28750f6278efd3a8e11a>96+ messages in thread</a></pre><li><pre><a href=#e5c21368d47ad2cf5f94a28750f6278efd3a8e11a id=m5c21368d47ad2cf5f94a28750f6278efd3a8e11a>*</a> <b>[PATCH 03/14] KVM: arm64: Directly read owner id field in stage2_pte_is_counted()</b>
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r5c21368d47ad2cf5f94a28750f6278efd3a8e11a>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-kernel, kvmarm, linux-arm-kernel, kvm

A subsequent change to KVM will make use of additional bits in invalid
ptes. Prepare for said change by explicitly checking the valid bit and
owner fields in stage2_pte_is_counted()

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-4-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-4-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 7 ++++++-
 1 file <a href=#e5c21368d47ad2cf5f94a28750f6278efd3a8e11a>changed</a>, 6 insertions(+), 1 deletion(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-4-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-4-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 5c0c8028d71c..b6ce786ae570 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -172,6 +172,11 @@ static kvm_pte_t kvm_init_invalid_leaf_owner(u8 owner_id)
</span> 	return FIELD_PREP(KVM_INVALID_PTE_OWNER_MASK, owner_id);
 }
 
<span class=add>+static u8 kvm_invalid_pte_owner(kvm_pte_t pte)
+{
+	return FIELD_GET(KVM_INVALID_PTE_OWNER_MASK, pte);
+}
+
</span> static int kvm_pgtable_visitor_cb(struct kvm_pgtable_walk_data *data, u64 addr,
 				  u32 level, kvm_pte_t *ptep,
 				  enum kvm_pgtable_walk_flags flag)
<span class=hunk>@@ -679,7 +684,7 @@ static bool stage2_pte_is_counted(kvm_pte_t pte)
</span> 	 * encode ownership of a page to another entity than the page-table
 	 * owner, whose id is 0.
 	 */
<span class=del>-	return !!pte;
</span><span class=add>+	return kvm_pte_valid(pte) || kvm_invalid_pte_owner(pte);
</span> }
 
 static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_s2_mmu *mmu, u64 addr,
-- 
2.37.2.672.g94769d06f0-goog

_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m5c21368d47ad2cf5f94a28750f6278efd3a8e11a id=e5c21368d47ad2cf5f94a28750f6278efd3a8e11a>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-4-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r5c21368d47ad2cf5f94a28750f6278efd3a8e11a>96+ messages in thread</a></pre></ul><li><pre><a href=#e87296201935175ac920837e7c079530409989146 id=m87296201935175ac920837e7c079530409989146>*</a> <b>[PATCH 04/14] KVM: arm64: Read the PTE once per visit</b>
  2022-08-30 19:41 ` <a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb>Oliver Upton</a>
  (?)
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r87296201935175ac920837e7c079530409989146>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-arm-kernel, kvmarm, kvm, linux-kernel

The page table walkers read the PTE multiple times per visit. Presently,
that is safe as changes to the non-leaf PTEs are serialized. A
subsequent change to KVM will enable parallel modifications to the stage
2 page tables. Prepare by ensuring a PTE is read only once per visit.

Promote the PTE read in __kvm_pgtable_visit() to READ_ONCE() and pass
the observed value through to callbacks. Note that the PTE is passed as
a pointer to the callbacks; visitors that install new tables need to aim
traversal at the new table.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h href=#Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>arch/arm64/include/asm/kvm_pgtable.h</a>  |  8 ++-
 <a id=iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c href=#Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c>arch/arm64/kvm/hyp/nvhe/mem_protect.c</a> |  4 +-
 <a id=iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:setup.c href=#Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:setup.c>arch/arm64/kvm/hyp/nvhe/setup.c</a>       |  4 +-
 <a id=iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a>          | 73 ++++++++++++++-------------
 4 files <a href=#e87296201935175ac920837e7c079530409989146>changed</a>, 48 insertions(+), 41 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h id=Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>diff</a> --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index c25633f53b2b..47920ae3f7e7 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -195,7 +195,7 @@ enum kvm_pgtable_walk_flags {
</span> };
 
 typedef int (*kvm_pgtable_visitor_fn_t)(u64 addr, u64 end, u32 level,
<span class=del>-					kvm_pte_t *ptep,
</span><span class=add>+					kvm_pte_t *ptep, kvm_pte_t *old,
</span> 					enum kvm_pgtable_walk_flags flag,
 					void * const arg);
 
<span class=hunk>@@ -561,4 +561,10 @@ enum kvm_pgtable_prot kvm_pgtable_stage2_pte_prot(kvm_pte_t pte);
</span>  *	   kvm_pgtable_prot format.
  */
 enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte);
<span class=add>+
+static inline kvm_pte_t kvm_pte_read(kvm_pte_t *ptep)
+{
+	return READ_ONCE(*ptep);
+}
+
</span> #endif	/* __ARM64_KVM_PGTABLE_H__ */
<span class=head><a href=#iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c id=Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c>diff</a> --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
index a930fdee6fce..61cf223e0796 100644
--- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c
+++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
</span><span class=hunk>@@ -419,12 +419,12 @@ struct check_walk_data {
</span> };
 
 static int __check_page_state_visitor(u64 addr, u64 end, u32 level,
<span class=del>-				      kvm_pte_t *ptep,
</span><span class=add>+				      kvm_pte_t *ptep, kvm_pte_t *old,
</span> 				      enum kvm_pgtable_walk_flags flag,
 				      void * const arg)
 {
 	struct check_walk_data *d = arg;
<span class=del>-	kvm_pte_t pte = *ptep;
</span><span class=add>+	kvm_pte_t pte = *old;
</span> 
 	if (kvm_pte_valid(pte) &amp;&amp; !addr_is_memory(kvm_pte_to_phys(pte)))
 		return -EINVAL;
<span class=head><a href=#iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:setup.c id=Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:setup.c>diff</a> --git a/arch/arm64/kvm/hyp/nvhe/setup.c b/arch/arm64/kvm/hyp/nvhe/setup.c
index e8d4ea2fcfa0..2b62ca58ebd4 100644
--- a/arch/arm64/kvm/hyp/nvhe/setup.c
+++ b/arch/arm64/kvm/hyp/nvhe/setup.c
</span><span class=hunk>@@ -187,14 +187,14 @@ static void hpool_put_page(void *addr)
</span> }
 
 static int finalize_host_mappings_walker(u64 addr, u64 end, u32 level,
<span class=del>-					 kvm_pte_t *ptep,
</span><span class=add>+					 kvm_pte_t *ptep, kvm_pte_t *old,
</span> 					 enum kvm_pgtable_walk_flags flag,
 					 void * const arg)
 {
 	struct kvm_pgtable_mm_ops *mm_ops = arg;
 	enum kvm_pgtable_prot prot;
 	enum pkvm_page_state state;
<span class=del>-	kvm_pte_t pte = *ptep;
</span><span class=add>+	kvm_pte_t pte = *old;
</span> 	phys_addr_t phys;
 
 	if (!kvm_pte_valid(pte))
<span class=head><a href=#iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index b6ce786ae570..430753fbb727 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -178,11 +178,11 @@ static u8 kvm_invalid_pte_owner(kvm_pte_t pte)
</span> }
 
 static int kvm_pgtable_visitor_cb(struct kvm_pgtable_walk_data *data, u64 addr,
<span class=del>-				  u32 level, kvm_pte_t *ptep,
</span><span class=add>+				  u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
</span> 				  enum kvm_pgtable_walk_flags flag)
 {
 	struct kvm_pgtable_walker *walker = data-&gt;walker;
<span class=del>-	return walker-&gt;cb(addr, data-&gt;end, level, ptep, flag, walker-&gt;arg);
</span><span class=add>+	return walker-&gt;cb(addr, data-&gt;end, level, ptep, old, flag, walker-&gt;arg);
</span> }
 
 static int __kvm_pgtable_walk(struct kvm_pgtable_walk_data *data,
<span class=hunk>@@ -193,17 +193,17 @@ static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
</span> {
 	int ret = 0;
 	u64 addr = data-&gt;addr;
<span class=del>-	kvm_pte_t *childp, pte = *ptep;
</span><span class=add>+	kvm_pte_t *childp, pte = kvm_pte_read(ptep);
</span> 	bool table = kvm_pte_table(pte, level);
 	enum kvm_pgtable_walk_flags flags = data-&gt;walker-&gt;flags;
 
 	if (table &amp;&amp; (flags &amp; KVM_PGTABLE_WALK_TABLE_PRE)) {
<span class=del>-		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep,
</span><span class=add>+		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep, &amp;pte,
</span> 					     KVM_PGTABLE_WALK_TABLE_PRE);
 	}
 
 	if (!table &amp;&amp; (flags &amp; KVM_PGTABLE_WALK_LEAF)) {
<span class=del>-		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep,
</span><span class=add>+		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep, &amp;pte,
</span> 					     KVM_PGTABLE_WALK_LEAF);
 		pte = *ptep;
 		table = kvm_pte_table(pte, level);
<span class=hunk>@@ -224,7 +224,7 @@ static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
</span> 		goto out;
 
 	if (flags &amp; KVM_PGTABLE_WALK_TABLE_POST) {
<span class=del>-		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep,
</span><span class=add>+		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep, &amp;pte,
</span> 					     KVM_PGTABLE_WALK_TABLE_POST);
 	}
 
<span class=hunk>@@ -297,12 +297,12 @@ struct leaf_walk_data {
</span> 	u32		level;
 };
 
<span class=del>-static int leaf_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span><span class=add>+static int leaf_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
</span> 		       enum kvm_pgtable_walk_flags flag, void * const arg)
 {
 	struct leaf_walk_data *data = arg;
 
<span class=del>-	data-&gt;pte   = *ptep;
</span><span class=add>+	data-&gt;pte   = *old;
</span> 	data-&gt;level = level;
 
 	return 0;
<span class=hunk>@@ -388,10 +388,10 @@ enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte)
</span> 	return prot;
 }
 
<span class=del>-static bool hyp_map_walker_try_leaf(u64 addr, u64 end, u32 level,
-				    kvm_pte_t *ptep, struct hyp_map_data *data)
</span><span class=add>+static bool hyp_map_walker_try_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
+				    kvm_pte_t old, struct hyp_map_data *data)
</span> {
<span class=del>-	kvm_pte_t new, old = *ptep;
</span><span class=add>+	kvm_pte_t new;
</span> 	u64 granule = kvm_granule_size(level), phys = data-&gt;phys;
 
 	if (!kvm_block_mapping_supported(addr, end, phys, level))
<span class=hunk>@@ -410,14 +410,14 @@ static bool hyp_map_walker_try_leaf(u64 addr, u64 end, u32 level,
</span> 	return true;
 }
 
<span class=del>-static int hyp_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span><span class=add>+static int hyp_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
</span> 			  enum kvm_pgtable_walk_flags flag, void * const arg)
 {
 	kvm_pte_t *childp;
 	struct hyp_map_data *data = arg;
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
 
<span class=del>-	if (hyp_map_walker_try_leaf(addr, end, level, ptep, arg))
</span><span class=add>+	if (hyp_map_walker_try_leaf(addr, end, level, ptep, *old, arg))
</span> 		return 0;
 
 	if (WARN_ON(level == KVM_PGTABLE_MAX_LEVELS - 1))
<span class=hunk>@@ -461,10 +461,10 @@ struct hyp_unmap_data {
</span> 	struct kvm_pgtable_mm_ops	*mm_ops;
 };
 
<span class=del>-static int hyp_unmap_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span><span class=add>+static int hyp_unmap_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
</span> 			    enum kvm_pgtable_walk_flags flag, void * const arg)
 {
<span class=del>-	kvm_pte_t pte = *ptep, *childp = NULL;
</span><span class=add>+	kvm_pte_t pte = *old, *childp = NULL;
</span> 	u64 granule = kvm_granule_size(level);
 	struct hyp_unmap_data *data = arg;
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
<span class=hunk>@@ -537,11 +537,11 @@ int kvm_pgtable_hyp_init(struct kvm_pgtable *pgt, u32 va_bits,
</span> 	return 0;
 }
 
<span class=del>-static int hyp_free_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span><span class=add>+static int hyp_free_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
</span> 			   enum kvm_pgtable_walk_flags flag, void * const arg)
 {
 	struct kvm_pgtable_mm_ops *mm_ops = arg;
<span class=del>-	kvm_pte_t pte = *ptep;
</span><span class=add>+	kvm_pte_t pte = *old;
</span> 
 	if (!kvm_pte_valid(pte))
 		return 0;
<span class=hunk>@@ -723,10 +723,10 @@ static bool stage2_leaf_mapping_allowed(u64 addr, u64 end, u32 level,
</span> }
 
 static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
<span class=del>-				      kvm_pte_t *ptep,
</span><span class=add>+				      kvm_pte_t *ptep, kvm_pte_t old,
</span> 				      struct stage2_map_data *data)
 {
<span class=del>-	kvm_pte_t new, old = *ptep;
</span><span class=add>+	kvm_pte_t new;
</span> 	u64 granule = kvm_granule_size(level), phys = data-&gt;phys;
 	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
<span class=hunk>@@ -772,11 +772,11 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 				struct stage2_map_data *data);
 
 static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
<span class=del>-				     kvm_pte_t *ptep,
</span><span class=add>+				     kvm_pte_t *ptep, kvm_pte_t *old,
</span> 				     struct stage2_map_data *data)
 {
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
<span class=del>-	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
</span><span class=add>+	kvm_pte_t *childp = kvm_pte_follow(*old, mm_ops);
</span> 	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
 	int ret;
 
<span class=hunk>@@ -801,13 +801,14 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
</span> }
 
 static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=del>-				struct stage2_map_data *data)
</span><span class=add>+				kvm_pte_t *old, struct stage2_map_data *data)
</span> {
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
<span class=del>-	kvm_pte_t *childp, pte = *ptep;
</span><span class=add>+	kvm_pte_t *childp, pte = *old;
</span> 	int ret;
 
<span class=del>-	ret = stage2_map_walker_try_leaf(addr, end, level, ptep, data);
</span><span class=add>+	ret = stage2_map_walker_try_leaf(addr, end, level, ptep, pte, data);
+
</span> 	if (ret != -E2BIG)
 		return ret;
 
<span class=hunk>@@ -844,16 +845,16 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span>  * Otherwise, the LEAF callback performs the mapping at the existing leaves
  * instead.
  */
<span class=del>-static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span><span class=add>+static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
</span> 			     enum kvm_pgtable_walk_flags flag, void * const arg)
 {
 	struct stage2_map_data *data = arg;
 
 	switch (flag) {
 	case KVM_PGTABLE_WALK_TABLE_PRE:
<span class=del>-		return stage2_map_walk_table_pre(addr, end, level, ptep, data);
</span><span class=add>+		return stage2_map_walk_table_pre(addr, end, level, ptep, old, data);
</span> 	case KVM_PGTABLE_WALK_LEAF:
<span class=del>-		return stage2_map_walk_leaf(addr, end, level, ptep, data);
</span><span class=add>+		return stage2_map_walk_leaf(addr, end, level, ptep, old, data);
</span> 	default:
 		return -EINVAL;
 	}
<span class=hunk>@@ -918,13 +919,13 @@ int kvm_pgtable_stage2_set_owner(struct kvm_pgtable *pgt, u64 addr, u64 size,
</span> }
 
 static int stage2_unmap_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=del>-			       enum kvm_pgtable_walk_flags flag,
</span><span class=add>+			       kvm_pte_t *old, enum kvm_pgtable_walk_flags flag,
</span> 			       void * const arg)
 {
 	struct kvm_pgtable *pgt = arg;
 	struct kvm_s2_mmu *mmu = pgt-&gt;mmu;
 	struct kvm_pgtable_mm_ops *mm_ops = pgt-&gt;mm_ops;
<span class=del>-	kvm_pte_t pte = *ptep, *childp = NULL;
</span><span class=add>+	kvm_pte_t pte = *old, *childp = NULL;
</span> 	bool need_flush = false;
 
 	if (!kvm_pte_valid(pte)) {
<span class=hunk>@@ -981,10 +982,10 @@ struct stage2_attr_data {
</span> };
 
 static int stage2_attr_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=del>-			      enum kvm_pgtable_walk_flags flag,
</span><span class=add>+			      kvm_pte_t *old, enum kvm_pgtable_walk_flags flag,
</span> 			      void * const arg)
 {
<span class=del>-	kvm_pte_t pte = *ptep;
</span><span class=add>+	kvm_pte_t pte = *old;
</span> 	struct stage2_attr_data *data = arg;
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
 
<span class=hunk>@@ -1007,7 +1008,7 @@ static int stage2_attr_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 		 * stage-2 PTE if we are going to add executable permission.
 		 */
 		if (mm_ops-&gt;icache_inval_pou &amp;&amp;
<span class=del>-		    stage2_pte_executable(pte) &amp;&amp; !stage2_pte_executable(*ptep))
</span><span class=add>+		    stage2_pte_executable(pte) &amp;&amp; !stage2_pte_executable(data-&gt;pte))
</span> 			mm_ops-&gt;icache_inval_pou(kvm_pte_follow(pte, mm_ops),
 						  kvm_granule_size(level));
 		WRITE_ONCE(*ptep, pte);
<span class=hunk>@@ -1109,12 +1110,12 @@ int kvm_pgtable_stage2_relax_perms(struct kvm_pgtable *pgt, u64 addr,
</span> }
 
 static int stage2_flush_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=del>-			       enum kvm_pgtable_walk_flags flag,
</span><span class=add>+			       kvm_pte_t *old, enum kvm_pgtable_walk_flags flag,
</span> 			       void * const arg)
 {
 	struct kvm_pgtable *pgt = arg;
 	struct kvm_pgtable_mm_ops *mm_ops = pgt-&gt;mm_ops;
<span class=del>-	kvm_pte_t pte = *ptep;
</span><span class=add>+	kvm_pte_t pte = *old;
</span> 
 	if (!kvm_pte_valid(pte) || !stage2_pte_cacheable(pgt, pte))
 		return 0;
<span class=hunk>@@ -1169,11 +1170,11 @@ int __kvm_pgtable_stage2_init(struct kvm_pgtable *pgt, struct kvm_s2_mmu *mmu,
</span> }
 
 static int stage2_free_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=del>-			      enum kvm_pgtable_walk_flags flag,
</span><span class=add>+			      kvm_pte_t *old, enum kvm_pgtable_walk_flags flag,
</span> 			      void * const arg)
 {
 	struct kvm_pgtable_mm_ops *mm_ops = arg;
<span class=del>-	kvm_pte_t pte = *ptep;
</span><span class=add>+	kvm_pte_t pte = *old;
</span> 
 	if (!stage2_pte_is_counted(pte))
 		return 0;
-- 
2.37.2.672.g94769d06f0-goog


<a href=#m87296201935175ac920837e7c079530409989146 id=e87296201935175ac920837e7c079530409989146>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r87296201935175ac920837e7c079530409989146>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e87296201935175ac920837e7c079530409989146 id=m87296201935175ac920837e7c079530409989146>*</a> <b>[PATCH 04/14] KVM: arm64: Read the PTE once per visit</b>
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r87296201935175ac920837e7c079530409989146>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-arm-kernel, kvmarm, kvm, linux-kernel

The page table walkers read the PTE multiple times per visit. Presently,
that is safe as changes to the non-leaf PTEs are serialized. A
subsequent change to KVM will enable parallel modifications to the stage
2 page tables. Prepare by ensuring a PTE is read only once per visit.

Promote the PTE read in __kvm_pgtable_visit() to READ_ONCE() and pass
the observed value through to callbacks. Note that the PTE is passed as
a pointer to the callbacks; visitors that install new tables need to aim
traversal at the new table.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h href=#Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>arch/arm64/include/asm/kvm_pgtable.h</a>  |  8 ++-
 <a id=iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c href=#Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c>arch/arm64/kvm/hyp/nvhe/mem_protect.c</a> |  4 +-
 <a id=iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:setup.c href=#Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:setup.c>arch/arm64/kvm/hyp/nvhe/setup.c</a>       |  4 +-
 <a id=iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a>          | 73 ++++++++++++++-------------
 4 files <a href=#e87296201935175ac920837e7c079530409989146>changed</a>, 48 insertions(+), 41 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h id=Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>diff</a> --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index c25633f53b2b..47920ae3f7e7 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -195,7 +195,7 @@ enum kvm_pgtable_walk_flags {
</span> };
 
 typedef int (*kvm_pgtable_visitor_fn_t)(u64 addr, u64 end, u32 level,
<span class=del>-					kvm_pte_t *ptep,
</span><span class=add>+					kvm_pte_t *ptep, kvm_pte_t *old,
</span> 					enum kvm_pgtable_walk_flags flag,
 					void * const arg);
 
<span class=hunk>@@ -561,4 +561,10 @@ enum kvm_pgtable_prot kvm_pgtable_stage2_pte_prot(kvm_pte_t pte);
</span>  *	   kvm_pgtable_prot format.
  */
 enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte);
<span class=add>+
+static inline kvm_pte_t kvm_pte_read(kvm_pte_t *ptep)
+{
+	return READ_ONCE(*ptep);
+}
+
</span> #endif	/* __ARM64_KVM_PGTABLE_H__ */
<span class=head><a href=#iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c id=Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c>diff</a> --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
index a930fdee6fce..61cf223e0796 100644
--- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c
+++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
</span><span class=hunk>@@ -419,12 +419,12 @@ struct check_walk_data {
</span> };
 
 static int __check_page_state_visitor(u64 addr, u64 end, u32 level,
<span class=del>-				      kvm_pte_t *ptep,
</span><span class=add>+				      kvm_pte_t *ptep, kvm_pte_t *old,
</span> 				      enum kvm_pgtable_walk_flags flag,
 				      void * const arg)
 {
 	struct check_walk_data *d = arg;
<span class=del>-	kvm_pte_t pte = *ptep;
</span><span class=add>+	kvm_pte_t pte = *old;
</span> 
 	if (kvm_pte_valid(pte) &amp;&amp; !addr_is_memory(kvm_pte_to_phys(pte)))
 		return -EINVAL;
<span class=head><a href=#iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:setup.c id=Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:setup.c>diff</a> --git a/arch/arm64/kvm/hyp/nvhe/setup.c b/arch/arm64/kvm/hyp/nvhe/setup.c
index e8d4ea2fcfa0..2b62ca58ebd4 100644
--- a/arch/arm64/kvm/hyp/nvhe/setup.c
+++ b/arch/arm64/kvm/hyp/nvhe/setup.c
</span><span class=hunk>@@ -187,14 +187,14 @@ static void hpool_put_page(void *addr)
</span> }
 
 static int finalize_host_mappings_walker(u64 addr, u64 end, u32 level,
<span class=del>-					 kvm_pte_t *ptep,
</span><span class=add>+					 kvm_pte_t *ptep, kvm_pte_t *old,
</span> 					 enum kvm_pgtable_walk_flags flag,
 					 void * const arg)
 {
 	struct kvm_pgtable_mm_ops *mm_ops = arg;
 	enum kvm_pgtable_prot prot;
 	enum pkvm_page_state state;
<span class=del>-	kvm_pte_t pte = *ptep;
</span><span class=add>+	kvm_pte_t pte = *old;
</span> 	phys_addr_t phys;
 
 	if (!kvm_pte_valid(pte))
<span class=head><a href=#iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index b6ce786ae570..430753fbb727 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -178,11 +178,11 @@ static u8 kvm_invalid_pte_owner(kvm_pte_t pte)
</span> }
 
 static int kvm_pgtable_visitor_cb(struct kvm_pgtable_walk_data *data, u64 addr,
<span class=del>-				  u32 level, kvm_pte_t *ptep,
</span><span class=add>+				  u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
</span> 				  enum kvm_pgtable_walk_flags flag)
 {
 	struct kvm_pgtable_walker *walker = data-&gt;walker;
<span class=del>-	return walker-&gt;cb(addr, data-&gt;end, level, ptep, flag, walker-&gt;arg);
</span><span class=add>+	return walker-&gt;cb(addr, data-&gt;end, level, ptep, old, flag, walker-&gt;arg);
</span> }
 
 static int __kvm_pgtable_walk(struct kvm_pgtable_walk_data *data,
<span class=hunk>@@ -193,17 +193,17 @@ static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
</span> {
 	int ret = 0;
 	u64 addr = data-&gt;addr;
<span class=del>-	kvm_pte_t *childp, pte = *ptep;
</span><span class=add>+	kvm_pte_t *childp, pte = kvm_pte_read(ptep);
</span> 	bool table = kvm_pte_table(pte, level);
 	enum kvm_pgtable_walk_flags flags = data-&gt;walker-&gt;flags;
 
 	if (table &amp;&amp; (flags &amp; KVM_PGTABLE_WALK_TABLE_PRE)) {
<span class=del>-		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep,
</span><span class=add>+		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep, &amp;pte,
</span> 					     KVM_PGTABLE_WALK_TABLE_PRE);
 	}
 
 	if (!table &amp;&amp; (flags &amp; KVM_PGTABLE_WALK_LEAF)) {
<span class=del>-		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep,
</span><span class=add>+		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep, &amp;pte,
</span> 					     KVM_PGTABLE_WALK_LEAF);
 		pte = *ptep;
 		table = kvm_pte_table(pte, level);
<span class=hunk>@@ -224,7 +224,7 @@ static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
</span> 		goto out;
 
 	if (flags &amp; KVM_PGTABLE_WALK_TABLE_POST) {
<span class=del>-		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep,
</span><span class=add>+		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep, &amp;pte,
</span> 					     KVM_PGTABLE_WALK_TABLE_POST);
 	}
 
<span class=hunk>@@ -297,12 +297,12 @@ struct leaf_walk_data {
</span> 	u32		level;
 };
 
<span class=del>-static int leaf_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span><span class=add>+static int leaf_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
</span> 		       enum kvm_pgtable_walk_flags flag, void * const arg)
 {
 	struct leaf_walk_data *data = arg;
 
<span class=del>-	data-&gt;pte   = *ptep;
</span><span class=add>+	data-&gt;pte   = *old;
</span> 	data-&gt;level = level;
 
 	return 0;
<span class=hunk>@@ -388,10 +388,10 @@ enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte)
</span> 	return prot;
 }
 
<span class=del>-static bool hyp_map_walker_try_leaf(u64 addr, u64 end, u32 level,
-				    kvm_pte_t *ptep, struct hyp_map_data *data)
</span><span class=add>+static bool hyp_map_walker_try_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
+				    kvm_pte_t old, struct hyp_map_data *data)
</span> {
<span class=del>-	kvm_pte_t new, old = *ptep;
</span><span class=add>+	kvm_pte_t new;
</span> 	u64 granule = kvm_granule_size(level), phys = data-&gt;phys;
 
 	if (!kvm_block_mapping_supported(addr, end, phys, level))
<span class=hunk>@@ -410,14 +410,14 @@ static bool hyp_map_walker_try_leaf(u64 addr, u64 end, u32 level,
</span> 	return true;
 }
 
<span class=del>-static int hyp_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span><span class=add>+static int hyp_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
</span> 			  enum kvm_pgtable_walk_flags flag, void * const arg)
 {
 	kvm_pte_t *childp;
 	struct hyp_map_data *data = arg;
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
 
<span class=del>-	if (hyp_map_walker_try_leaf(addr, end, level, ptep, arg))
</span><span class=add>+	if (hyp_map_walker_try_leaf(addr, end, level, ptep, *old, arg))
</span> 		return 0;
 
 	if (WARN_ON(level == KVM_PGTABLE_MAX_LEVELS - 1))
<span class=hunk>@@ -461,10 +461,10 @@ struct hyp_unmap_data {
</span> 	struct kvm_pgtable_mm_ops	*mm_ops;
 };
 
<span class=del>-static int hyp_unmap_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span><span class=add>+static int hyp_unmap_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
</span> 			    enum kvm_pgtable_walk_flags flag, void * const arg)
 {
<span class=del>-	kvm_pte_t pte = *ptep, *childp = NULL;
</span><span class=add>+	kvm_pte_t pte = *old, *childp = NULL;
</span> 	u64 granule = kvm_granule_size(level);
 	struct hyp_unmap_data *data = arg;
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
<span class=hunk>@@ -537,11 +537,11 @@ int kvm_pgtable_hyp_init(struct kvm_pgtable *pgt, u32 va_bits,
</span> 	return 0;
 }
 
<span class=del>-static int hyp_free_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span><span class=add>+static int hyp_free_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
</span> 			   enum kvm_pgtable_walk_flags flag, void * const arg)
 {
 	struct kvm_pgtable_mm_ops *mm_ops = arg;
<span class=del>-	kvm_pte_t pte = *ptep;
</span><span class=add>+	kvm_pte_t pte = *old;
</span> 
 	if (!kvm_pte_valid(pte))
 		return 0;
<span class=hunk>@@ -723,10 +723,10 @@ static bool stage2_leaf_mapping_allowed(u64 addr, u64 end, u32 level,
</span> }
 
 static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
<span class=del>-				      kvm_pte_t *ptep,
</span><span class=add>+				      kvm_pte_t *ptep, kvm_pte_t old,
</span> 				      struct stage2_map_data *data)
 {
<span class=del>-	kvm_pte_t new, old = *ptep;
</span><span class=add>+	kvm_pte_t new;
</span> 	u64 granule = kvm_granule_size(level), phys = data-&gt;phys;
 	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
<span class=hunk>@@ -772,11 +772,11 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 				struct stage2_map_data *data);
 
 static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
<span class=del>-				     kvm_pte_t *ptep,
</span><span class=add>+				     kvm_pte_t *ptep, kvm_pte_t *old,
</span> 				     struct stage2_map_data *data)
 {
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
<span class=del>-	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
</span><span class=add>+	kvm_pte_t *childp = kvm_pte_follow(*old, mm_ops);
</span> 	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
 	int ret;
 
<span class=hunk>@@ -801,13 +801,14 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
</span> }
 
 static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=del>-				struct stage2_map_data *data)
</span><span class=add>+				kvm_pte_t *old, struct stage2_map_data *data)
</span> {
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
<span class=del>-	kvm_pte_t *childp, pte = *ptep;
</span><span class=add>+	kvm_pte_t *childp, pte = *old;
</span> 	int ret;
 
<span class=del>-	ret = stage2_map_walker_try_leaf(addr, end, level, ptep, data);
</span><span class=add>+	ret = stage2_map_walker_try_leaf(addr, end, level, ptep, pte, data);
+
</span> 	if (ret != -E2BIG)
 		return ret;
 
<span class=hunk>@@ -844,16 +845,16 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span>  * Otherwise, the LEAF callback performs the mapping at the existing leaves
  * instead.
  */
<span class=del>-static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span><span class=add>+static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
</span> 			     enum kvm_pgtable_walk_flags flag, void * const arg)
 {
 	struct stage2_map_data *data = arg;
 
 	switch (flag) {
 	case KVM_PGTABLE_WALK_TABLE_PRE:
<span class=del>-		return stage2_map_walk_table_pre(addr, end, level, ptep, data);
</span><span class=add>+		return stage2_map_walk_table_pre(addr, end, level, ptep, old, data);
</span> 	case KVM_PGTABLE_WALK_LEAF:
<span class=del>-		return stage2_map_walk_leaf(addr, end, level, ptep, data);
</span><span class=add>+		return stage2_map_walk_leaf(addr, end, level, ptep, old, data);
</span> 	default:
 		return -EINVAL;
 	}
<span class=hunk>@@ -918,13 +919,13 @@ int kvm_pgtable_stage2_set_owner(struct kvm_pgtable *pgt, u64 addr, u64 size,
</span> }
 
 static int stage2_unmap_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=del>-			       enum kvm_pgtable_walk_flags flag,
</span><span class=add>+			       kvm_pte_t *old, enum kvm_pgtable_walk_flags flag,
</span> 			       void * const arg)
 {
 	struct kvm_pgtable *pgt = arg;
 	struct kvm_s2_mmu *mmu = pgt-&gt;mmu;
 	struct kvm_pgtable_mm_ops *mm_ops = pgt-&gt;mm_ops;
<span class=del>-	kvm_pte_t pte = *ptep, *childp = NULL;
</span><span class=add>+	kvm_pte_t pte = *old, *childp = NULL;
</span> 	bool need_flush = false;
 
 	if (!kvm_pte_valid(pte)) {
<span class=hunk>@@ -981,10 +982,10 @@ struct stage2_attr_data {
</span> };
 
 static int stage2_attr_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=del>-			      enum kvm_pgtable_walk_flags flag,
</span><span class=add>+			      kvm_pte_t *old, enum kvm_pgtable_walk_flags flag,
</span> 			      void * const arg)
 {
<span class=del>-	kvm_pte_t pte = *ptep;
</span><span class=add>+	kvm_pte_t pte = *old;
</span> 	struct stage2_attr_data *data = arg;
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
 
<span class=hunk>@@ -1007,7 +1008,7 @@ static int stage2_attr_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 		 * stage-2 PTE if we are going to add executable permission.
 		 */
 		if (mm_ops-&gt;icache_inval_pou &amp;&amp;
<span class=del>-		    stage2_pte_executable(pte) &amp;&amp; !stage2_pte_executable(*ptep))
</span><span class=add>+		    stage2_pte_executable(pte) &amp;&amp; !stage2_pte_executable(data-&gt;pte))
</span> 			mm_ops-&gt;icache_inval_pou(kvm_pte_follow(pte, mm_ops),
 						  kvm_granule_size(level));
 		WRITE_ONCE(*ptep, pte);
<span class=hunk>@@ -1109,12 +1110,12 @@ int kvm_pgtable_stage2_relax_perms(struct kvm_pgtable *pgt, u64 addr,
</span> }
 
 static int stage2_flush_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=del>-			       enum kvm_pgtable_walk_flags flag,
</span><span class=add>+			       kvm_pte_t *old, enum kvm_pgtable_walk_flags flag,
</span> 			       void * const arg)
 {
 	struct kvm_pgtable *pgt = arg;
 	struct kvm_pgtable_mm_ops *mm_ops = pgt-&gt;mm_ops;
<span class=del>-	kvm_pte_t pte = *ptep;
</span><span class=add>+	kvm_pte_t pte = *old;
</span> 
 	if (!kvm_pte_valid(pte) || !stage2_pte_cacheable(pgt, pte))
 		return 0;
<span class=hunk>@@ -1169,11 +1170,11 @@ int __kvm_pgtable_stage2_init(struct kvm_pgtable *pgt, struct kvm_s2_mmu *mmu,
</span> }
 
 static int stage2_free_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=del>-			      enum kvm_pgtable_walk_flags flag,
</span><span class=add>+			      kvm_pte_t *old, enum kvm_pgtable_walk_flags flag,
</span> 			      void * const arg)
 {
 	struct kvm_pgtable_mm_ops *mm_ops = arg;
<span class=del>-	kvm_pte_t pte = *ptep;
</span><span class=add>+	kvm_pte_t pte = *old;
</span> 
 	if (!stage2_pte_is_counted(pte))
 		return 0;
-- 
2.37.2.672.g94769d06f0-goog


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m87296201935175ac920837e7c079530409989146 id=e87296201935175ac920837e7c079530409989146>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r87296201935175ac920837e7c079530409989146>96+ messages in thread</a></pre><li><pre><a href=#e87296201935175ac920837e7c079530409989146 id=m87296201935175ac920837e7c079530409989146>*</a> <b>[PATCH 04/14] KVM: arm64: Read the PTE once per visit</b>
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r87296201935175ac920837e7c079530409989146>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-kernel, kvmarm, linux-arm-kernel, kvm

The page table walkers read the PTE multiple times per visit. Presently,
that is safe as changes to the non-leaf PTEs are serialized. A
subsequent change to KVM will enable parallel modifications to the stage
2 page tables. Prepare by ensuring a PTE is read only once per visit.

Promote the PTE read in __kvm_pgtable_visit() to READ_ONCE() and pass
the observed value through to callbacks. Note that the PTE is passed as
a pointer to the callbacks; visitors that install new tables need to aim
traversal at the new table.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h href=#Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>arch/arm64/include/asm/kvm_pgtable.h</a>  |  8 ++-
 <a id=iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c href=#Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c>arch/arm64/kvm/hyp/nvhe/mem_protect.c</a> |  4 +-
 <a id=iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:setup.c href=#Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:setup.c>arch/arm64/kvm/hyp/nvhe/setup.c</a>       |  4 +-
 <a id=iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a>          | 73 ++++++++++++++-------------
 4 files <a href=#e87296201935175ac920837e7c079530409989146>changed</a>, 48 insertions(+), 41 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h id=Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>diff</a> --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index c25633f53b2b..47920ae3f7e7 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -195,7 +195,7 @@ enum kvm_pgtable_walk_flags {
</span> };
 
 typedef int (*kvm_pgtable_visitor_fn_t)(u64 addr, u64 end, u32 level,
<span class=del>-					kvm_pte_t *ptep,
</span><span class=add>+					kvm_pte_t *ptep, kvm_pte_t *old,
</span> 					enum kvm_pgtable_walk_flags flag,
 					void * const arg);
 
<span class=hunk>@@ -561,4 +561,10 @@ enum kvm_pgtable_prot kvm_pgtable_stage2_pte_prot(kvm_pte_t pte);
</span>  *	   kvm_pgtable_prot format.
  */
 enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte);
<span class=add>+
+static inline kvm_pte_t kvm_pte_read(kvm_pte_t *ptep)
+{
+	return READ_ONCE(*ptep);
+}
+
</span> #endif	/* __ARM64_KVM_PGTABLE_H__ */
<span class=head><a href=#iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c id=Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c>diff</a> --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
index a930fdee6fce..61cf223e0796 100644
--- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c
+++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
</span><span class=hunk>@@ -419,12 +419,12 @@ struct check_walk_data {
</span> };
 
 static int __check_page_state_visitor(u64 addr, u64 end, u32 level,
<span class=del>-				      kvm_pte_t *ptep,
</span><span class=add>+				      kvm_pte_t *ptep, kvm_pte_t *old,
</span> 				      enum kvm_pgtable_walk_flags flag,
 				      void * const arg)
 {
 	struct check_walk_data *d = arg;
<span class=del>-	kvm_pte_t pte = *ptep;
</span><span class=add>+	kvm_pte_t pte = *old;
</span> 
 	if (kvm_pte_valid(pte) &amp;&amp; !addr_is_memory(kvm_pte_to_phys(pte)))
 		return -EINVAL;
<span class=head><a href=#iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:setup.c id=Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:setup.c>diff</a> --git a/arch/arm64/kvm/hyp/nvhe/setup.c b/arch/arm64/kvm/hyp/nvhe/setup.c
index e8d4ea2fcfa0..2b62ca58ebd4 100644
--- a/arch/arm64/kvm/hyp/nvhe/setup.c
+++ b/arch/arm64/kvm/hyp/nvhe/setup.c
</span><span class=hunk>@@ -187,14 +187,14 @@ static void hpool_put_page(void *addr)
</span> }
 
 static int finalize_host_mappings_walker(u64 addr, u64 end, u32 level,
<span class=del>-					 kvm_pte_t *ptep,
</span><span class=add>+					 kvm_pte_t *ptep, kvm_pte_t *old,
</span> 					 enum kvm_pgtable_walk_flags flag,
 					 void * const arg)
 {
 	struct kvm_pgtable_mm_ops *mm_ops = arg;
 	enum kvm_pgtable_prot prot;
 	enum pkvm_page_state state;
<span class=del>-	kvm_pte_t pte = *ptep;
</span><span class=add>+	kvm_pte_t pte = *old;
</span> 	phys_addr_t phys;
 
 	if (!kvm_pte_valid(pte))
<span class=head><a href=#iZ2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-5-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index b6ce786ae570..430753fbb727 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -178,11 +178,11 @@ static u8 kvm_invalid_pte_owner(kvm_pte_t pte)
</span> }
 
 static int kvm_pgtable_visitor_cb(struct kvm_pgtable_walk_data *data, u64 addr,
<span class=del>-				  u32 level, kvm_pte_t *ptep,
</span><span class=add>+				  u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
</span> 				  enum kvm_pgtable_walk_flags flag)
 {
 	struct kvm_pgtable_walker *walker = data-&gt;walker;
<span class=del>-	return walker-&gt;cb(addr, data-&gt;end, level, ptep, flag, walker-&gt;arg);
</span><span class=add>+	return walker-&gt;cb(addr, data-&gt;end, level, ptep, old, flag, walker-&gt;arg);
</span> }
 
 static int __kvm_pgtable_walk(struct kvm_pgtable_walk_data *data,
<span class=hunk>@@ -193,17 +193,17 @@ static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
</span> {
 	int ret = 0;
 	u64 addr = data-&gt;addr;
<span class=del>-	kvm_pte_t *childp, pte = *ptep;
</span><span class=add>+	kvm_pte_t *childp, pte = kvm_pte_read(ptep);
</span> 	bool table = kvm_pte_table(pte, level);
 	enum kvm_pgtable_walk_flags flags = data-&gt;walker-&gt;flags;
 
 	if (table &amp;&amp; (flags &amp; KVM_PGTABLE_WALK_TABLE_PRE)) {
<span class=del>-		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep,
</span><span class=add>+		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep, &amp;pte,
</span> 					     KVM_PGTABLE_WALK_TABLE_PRE);
 	}
 
 	if (!table &amp;&amp; (flags &amp; KVM_PGTABLE_WALK_LEAF)) {
<span class=del>-		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep,
</span><span class=add>+		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep, &amp;pte,
</span> 					     KVM_PGTABLE_WALK_LEAF);
 		pte = *ptep;
 		table = kvm_pte_table(pte, level);
<span class=hunk>@@ -224,7 +224,7 @@ static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
</span> 		goto out;
 
 	if (flags &amp; KVM_PGTABLE_WALK_TABLE_POST) {
<span class=del>-		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep,
</span><span class=add>+		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep, &amp;pte,
</span> 					     KVM_PGTABLE_WALK_TABLE_POST);
 	}
 
<span class=hunk>@@ -297,12 +297,12 @@ struct leaf_walk_data {
</span> 	u32		level;
 };
 
<span class=del>-static int leaf_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span><span class=add>+static int leaf_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
</span> 		       enum kvm_pgtable_walk_flags flag, void * const arg)
 {
 	struct leaf_walk_data *data = arg;
 
<span class=del>-	data-&gt;pte   = *ptep;
</span><span class=add>+	data-&gt;pte   = *old;
</span> 	data-&gt;level = level;
 
 	return 0;
<span class=hunk>@@ -388,10 +388,10 @@ enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte)
</span> 	return prot;
 }
 
<span class=del>-static bool hyp_map_walker_try_leaf(u64 addr, u64 end, u32 level,
-				    kvm_pte_t *ptep, struct hyp_map_data *data)
</span><span class=add>+static bool hyp_map_walker_try_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
+				    kvm_pte_t old, struct hyp_map_data *data)
</span> {
<span class=del>-	kvm_pte_t new, old = *ptep;
</span><span class=add>+	kvm_pte_t new;
</span> 	u64 granule = kvm_granule_size(level), phys = data-&gt;phys;
 
 	if (!kvm_block_mapping_supported(addr, end, phys, level))
<span class=hunk>@@ -410,14 +410,14 @@ static bool hyp_map_walker_try_leaf(u64 addr, u64 end, u32 level,
</span> 	return true;
 }
 
<span class=del>-static int hyp_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span><span class=add>+static int hyp_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
</span> 			  enum kvm_pgtable_walk_flags flag, void * const arg)
 {
 	kvm_pte_t *childp;
 	struct hyp_map_data *data = arg;
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
 
<span class=del>-	if (hyp_map_walker_try_leaf(addr, end, level, ptep, arg))
</span><span class=add>+	if (hyp_map_walker_try_leaf(addr, end, level, ptep, *old, arg))
</span> 		return 0;
 
 	if (WARN_ON(level == KVM_PGTABLE_MAX_LEVELS - 1))
<span class=hunk>@@ -461,10 +461,10 @@ struct hyp_unmap_data {
</span> 	struct kvm_pgtable_mm_ops	*mm_ops;
 };
 
<span class=del>-static int hyp_unmap_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span><span class=add>+static int hyp_unmap_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
</span> 			    enum kvm_pgtable_walk_flags flag, void * const arg)
 {
<span class=del>-	kvm_pte_t pte = *ptep, *childp = NULL;
</span><span class=add>+	kvm_pte_t pte = *old, *childp = NULL;
</span> 	u64 granule = kvm_granule_size(level);
 	struct hyp_unmap_data *data = arg;
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
<span class=hunk>@@ -537,11 +537,11 @@ int kvm_pgtable_hyp_init(struct kvm_pgtable *pgt, u32 va_bits,
</span> 	return 0;
 }
 
<span class=del>-static int hyp_free_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span><span class=add>+static int hyp_free_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
</span> 			   enum kvm_pgtable_walk_flags flag, void * const arg)
 {
 	struct kvm_pgtable_mm_ops *mm_ops = arg;
<span class=del>-	kvm_pte_t pte = *ptep;
</span><span class=add>+	kvm_pte_t pte = *old;
</span> 
 	if (!kvm_pte_valid(pte))
 		return 0;
<span class=hunk>@@ -723,10 +723,10 @@ static bool stage2_leaf_mapping_allowed(u64 addr, u64 end, u32 level,
</span> }
 
 static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
<span class=del>-				      kvm_pte_t *ptep,
</span><span class=add>+				      kvm_pte_t *ptep, kvm_pte_t old,
</span> 				      struct stage2_map_data *data)
 {
<span class=del>-	kvm_pte_t new, old = *ptep;
</span><span class=add>+	kvm_pte_t new;
</span> 	u64 granule = kvm_granule_size(level), phys = data-&gt;phys;
 	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
<span class=hunk>@@ -772,11 +772,11 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 				struct stage2_map_data *data);
 
 static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
<span class=del>-				     kvm_pte_t *ptep,
</span><span class=add>+				     kvm_pte_t *ptep, kvm_pte_t *old,
</span> 				     struct stage2_map_data *data)
 {
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
<span class=del>-	kvm_pte_t *childp = kvm_pte_follow(*ptep, mm_ops);
</span><span class=add>+	kvm_pte_t *childp = kvm_pte_follow(*old, mm_ops);
</span> 	struct kvm_pgtable *pgt = data-&gt;mmu-&gt;pgt;
 	int ret;
 
<span class=hunk>@@ -801,13 +801,14 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
</span> }
 
 static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=del>-				struct stage2_map_data *data)
</span><span class=add>+				kvm_pte_t *old, struct stage2_map_data *data)
</span> {
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
<span class=del>-	kvm_pte_t *childp, pte = *ptep;
</span><span class=add>+	kvm_pte_t *childp, pte = *old;
</span> 	int ret;
 
<span class=del>-	ret = stage2_map_walker_try_leaf(addr, end, level, ptep, data);
</span><span class=add>+	ret = stage2_map_walker_try_leaf(addr, end, level, ptep, pte, data);
+
</span> 	if (ret != -E2BIG)
 		return ret;
 
<span class=hunk>@@ -844,16 +845,16 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span>  * Otherwise, the LEAF callback performs the mapping at the existing leaves
  * instead.
  */
<span class=del>-static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span><span class=add>+static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
</span> 			     enum kvm_pgtable_walk_flags flag, void * const arg)
 {
 	struct stage2_map_data *data = arg;
 
 	switch (flag) {
 	case KVM_PGTABLE_WALK_TABLE_PRE:
<span class=del>-		return stage2_map_walk_table_pre(addr, end, level, ptep, data);
</span><span class=add>+		return stage2_map_walk_table_pre(addr, end, level, ptep, old, data);
</span> 	case KVM_PGTABLE_WALK_LEAF:
<span class=del>-		return stage2_map_walk_leaf(addr, end, level, ptep, data);
</span><span class=add>+		return stage2_map_walk_leaf(addr, end, level, ptep, old, data);
</span> 	default:
 		return -EINVAL;
 	}
<span class=hunk>@@ -918,13 +919,13 @@ int kvm_pgtable_stage2_set_owner(struct kvm_pgtable *pgt, u64 addr, u64 size,
</span> }
 
 static int stage2_unmap_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=del>-			       enum kvm_pgtable_walk_flags flag,
</span><span class=add>+			       kvm_pte_t *old, enum kvm_pgtable_walk_flags flag,
</span> 			       void * const arg)
 {
 	struct kvm_pgtable *pgt = arg;
 	struct kvm_s2_mmu *mmu = pgt-&gt;mmu;
 	struct kvm_pgtable_mm_ops *mm_ops = pgt-&gt;mm_ops;
<span class=del>-	kvm_pte_t pte = *ptep, *childp = NULL;
</span><span class=add>+	kvm_pte_t pte = *old, *childp = NULL;
</span> 	bool need_flush = false;
 
 	if (!kvm_pte_valid(pte)) {
<span class=hunk>@@ -981,10 +982,10 @@ struct stage2_attr_data {
</span> };
 
 static int stage2_attr_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=del>-			      enum kvm_pgtable_walk_flags flag,
</span><span class=add>+			      kvm_pte_t *old, enum kvm_pgtable_walk_flags flag,
</span> 			      void * const arg)
 {
<span class=del>-	kvm_pte_t pte = *ptep;
</span><span class=add>+	kvm_pte_t pte = *old;
</span> 	struct stage2_attr_data *data = arg;
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
 
<span class=hunk>@@ -1007,7 +1008,7 @@ static int stage2_attr_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 		 * stage-2 PTE if we are going to add executable permission.
 		 */
 		if (mm_ops-&gt;icache_inval_pou &amp;&amp;
<span class=del>-		    stage2_pte_executable(pte) &amp;&amp; !stage2_pte_executable(*ptep))
</span><span class=add>+		    stage2_pte_executable(pte) &amp;&amp; !stage2_pte_executable(data-&gt;pte))
</span> 			mm_ops-&gt;icache_inval_pou(kvm_pte_follow(pte, mm_ops),
 						  kvm_granule_size(level));
 		WRITE_ONCE(*ptep, pte);
<span class=hunk>@@ -1109,12 +1110,12 @@ int kvm_pgtable_stage2_relax_perms(struct kvm_pgtable *pgt, u64 addr,
</span> }
 
 static int stage2_flush_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=del>-			       enum kvm_pgtable_walk_flags flag,
</span><span class=add>+			       kvm_pte_t *old, enum kvm_pgtable_walk_flags flag,
</span> 			       void * const arg)
 {
 	struct kvm_pgtable *pgt = arg;
 	struct kvm_pgtable_mm_ops *mm_ops = pgt-&gt;mm_ops;
<span class=del>-	kvm_pte_t pte = *ptep;
</span><span class=add>+	kvm_pte_t pte = *old;
</span> 
 	if (!kvm_pte_valid(pte) || !stage2_pte_cacheable(pgt, pte))
 		return 0;
<span class=hunk>@@ -1169,11 +1170,11 @@ int __kvm_pgtable_stage2_init(struct kvm_pgtable *pgt, struct kvm_s2_mmu *mmu,
</span> }
 
 static int stage2_free_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=del>-			      enum kvm_pgtable_walk_flags flag,
</span><span class=add>+			      kvm_pte_t *old, enum kvm_pgtable_walk_flags flag,
</span> 			      void * const arg)
 {
 	struct kvm_pgtable_mm_ops *mm_ops = arg;
<span class=del>-	kvm_pte_t pte = *ptep;
</span><span class=add>+	kvm_pte_t pte = *old;
</span> 
 	if (!stage2_pte_is_counted(pte))
 		return 0;
-- 
2.37.2.672.g94769d06f0-goog

_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m87296201935175ac920837e7c079530409989146 id=e87296201935175ac920837e7c079530409989146>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-5-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r87296201935175ac920837e7c079530409989146>96+ messages in thread</a></pre></ul><li><pre><a href=#e0c83aa5eb3183e4da038ba8b166c16ec0318c6af id=m0c83aa5eb3183e4da038ba8b166c16ec0318c6af>*</a> <b>[PATCH 05/14] KVM: arm64: Split init and set for table PTE</b>
  2022-08-30 19:41 ` <a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb>Oliver Upton</a>
  (?)
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r0c83aa5eb3183e4da038ba8b166c16ec0318c6af>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-arm-kernel, kvmarm, kvm, linux-kernel

Create a helper to initialize a stage-2 table and directly call
smp_store_release() to install it.

A subsequent change to KVM will tweak the way we traverse the page
tables, requiring that the visitor callbacks steer the walker down a
newly installed table. Furthermore, when stage-2 faults are serviced
in parallel the PTE must be considered volatile, so walkers will need
to stash a pointer to the new table.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-6-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-6-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 20 ++++++++++----------
 1 file <a href=#e0c83aa5eb3183e4da038ba8b166c16ec0318c6af>changed</a>, 10 insertions(+), 10 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-6-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-6-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 430753fbb727..331f6e3b2c20 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -142,16 +142,13 @@ static void kvm_clear_pte(kvm_pte_t *ptep)
</span> 	WRITE_ONCE(*ptep, 0);
 }
 
<span class=del>-static void kvm_set_table_pte(kvm_pte_t *ptep, kvm_pte_t *childp,
-			      struct kvm_pgtable_mm_ops *mm_ops)
</span><span class=add>+static kvm_pte_t kvm_init_table_pte(kvm_pte_t *childp, struct kvm_pgtable_mm_ops *mm_ops)
</span> {
<span class=del>-	kvm_pte_t old = *ptep, pte = kvm_phys_to_pte(mm_ops-&gt;virt_to_phys(childp));
</span><span class=add>+	kvm_pte_t pte = kvm_phys_to_pte(mm_ops-&gt;virt_to_phys(childp));
</span> 
 	pte |= FIELD_PREP(KVM_PTE_TYPE, KVM_PTE_TYPE_TABLE);
 	pte |= KVM_PTE_VALID;
<span class=del>-
-	WARN_ON(kvm_pte_valid(old));
-	smp_store_release(ptep, pte);
</span><span class=add>+	return pte;
</span> }
 
 static kvm_pte_t kvm_init_valid_leaf_pte(u64 pa, kvm_pte_t attr, u32 level)
<span class=hunk>@@ -413,7 +410,7 @@ static bool hyp_map_walker_try_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *pte
</span> static int hyp_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
 			  enum kvm_pgtable_walk_flags flag, void * const arg)
 {
<span class=del>-	kvm_pte_t *childp;
</span><span class=add>+	kvm_pte_t *childp, new;
</span> 	struct hyp_map_data *data = arg;
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
 
<span class=hunk>@@ -427,8 +424,10 @@ static int hyp_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte
</span> 	if (!childp)
 		return -ENOMEM;
 
<span class=del>-	kvm_set_table_pte(ptep, childp, mm_ops);
</span><span class=add>+	new = kvm_init_table_pte(childp, mm_ops);
</span> 	mm_ops-&gt;get_page(ptep);
<span class=add>+	smp_store_release(ptep, new);
+
</span> 	return 0;
 }
 
<span class=hunk>@@ -804,7 +803,7 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 				kvm_pte_t *old, struct stage2_map_data *data)
 {
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
<span class=del>-	kvm_pte_t *childp, pte = *old;
</span><span class=add>+	kvm_pte_t *childp, pte = *old, new;
</span> 	int ret;
 
 	ret = stage2_map_walker_try_leaf(addr, end, level, ptep, pte, data);
<span class=hunk>@@ -830,8 +829,9 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 	if (stage2_pte_is_counted(pte))
 		stage2_put_pte(ptep, data-&gt;mmu, addr, level, mm_ops);
 
<span class=del>-	kvm_set_table_pte(ptep, childp, mm_ops);
</span><span class=add>+	new = kvm_init_table_pte(childp, mm_ops);
</span> 	mm_ops-&gt;get_page(ptep);
<span class=add>+	smp_store_release(ptep, new);
</span> 
 	return 0;
 }
-- 
2.37.2.672.g94769d06f0-goog


<a href=#m0c83aa5eb3183e4da038ba8b166c16ec0318c6af id=e0c83aa5eb3183e4da038ba8b166c16ec0318c6af>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r0c83aa5eb3183e4da038ba8b166c16ec0318c6af>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e0c83aa5eb3183e4da038ba8b166c16ec0318c6af id=m0c83aa5eb3183e4da038ba8b166c16ec0318c6af>*</a> <b>[PATCH 05/14] KVM: arm64: Split init and set for table PTE</b>
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r0c83aa5eb3183e4da038ba8b166c16ec0318c6af>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-arm-kernel, kvmarm, kvm, linux-kernel

Create a helper to initialize a stage-2 table and directly call
smp_store_release() to install it.

A subsequent change to KVM will tweak the way we traverse the page
tables, requiring that the visitor callbacks steer the walker down a
newly installed table. Furthermore, when stage-2 faults are serviced
in parallel the PTE must be considered volatile, so walkers will need
to stash a pointer to the new table.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-6-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-6-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 20 ++++++++++----------
 1 file <a href=#e0c83aa5eb3183e4da038ba8b166c16ec0318c6af>changed</a>, 10 insertions(+), 10 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-6-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-6-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 430753fbb727..331f6e3b2c20 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -142,16 +142,13 @@ static void kvm_clear_pte(kvm_pte_t *ptep)
</span> 	WRITE_ONCE(*ptep, 0);
 }
 
<span class=del>-static void kvm_set_table_pte(kvm_pte_t *ptep, kvm_pte_t *childp,
-			      struct kvm_pgtable_mm_ops *mm_ops)
</span><span class=add>+static kvm_pte_t kvm_init_table_pte(kvm_pte_t *childp, struct kvm_pgtable_mm_ops *mm_ops)
</span> {
<span class=del>-	kvm_pte_t old = *ptep, pte = kvm_phys_to_pte(mm_ops-&gt;virt_to_phys(childp));
</span><span class=add>+	kvm_pte_t pte = kvm_phys_to_pte(mm_ops-&gt;virt_to_phys(childp));
</span> 
 	pte |= FIELD_PREP(KVM_PTE_TYPE, KVM_PTE_TYPE_TABLE);
 	pte |= KVM_PTE_VALID;
<span class=del>-
-	WARN_ON(kvm_pte_valid(old));
-	smp_store_release(ptep, pte);
</span><span class=add>+	return pte;
</span> }
 
 static kvm_pte_t kvm_init_valid_leaf_pte(u64 pa, kvm_pte_t attr, u32 level)
<span class=hunk>@@ -413,7 +410,7 @@ static bool hyp_map_walker_try_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *pte
</span> static int hyp_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
 			  enum kvm_pgtable_walk_flags flag, void * const arg)
 {
<span class=del>-	kvm_pte_t *childp;
</span><span class=add>+	kvm_pte_t *childp, new;
</span> 	struct hyp_map_data *data = arg;
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
 
<span class=hunk>@@ -427,8 +424,10 @@ static int hyp_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte
</span> 	if (!childp)
 		return -ENOMEM;
 
<span class=del>-	kvm_set_table_pte(ptep, childp, mm_ops);
</span><span class=add>+	new = kvm_init_table_pte(childp, mm_ops);
</span> 	mm_ops-&gt;get_page(ptep);
<span class=add>+	smp_store_release(ptep, new);
+
</span> 	return 0;
 }
 
<span class=hunk>@@ -804,7 +803,7 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 				kvm_pte_t *old, struct stage2_map_data *data)
 {
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
<span class=del>-	kvm_pte_t *childp, pte = *old;
</span><span class=add>+	kvm_pte_t *childp, pte = *old, new;
</span> 	int ret;
 
 	ret = stage2_map_walker_try_leaf(addr, end, level, ptep, pte, data);
<span class=hunk>@@ -830,8 +829,9 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 	if (stage2_pte_is_counted(pte))
 		stage2_put_pte(ptep, data-&gt;mmu, addr, level, mm_ops);
 
<span class=del>-	kvm_set_table_pte(ptep, childp, mm_ops);
</span><span class=add>+	new = kvm_init_table_pte(childp, mm_ops);
</span> 	mm_ops-&gt;get_page(ptep);
<span class=add>+	smp_store_release(ptep, new);
</span> 
 	return 0;
 }
-- 
2.37.2.672.g94769d06f0-goog


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m0c83aa5eb3183e4da038ba8b166c16ec0318c6af id=e0c83aa5eb3183e4da038ba8b166c16ec0318c6af>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r0c83aa5eb3183e4da038ba8b166c16ec0318c6af>96+ messages in thread</a></pre><li><pre><a href=#e0c83aa5eb3183e4da038ba8b166c16ec0318c6af id=m0c83aa5eb3183e4da038ba8b166c16ec0318c6af>*</a> <b>[PATCH 05/14] KVM: arm64: Split init and set for table PTE</b>
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r0c83aa5eb3183e4da038ba8b166c16ec0318c6af>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-kernel, kvmarm, linux-arm-kernel, kvm

Create a helper to initialize a stage-2 table and directly call
smp_store_release() to install it.

A subsequent change to KVM will tweak the way we traverse the page
tables, requiring that the visitor callbacks steer the walker down a
newly installed table. Furthermore, when stage-2 faults are serviced
in parallel the PTE must be considered volatile, so walkers will need
to stash a pointer to the new table.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-6-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-6-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 20 ++++++++++----------
 1 file <a href=#e0c83aa5eb3183e4da038ba8b166c16ec0318c6af>changed</a>, 10 insertions(+), 10 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-6-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-6-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 430753fbb727..331f6e3b2c20 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -142,16 +142,13 @@ static void kvm_clear_pte(kvm_pte_t *ptep)
</span> 	WRITE_ONCE(*ptep, 0);
 }
 
<span class=del>-static void kvm_set_table_pte(kvm_pte_t *ptep, kvm_pte_t *childp,
-			      struct kvm_pgtable_mm_ops *mm_ops)
</span><span class=add>+static kvm_pte_t kvm_init_table_pte(kvm_pte_t *childp, struct kvm_pgtable_mm_ops *mm_ops)
</span> {
<span class=del>-	kvm_pte_t old = *ptep, pte = kvm_phys_to_pte(mm_ops-&gt;virt_to_phys(childp));
</span><span class=add>+	kvm_pte_t pte = kvm_phys_to_pte(mm_ops-&gt;virt_to_phys(childp));
</span> 
 	pte |= FIELD_PREP(KVM_PTE_TYPE, KVM_PTE_TYPE_TABLE);
 	pte |= KVM_PTE_VALID;
<span class=del>-
-	WARN_ON(kvm_pte_valid(old));
-	smp_store_release(ptep, pte);
</span><span class=add>+	return pte;
</span> }
 
 static kvm_pte_t kvm_init_valid_leaf_pte(u64 pa, kvm_pte_t attr, u32 level)
<span class=hunk>@@ -413,7 +410,7 @@ static bool hyp_map_walker_try_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *pte
</span> static int hyp_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte_t *old,
 			  enum kvm_pgtable_walk_flags flag, void * const arg)
 {
<span class=del>-	kvm_pte_t *childp;
</span><span class=add>+	kvm_pte_t *childp, new;
</span> 	struct hyp_map_data *data = arg;
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
 
<span class=hunk>@@ -427,8 +424,10 @@ static int hyp_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte
</span> 	if (!childp)
 		return -ENOMEM;
 
<span class=del>-	kvm_set_table_pte(ptep, childp, mm_ops);
</span><span class=add>+	new = kvm_init_table_pte(childp, mm_ops);
</span> 	mm_ops-&gt;get_page(ptep);
<span class=add>+	smp_store_release(ptep, new);
+
</span> 	return 0;
 }
 
<span class=hunk>@@ -804,7 +803,7 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 				kvm_pte_t *old, struct stage2_map_data *data)
 {
 	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
<span class=del>-	kvm_pte_t *childp, pte = *old;
</span><span class=add>+	kvm_pte_t *childp, pte = *old, new;
</span> 	int ret;
 
 	ret = stage2_map_walker_try_leaf(addr, end, level, ptep, pte, data);
<span class=hunk>@@ -830,8 +829,9 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 	if (stage2_pte_is_counted(pte))
 		stage2_put_pte(ptep, data-&gt;mmu, addr, level, mm_ops);
 
<span class=del>-	kvm_set_table_pte(ptep, childp, mm_ops);
</span><span class=add>+	new = kvm_init_table_pte(childp, mm_ops);
</span> 	mm_ops-&gt;get_page(ptep);
<span class=add>+	smp_store_release(ptep, new);
</span> 
 	return 0;
 }
-- 
2.37.2.672.g94769d06f0-goog

_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m0c83aa5eb3183e4da038ba8b166c16ec0318c6af id=e0c83aa5eb3183e4da038ba8b166c16ec0318c6af>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-6-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r0c83aa5eb3183e4da038ba8b166c16ec0318c6af>96+ messages in thread</a></pre></ul><li><pre><a href=#ed064c93aacab4c28bae96834e85250c5d1b20665 id=md064c93aacab4c28bae96834e85250c5d1b20665>*</a> <b>[PATCH 06/14] KVM: arm64: Return next table from map callbacks</b>
  2022-08-30 19:41 ` <a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb>Oliver Upton</a>
  (?)
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#rd064c93aacab4c28bae96834e85250c5d1b20665>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-arm-kernel, kvmarm, kvm, linux-kernel

The map walkers install new page tables during their traversal. Return
the newly-installed table PTE from the map callbacks to point the walker
at the new table w/o rereading the ptep.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-7-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-7-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 9 +++++----
 1 file <a href=#ed064c93aacab4c28bae96834e85250c5d1b20665>changed</a>, 5 insertions(+), 4 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-7-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-7-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 331f6e3b2c20..f911509e6512 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -202,13 +202,12 @@ static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
</span> 	if (!table &amp;&amp; (flags &amp; KVM_PGTABLE_WALK_LEAF)) {
 		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep, &amp;pte,
 					     KVM_PGTABLE_WALK_LEAF);
<span class=del>-		pte = *ptep;
-		table = kvm_pte_table(pte, level);
</span> 	}
 
 	if (ret)
 		goto out;
 
<span class=add>+	table = kvm_pte_table(pte, level);
</span> 	if (!table) {
 		data-&gt;addr = ALIGN_DOWN(data-&gt;addr, kvm_granule_size(level));
 		data-&gt;addr += kvm_granule_size(level);
<span class=hunk>@@ -427,6 +426,7 @@ static int hyp_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte
</span> 	new = kvm_init_table_pte(childp, mm_ops);
 	mm_ops-&gt;get_page(ptep);
 	smp_store_release(ptep, new);
<span class=add>+	*old = new;
</span> 
 	return 0;
 }
<span class=hunk>@@ -768,7 +768,7 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
</span> }
 
 static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=del>-				struct stage2_map_data *data);
</span><span class=add>+				kvm_pte_t *old, struct stage2_map_data *data);
</span> 
 static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
 				     kvm_pte_t *ptep, kvm_pte_t *old,
<span class=hunk>@@ -791,7 +791,7 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
</span> 	 */
 	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
 
<span class=del>-	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
</span><span class=add>+	ret = stage2_map_walk_leaf(addr, end, level, ptep, old, data);
</span> 
 	mm_ops-&gt;put_page(ptep);
 	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
<span class=hunk>@@ -832,6 +832,7 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 	new = kvm_init_table_pte(childp, mm_ops);
 	mm_ops-&gt;get_page(ptep);
 	smp_store_release(ptep, new);
<span class=add>+	*old = new;
</span> 
 	return 0;
 }
-- 
2.37.2.672.g94769d06f0-goog


<a href=#md064c93aacab4c28bae96834e85250c5d1b20665 id=ed064c93aacab4c28bae96834e85250c5d1b20665>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#rd064c93aacab4c28bae96834e85250c5d1b20665>96+ messages in thread</a></pre><li><ul><li><pre><a href=#ed064c93aacab4c28bae96834e85250c5d1b20665 id=md064c93aacab4c28bae96834e85250c5d1b20665>*</a> <b>[PATCH 06/14] KVM: arm64: Return next table from map callbacks</b>
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#rd064c93aacab4c28bae96834e85250c5d1b20665>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-arm-kernel, kvmarm, kvm, linux-kernel

The map walkers install new page tables during their traversal. Return
the newly-installed table PTE from the map callbacks to point the walker
at the new table w/o rereading the ptep.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-7-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-7-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 9 +++++----
 1 file <a href=#ed064c93aacab4c28bae96834e85250c5d1b20665>changed</a>, 5 insertions(+), 4 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-7-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-7-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 331f6e3b2c20..f911509e6512 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -202,13 +202,12 @@ static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
</span> 	if (!table &amp;&amp; (flags &amp; KVM_PGTABLE_WALK_LEAF)) {
 		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep, &amp;pte,
 					     KVM_PGTABLE_WALK_LEAF);
<span class=del>-		pte = *ptep;
-		table = kvm_pte_table(pte, level);
</span> 	}
 
 	if (ret)
 		goto out;
 
<span class=add>+	table = kvm_pte_table(pte, level);
</span> 	if (!table) {
 		data-&gt;addr = ALIGN_DOWN(data-&gt;addr, kvm_granule_size(level));
 		data-&gt;addr += kvm_granule_size(level);
<span class=hunk>@@ -427,6 +426,7 @@ static int hyp_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte
</span> 	new = kvm_init_table_pte(childp, mm_ops);
 	mm_ops-&gt;get_page(ptep);
 	smp_store_release(ptep, new);
<span class=add>+	*old = new;
</span> 
 	return 0;
 }
<span class=hunk>@@ -768,7 +768,7 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
</span> }
 
 static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=del>-				struct stage2_map_data *data);
</span><span class=add>+				kvm_pte_t *old, struct stage2_map_data *data);
</span> 
 static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
 				     kvm_pte_t *ptep, kvm_pte_t *old,
<span class=hunk>@@ -791,7 +791,7 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
</span> 	 */
 	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
 
<span class=del>-	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
</span><span class=add>+	ret = stage2_map_walk_leaf(addr, end, level, ptep, old, data);
</span> 
 	mm_ops-&gt;put_page(ptep);
 	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
<span class=hunk>@@ -832,6 +832,7 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 	new = kvm_init_table_pte(childp, mm_ops);
 	mm_ops-&gt;get_page(ptep);
 	smp_store_release(ptep, new);
<span class=add>+	*old = new;
</span> 
 	return 0;
 }
-- 
2.37.2.672.g94769d06f0-goog


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#md064c93aacab4c28bae96834e85250c5d1b20665 id=ed064c93aacab4c28bae96834e85250c5d1b20665>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#rd064c93aacab4c28bae96834e85250c5d1b20665>96+ messages in thread</a></pre><li><pre><a href=#ed064c93aacab4c28bae96834e85250c5d1b20665 id=md064c93aacab4c28bae96834e85250c5d1b20665>*</a> <b>[PATCH 06/14] KVM: arm64: Return next table from map callbacks</b>
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#rd064c93aacab4c28bae96834e85250c5d1b20665>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-kernel, kvmarm, linux-arm-kernel, kvm

The map walkers install new page tables during their traversal. Return
the newly-installed table PTE from the map callbacks to point the walker
at the new table w/o rereading the ptep.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-7-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-7-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 9 +++++----
 1 file <a href=#ed064c93aacab4c28bae96834e85250c5d1b20665>changed</a>, 5 insertions(+), 4 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-7-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-7-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 331f6e3b2c20..f911509e6512 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -202,13 +202,12 @@ static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
</span> 	if (!table &amp;&amp; (flags &amp; KVM_PGTABLE_WALK_LEAF)) {
 		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep, &amp;pte,
 					     KVM_PGTABLE_WALK_LEAF);
<span class=del>-		pte = *ptep;
-		table = kvm_pte_table(pte, level);
</span> 	}
 
 	if (ret)
 		goto out;
 
<span class=add>+	table = kvm_pte_table(pte, level);
</span> 	if (!table) {
 		data-&gt;addr = ALIGN_DOWN(data-&gt;addr, kvm_granule_size(level));
 		data-&gt;addr += kvm_granule_size(level);
<span class=hunk>@@ -427,6 +426,7 @@ static int hyp_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte
</span> 	new = kvm_init_table_pte(childp, mm_ops);
 	mm_ops-&gt;get_page(ptep);
 	smp_store_release(ptep, new);
<span class=add>+	*old = new;
</span> 
 	return 0;
 }
<span class=hunk>@@ -768,7 +768,7 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
</span> }
 
 static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=del>-				struct stage2_map_data *data);
</span><span class=add>+				kvm_pte_t *old, struct stage2_map_data *data);
</span> 
 static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
 				     kvm_pte_t *ptep, kvm_pte_t *old,
<span class=hunk>@@ -791,7 +791,7 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
</span> 	 */
 	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
 
<span class=del>-	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
</span><span class=add>+	ret = stage2_map_walk_leaf(addr, end, level, ptep, old, data);
</span> 
 	mm_ops-&gt;put_page(ptep);
 	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
<span class=hunk>@@ -832,6 +832,7 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 	new = kvm_init_table_pte(childp, mm_ops);
 	mm_ops-&gt;get_page(ptep);
 	smp_store_release(ptep, new);
<span class=add>+	*old = new;
</span> 
 	return 0;
 }
-- 
2.37.2.672.g94769d06f0-goog

_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#md064c93aacab4c28bae96834e85250c5d1b20665 id=ed064c93aacab4c28bae96834e85250c5d1b20665>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-7-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#rd064c93aacab4c28bae96834e85250c5d1b20665>96+ messages in thread</a></pre><li><pre><a href=#e6f8c8d7673199558c639367a6d59cc5a82236961 id=m6f8c8d7673199558c639367a6d59cc5a82236961>*</a> <b>Re: [PATCH 06/14] KVM: arm64: Return next table from map callbacks</b>
  2022-08-30 19:41   ` <a href=#md064c93aacab4c28bae96834e85250c5d1b20665>Oliver Upton</a>
  (?)
<b>@ 2022-09-07 21:32     ` David Matlack</b>
  <a href=#r6f8c8d7673199558c639367a6d59cc5a82236961>-1 siblings, 0 replies; 96+ messages in thread</a>
From: David Matlack @ 2022-09-07 21:32 UTC (<a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

On Tue, Aug 30, 2022 at 07:41:24PM +0000, Oliver Upton wrote:
<span class=q>&gt; The map walkers install new page tables during their traversal. Return
&gt; the newly-installed table PTE from the map callbacks to point the walker
&gt; at the new table w/o rereading the ptep.
&gt; 
&gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; ---
&gt;  arch/arm64/kvm/hyp/pgtable.c | 9 +++++----
&gt;  1 file changed, 5 insertions(+), 4 deletions(-)
&gt; 
&gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; index 331f6e3b2c20..f911509e6512 100644
&gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; @@ -202,13 +202,12 @@ static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
&gt;  	if (!table &amp;&amp; (flags &amp; KVM_PGTABLE_WALK_LEAF)) {
&gt;  		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep, &amp;pte,
&gt;  					     KVM_PGTABLE_WALK_LEAF);
&gt; -		pte = *ptep;
&gt; -		table = kvm_pte_table(pte, level);
&gt;  	}
&gt;  
&gt;  	if (ret)
&gt;  		goto out;
</span>
Rather than passing a pointer to the local variable pte and requiring
all downstream code to update it (and deal with dereferencing to read
the old pte), wouldn't it be simpler to just re-read the PTE here? e.g.

        /*
         * Explicitly re-read the PTE since it may have been modified
         * during the TABLE_PRE or LEAF callback.
         */
        pte = kvm_pte_read(ptep);

This should also result in better behavior once parallelization is
introduced, because it will prevent the walker from traversing down and
doing a bunch of work on page tables that are in the process of being
freed by some other thread.

<span class=q>&gt;  
&gt; +	table = kvm_pte_table(pte, level);
&gt;  	if (!table) {
</span>
nit: Technically there's no reason to set @table again. e.g. This could
just be:

        if (!kvm_pte_table(pte, level)) {

<span class=q>&gt;  		data-&gt;addr = ALIGN_DOWN(data-&gt;addr, kvm_granule_size(level));
&gt;  		data-&gt;addr += kvm_granule_size(level);
&gt; @@ -427,6 +426,7 @@ static int hyp_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte
&gt;  	new = kvm_init_table_pte(childp, mm_ops);
&gt;  	mm_ops-&gt;get_page(ptep);
&gt;  	smp_store_release(ptep, new);
&gt; +	*old = new;
&gt;  
&gt;  	return 0;
&gt;  }
&gt; @@ -768,7 +768,7 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
&gt;  }
&gt;  
&gt;  static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; -				struct stage2_map_data *data);
&gt; +				kvm_pte_t *old, struct stage2_map_data *data);
&gt;  
&gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  				     kvm_pte_t *ptep, kvm_pte_t *old,
&gt; @@ -791,7 +791,7 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  	 */
&gt;  	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt;  
&gt; -	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; +	ret = stage2_map_walk_leaf(addr, end, level, ptep, old, data);
&gt;  
&gt;  	mm_ops-&gt;put_page(ptep);
&gt;  	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
&gt; @@ -832,6 +832,7 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  	new = kvm_init_table_pte(childp, mm_ops);
&gt;  	mm_ops-&gt;get_page(ptep);
&gt;  	smp_store_release(ptep, new);
&gt; +	*old = new;
&gt;  
&gt;  	return 0;
&gt;  }
&gt; -- 
&gt; 2.37.2.672.g94769d06f0-goog
&gt; 
</span>
<a href=#m6f8c8d7673199558c639367a6d59cc5a82236961 id=e6f8c8d7673199558c639367a6d59cc5a82236961>^</a> <a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/t/#u><b>nested</b></a>] <a href=#r6f8c8d7673199558c639367a6d59cc5a82236961>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e6f8c8d7673199558c639367a6d59cc5a82236961 id=m6f8c8d7673199558c639367a6d59cc5a82236961>*</a> <b>Re: [PATCH 06/14] KVM: arm64: Return next table from map callbacks</b>
<b>@ 2022-09-07 21:32     ` David Matlack</b>
  <a href=#r6f8c8d7673199558c639367a6d59cc5a82236961>0 siblings, 0 replies; 96+ messages in thread</a>
From: David Matlack @ 2022-09-07 21:32 UTC (<a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: kvm, Marc Zyngier, linux-kernel, Catalin Marinas, Ben Gardon,
	Paolo Bonzini, Will Deacon, kvmarm, linux-arm-kernel

On Tue, Aug 30, 2022 at 07:41:24PM +0000, Oliver Upton wrote:
<span class=q>&gt; The map walkers install new page tables during their traversal. Return
&gt; the newly-installed table PTE from the map callbacks to point the walker
&gt; at the new table w/o rereading the ptep.
&gt; 
&gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; ---
&gt;  arch/arm64/kvm/hyp/pgtable.c | 9 +++++----
&gt;  1 file changed, 5 insertions(+), 4 deletions(-)
&gt; 
&gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; index 331f6e3b2c20..f911509e6512 100644
&gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; @@ -202,13 +202,12 @@ static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
&gt;  	if (!table &amp;&amp; (flags &amp; KVM_PGTABLE_WALK_LEAF)) {
&gt;  		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep, &amp;pte,
&gt;  					     KVM_PGTABLE_WALK_LEAF);
&gt; -		pte = *ptep;
&gt; -		table = kvm_pte_table(pte, level);
&gt;  	}
&gt;  
&gt;  	if (ret)
&gt;  		goto out;
</span>
Rather than passing a pointer to the local variable pte and requiring
all downstream code to update it (and deal with dereferencing to read
the old pte), wouldn't it be simpler to just re-read the PTE here? e.g.

        /*
         * Explicitly re-read the PTE since it may have been modified
         * during the TABLE_PRE or LEAF callback.
         */
        pte = kvm_pte_read(ptep);

This should also result in better behavior once parallelization is
introduced, because it will prevent the walker from traversing down and
doing a bunch of work on page tables that are in the process of being
freed by some other thread.

<span class=q>&gt;  
&gt; +	table = kvm_pte_table(pte, level);
&gt;  	if (!table) {
</span>
nit: Technically there's no reason to set @table again. e.g. This could
just be:

        if (!kvm_pte_table(pte, level)) {

<span class=q>&gt;  		data-&gt;addr = ALIGN_DOWN(data-&gt;addr, kvm_granule_size(level));
&gt;  		data-&gt;addr += kvm_granule_size(level);
&gt; @@ -427,6 +426,7 @@ static int hyp_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte
&gt;  	new = kvm_init_table_pte(childp, mm_ops);
&gt;  	mm_ops-&gt;get_page(ptep);
&gt;  	smp_store_release(ptep, new);
&gt; +	*old = new;
&gt;  
&gt;  	return 0;
&gt;  }
&gt; @@ -768,7 +768,7 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
&gt;  }
&gt;  
&gt;  static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; -				struct stage2_map_data *data);
&gt; +				kvm_pte_t *old, struct stage2_map_data *data);
&gt;  
&gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  				     kvm_pte_t *ptep, kvm_pte_t *old,
&gt; @@ -791,7 +791,7 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  	 */
&gt;  	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt;  
&gt; -	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; +	ret = stage2_map_walk_leaf(addr, end, level, ptep, old, data);
&gt;  
&gt;  	mm_ops-&gt;put_page(ptep);
&gt;  	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
&gt; @@ -832,6 +832,7 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  	new = kvm_init_table_pte(childp, mm_ops);
&gt;  	mm_ops-&gt;get_page(ptep);
&gt;  	smp_store_release(ptep, new);
&gt; +	*old = new;
&gt;  
&gt;  	return 0;
&gt;  }
&gt; -- 
&gt; 2.37.2.672.g94769d06f0-goog
&gt; 
</span>_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m6f8c8d7673199558c639367a6d59cc5a82236961 id=e6f8c8d7673199558c639367a6d59cc5a82236961>^</a> <a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/t/#u><b>nested</b></a>] <a href=#r6f8c8d7673199558c639367a6d59cc5a82236961>96+ messages in thread</a></pre><li><pre><a href=#e6f8c8d7673199558c639367a6d59cc5a82236961 id=m6f8c8d7673199558c639367a6d59cc5a82236961>*</a> <b>Re: [PATCH 06/14] KVM: arm64: Return next table from map callbacks</b>
<b>@ 2022-09-07 21:32     ` David Matlack</b>
  <a href=#r6f8c8d7673199558c639367a6d59cc5a82236961>0 siblings, 0 replies; 96+ messages in thread</a>
From: David Matlack @ 2022-09-07 21:32 UTC (<a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

On Tue, Aug 30, 2022 at 07:41:24PM +0000, Oliver Upton wrote:
<span class=q>&gt; The map walkers install new page tables during their traversal. Return
&gt; the newly-installed table PTE from the map callbacks to point the walker
&gt; at the new table w/o rereading the ptep.
&gt; 
&gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; ---
&gt;  arch/arm64/kvm/hyp/pgtable.c | 9 +++++----
&gt;  1 file changed, 5 insertions(+), 4 deletions(-)
&gt; 
&gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; index 331f6e3b2c20..f911509e6512 100644
&gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; @@ -202,13 +202,12 @@ static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
&gt;  	if (!table &amp;&amp; (flags &amp; KVM_PGTABLE_WALK_LEAF)) {
&gt;  		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep, &amp;pte,
&gt;  					     KVM_PGTABLE_WALK_LEAF);
&gt; -		pte = *ptep;
&gt; -		table = kvm_pte_table(pte, level);
&gt;  	}
&gt;  
&gt;  	if (ret)
&gt;  		goto out;
</span>
Rather than passing a pointer to the local variable pte and requiring
all downstream code to update it (and deal with dereferencing to read
the old pte), wouldn't it be simpler to just re-read the PTE here? e.g.

        /*
         * Explicitly re-read the PTE since it may have been modified
         * during the TABLE_PRE or LEAF callback.
         */
        pte = kvm_pte_read(ptep);

This should also result in better behavior once parallelization is
introduced, because it will prevent the walker from traversing down and
doing a bunch of work on page tables that are in the process of being
freed by some other thread.

<span class=q>&gt;  
&gt; +	table = kvm_pte_table(pte, level);
&gt;  	if (!table) {
</span>
nit: Technically there's no reason to set @table again. e.g. This could
just be:

        if (!kvm_pte_table(pte, level)) {

<span class=q>&gt;  		data-&gt;addr = ALIGN_DOWN(data-&gt;addr, kvm_granule_size(level));
&gt;  		data-&gt;addr += kvm_granule_size(level);
&gt; @@ -427,6 +426,7 @@ static int hyp_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_pte
&gt;  	new = kvm_init_table_pte(childp, mm_ops);
&gt;  	mm_ops-&gt;get_page(ptep);
&gt;  	smp_store_release(ptep, new);
&gt; +	*old = new;
&gt;  
&gt;  	return 0;
&gt;  }
&gt; @@ -768,7 +768,7 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
&gt;  }
&gt;  
&gt;  static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; -				struct stage2_map_data *data);
&gt; +				kvm_pte_t *old, struct stage2_map_data *data);
&gt;  
&gt;  static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  				     kvm_pte_t *ptep, kvm_pte_t *old,
&gt; @@ -791,7 +791,7 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
&gt;  	 */
&gt;  	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt;  
&gt; -	ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
&gt; +	ret = stage2_map_walk_leaf(addr, end, level, ptep, old, data);
&gt;  
&gt;  	mm_ops-&gt;put_page(ptep);
&gt;  	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
&gt; @@ -832,6 +832,7 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  	new = kvm_init_table_pte(childp, mm_ops);
&gt;  	mm_ops-&gt;get_page(ptep);
&gt;  	smp_store_release(ptep, new);
&gt; +	*old = new;
&gt;  
&gt;  	return 0;
&gt;  }
&gt; -- 
&gt; 2.37.2.672.g94769d06f0-goog
&gt; 
</span>
_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m6f8c8d7673199558c639367a6d59cc5a82236961 id=e6f8c8d7673199558c639367a6d59cc5a82236961>^</a> <a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxkN7XmHiU3ddknR@google.com/t/#u><b>nested</b></a>] <a href=#r6f8c8d7673199558c639367a6d59cc5a82236961>96+ messages in thread</a></pre><li><pre><a href=#ef81c90edc45528cf7dd4450f40e52c4c65567a9b id=mf81c90edc45528cf7dd4450f40e52c4c65567a9b>*</a> <b>Re: [PATCH 06/14] KVM: arm64: Return next table from map callbacks</b>
  2022-09-07 21:32     ` <a href=#m6f8c8d7673199558c639367a6d59cc5a82236961>David Matlack</a>
  (?)
<b>@ 2022-09-09  9:38       ` Oliver Upton</b>
  <a href=#rf81c90edc45528cf7dd4450f40e52c4c65567a9b>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-09-09  9:38 UTC (<a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/raw>raw</a>)
  To: David Matlack
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

Hi David,

On Wed, Sep 07, 2022 at 02:32:29PM -0700, David Matlack wrote:
<span class=q>&gt; On Tue, Aug 30, 2022 at 07:41:24PM +0000, Oliver Upton wrote:
&gt; &gt; The map walkers install new page tables during their traversal. Return
&gt; &gt; the newly-installed table PTE from the map callbacks to point the walker
&gt; &gt; at the new table w/o rereading the ptep.
&gt; &gt; 
&gt; &gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; &gt; ---
&gt; &gt;  arch/arm64/kvm/hyp/pgtable.c | 9 +++++----
&gt; &gt;  1 file changed, 5 insertions(+), 4 deletions(-)
&gt; &gt; 
&gt; &gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; index 331f6e3b2c20..f911509e6512 100644
&gt; &gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; @@ -202,13 +202,12 @@ static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
&gt; &gt;  	if (!table &amp;&amp; (flags &amp; KVM_PGTABLE_WALK_LEAF)) {
&gt; &gt;  		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep, &amp;pte,
&gt; &gt;  					     KVM_PGTABLE_WALK_LEAF);
&gt; &gt; -		pte = *ptep;
&gt; &gt; -		table = kvm_pte_table(pte, level);
&gt; &gt;  	}
&gt; &gt;  
&gt; &gt;  	if (ret)
&gt; &gt;  		goto out;
&gt; 
&gt; Rather than passing a pointer to the local variable pte and requiring
&gt; all downstream code to update it (and deal with dereferencing to read
&gt; the old pte), wouldn't it be simpler to just re-read the PTE here?
</span>
Yeah, you're right. I had some odd rationalization about this, but
there's no need to force a walker to descend into the new table level as
it is wasted work if another thread unlinks it.

[...]

<span class=q>&gt; &gt;  
&gt; &gt; +	table = kvm_pte_table(pte, level);
&gt; &gt;  	if (!table) {
&gt; 
&gt; nit: Technically there's no reason to set @table again. e.g. This could
&gt; just be:
&gt; 
&gt;         if (!kvm_pte_table(pte, level)) {
</span>
Sure, I'll squish these lines together.

--
Thanks,
Oliver

<a href=#mf81c90edc45528cf7dd4450f40e52c4c65567a9b id=ef81c90edc45528cf7dd4450f40e52c4c65567a9b>^</a> <a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/t/#u><b>nested</b></a>] <a href=#rf81c90edc45528cf7dd4450f40e52c4c65567a9b>96+ messages in thread</a></pre><li><ul><li><pre><a href=#ef81c90edc45528cf7dd4450f40e52c4c65567a9b id=mf81c90edc45528cf7dd4450f40e52c4c65567a9b>*</a> <b>Re: [PATCH 06/14] KVM: arm64: Return next table from map callbacks</b>
<b>@ 2022-09-09  9:38       ` Oliver Upton</b>
  <a href=#rf81c90edc45528cf7dd4450f40e52c4c65567a9b>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-09-09  9:38 UTC (<a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/raw>raw</a>)
  To: David Matlack
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

Hi David,

On Wed, Sep 07, 2022 at 02:32:29PM -0700, David Matlack wrote:
<span class=q>&gt; On Tue, Aug 30, 2022 at 07:41:24PM +0000, Oliver Upton wrote:
&gt; &gt; The map walkers install new page tables during their traversal. Return
&gt; &gt; the newly-installed table PTE from the map callbacks to point the walker
&gt; &gt; at the new table w/o rereading the ptep.
&gt; &gt; 
&gt; &gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; &gt; ---
&gt; &gt;  arch/arm64/kvm/hyp/pgtable.c | 9 +++++----
&gt; &gt;  1 file changed, 5 insertions(+), 4 deletions(-)
&gt; &gt; 
&gt; &gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; index 331f6e3b2c20..f911509e6512 100644
&gt; &gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; @@ -202,13 +202,12 @@ static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
&gt; &gt;  	if (!table &amp;&amp; (flags &amp; KVM_PGTABLE_WALK_LEAF)) {
&gt; &gt;  		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep, &amp;pte,
&gt; &gt;  					     KVM_PGTABLE_WALK_LEAF);
&gt; &gt; -		pte = *ptep;
&gt; &gt; -		table = kvm_pte_table(pte, level);
&gt; &gt;  	}
&gt; &gt;  
&gt; &gt;  	if (ret)
&gt; &gt;  		goto out;
&gt; 
&gt; Rather than passing a pointer to the local variable pte and requiring
&gt; all downstream code to update it (and deal with dereferencing to read
&gt; the old pte), wouldn't it be simpler to just re-read the PTE here?
</span>
Yeah, you're right. I had some odd rationalization about this, but
there's no need to force a walker to descend into the new table level as
it is wasted work if another thread unlinks it.

[...]

<span class=q>&gt; &gt;  
&gt; &gt; +	table = kvm_pte_table(pte, level);
&gt; &gt;  	if (!table) {
&gt; 
&gt; nit: Technically there's no reason to set @table again. e.g. This could
&gt; just be:
&gt; 
&gt;         if (!kvm_pte_table(pte, level)) {
</span>
Sure, I'll squish these lines together.

--
Thanks,
Oliver

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#mf81c90edc45528cf7dd4450f40e52c4c65567a9b id=ef81c90edc45528cf7dd4450f40e52c4c65567a9b>^</a> <a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/t/#u><b>nested</b></a>] <a href=#rf81c90edc45528cf7dd4450f40e52c4c65567a9b>96+ messages in thread</a></pre><li><pre><a href=#ef81c90edc45528cf7dd4450f40e52c4c65567a9b id=mf81c90edc45528cf7dd4450f40e52c4c65567a9b>*</a> <b>Re: [PATCH 06/14] KVM: arm64: Return next table from map callbacks</b>
<b>@ 2022-09-09  9:38       ` Oliver Upton</b>
  <a href=#rf81c90edc45528cf7dd4450f40e52c4c65567a9b>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-09-09  9:38 UTC (<a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/raw>raw</a>)
  To: David Matlack
  Cc: kvm, Marc Zyngier, linux-kernel, Catalin Marinas, Ben Gardon,
	Paolo Bonzini, Will Deacon, kvmarm, linux-arm-kernel

Hi David,

On Wed, Sep 07, 2022 at 02:32:29PM -0700, David Matlack wrote:
<span class=q>&gt; On Tue, Aug 30, 2022 at 07:41:24PM +0000, Oliver Upton wrote:
&gt; &gt; The map walkers install new page tables during their traversal. Return
&gt; &gt; the newly-installed table PTE from the map callbacks to point the walker
&gt; &gt; at the new table w/o rereading the ptep.
&gt; &gt; 
&gt; &gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; &gt; ---
&gt; &gt;  arch/arm64/kvm/hyp/pgtable.c | 9 +++++----
&gt; &gt;  1 file changed, 5 insertions(+), 4 deletions(-)
&gt; &gt; 
&gt; &gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; index 331f6e3b2c20..f911509e6512 100644
&gt; &gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; @@ -202,13 +202,12 @@ static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
&gt; &gt;  	if (!table &amp;&amp; (flags &amp; KVM_PGTABLE_WALK_LEAF)) {
&gt; &gt;  		ret = kvm_pgtable_visitor_cb(data, addr, level, ptep, &amp;pte,
&gt; &gt;  					     KVM_PGTABLE_WALK_LEAF);
&gt; &gt; -		pte = *ptep;
&gt; &gt; -		table = kvm_pte_table(pte, level);
&gt; &gt;  	}
&gt; &gt;  
&gt; &gt;  	if (ret)
&gt; &gt;  		goto out;
&gt; 
&gt; Rather than passing a pointer to the local variable pte and requiring
&gt; all downstream code to update it (and deal with dereferencing to read
&gt; the old pte), wouldn't it be simpler to just re-read the PTE here?
</span>
Yeah, you're right. I had some odd rationalization about this, but
there's no need to force a walker to descend into the new table level as
it is wasted work if another thread unlinks it.

[...]

<span class=q>&gt; &gt;  
&gt; &gt; +	table = kvm_pte_table(pte, level);
&gt; &gt;  	if (!table) {
&gt; 
&gt; nit: Technically there's no reason to set @table again. e.g. This could
&gt; just be:
&gt; 
&gt;         if (!kvm_pte_table(pte, level)) {
</span>
Sure, I'll squish these lines together.

--
Thanks,
Oliver
_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#mf81c90edc45528cf7dd4450f40e52c4c65567a9b id=ef81c90edc45528cf7dd4450f40e52c4c65567a9b>^</a> <a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxsJj3ojGyhNw5Jn@google.com/t/#u><b>nested</b></a>] <a href=#rf81c90edc45528cf7dd4450f40e52c4c65567a9b>96+ messages in thread</a></pre></ul></ul></ul><li><pre><a href=#e51d5975bbb33a8f80342da992689bae320bdb065 id=m51d5975bbb33a8f80342da992689bae320bdb065>*</a> <b>[PATCH 07/14] KVM: arm64: Document behavior of pgtable visitor callback</b>
  2022-08-30 19:41 ` <a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb>Oliver Upton</a>
  (?)
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r51d5975bbb33a8f80342da992689bae320bdb065>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-kernel, kvmarm, linux-arm-kernel, kvm

The argument list to kvm_pgtable_visitor_fn_t has gotten rather long.
Additionally, @old serves as both an input and output parameter, which
isn't easily discerned from the declaration alone.

Document the meaning of the visitor callback arguments and the
conditions under which @old was written to.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-8-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h href=#Z2e.:..:20220830194132.962932-8-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>arch/arm64/include/asm/kvm_pgtable.h</a> | 16 ++++++++++++++++
 1 file <a href=#e51d5975bbb33a8f80342da992689bae320bdb065>changed</a>, 16 insertions(+)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-8-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h id=Z2e.:..:20220830194132.962932-8-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>diff</a> --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index 47920ae3f7e7..78fbb7be1af6 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -194,6 +194,22 @@ enum kvm_pgtable_walk_flags {
</span> 	KVM_PGTABLE_WALK_TABLE_POST		= BIT(2),
 };
 
<span class=add>+/**
+ * kvm_pgtable_visitor_fn_t - Page table traversal callback for visiting a PTE.
+ * @addr:	Input address (IA) mapped by the PTE.
+ * @end:	IA corresponding to the end of the page table traversal range.
+ * @ptep:	Pointer to the PTE.
+ * @old:	Value of the PTE observed by the visitor. Also used as an output
+ *		parameter for returning the new PTE value.
+ * @flag:	Flag identifying the entry type visited.
+ * @arg:	Argument passed to the callback function.
+ *
+ * Callback function signature invoked during page table traversal. Optionally
+ * returns the new value of the PTE via @old if the new value requires further
+ * traversal (i.e. installing a new table).
+ *
+ * Return: 0 on success, negative error code on failure.
+ */
</span> typedef int (*kvm_pgtable_visitor_fn_t)(u64 addr, u64 end, u32 level,
 					kvm_pte_t *ptep, kvm_pte_t *old,
 					enum kvm_pgtable_walk_flags flag,
-- 
2.37.2.672.g94769d06f0-goog

_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m51d5975bbb33a8f80342da992689bae320bdb065 id=e51d5975bbb33a8f80342da992689bae320bdb065>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r51d5975bbb33a8f80342da992689bae320bdb065>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e51d5975bbb33a8f80342da992689bae320bdb065 id=m51d5975bbb33a8f80342da992689bae320bdb065>*</a> <b>[PATCH 07/14] KVM: arm64: Document behavior of pgtable visitor callback</b>
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r51d5975bbb33a8f80342da992689bae320bdb065>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-arm-kernel, kvmarm, kvm, linux-kernel

The argument list to kvm_pgtable_visitor_fn_t has gotten rather long.
Additionally, @old serves as both an input and output parameter, which
isn't easily discerned from the declaration alone.

Document the meaning of the visitor callback arguments and the
conditions under which @old was written to.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-8-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h href=#Z2e.:..:20220830194132.962932-8-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>arch/arm64/include/asm/kvm_pgtable.h</a> | 16 ++++++++++++++++
 1 file <a href=#e51d5975bbb33a8f80342da992689bae320bdb065>changed</a>, 16 insertions(+)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-8-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h id=Z2e.:..:20220830194132.962932-8-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>diff</a> --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index 47920ae3f7e7..78fbb7be1af6 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -194,6 +194,22 @@ enum kvm_pgtable_walk_flags {
</span> 	KVM_PGTABLE_WALK_TABLE_POST		= BIT(2),
 };
 
<span class=add>+/**
+ * kvm_pgtable_visitor_fn_t - Page table traversal callback for visiting a PTE.
+ * @addr:	Input address (IA) mapped by the PTE.
+ * @end:	IA corresponding to the end of the page table traversal range.
+ * @ptep:	Pointer to the PTE.
+ * @old:	Value of the PTE observed by the visitor. Also used as an output
+ *		parameter for returning the new PTE value.
+ * @flag:	Flag identifying the entry type visited.
+ * @arg:	Argument passed to the callback function.
+ *
+ * Callback function signature invoked during page table traversal. Optionally
+ * returns the new value of the PTE via @old if the new value requires further
+ * traversal (i.e. installing a new table).
+ *
+ * Return: 0 on success, negative error code on failure.
+ */
</span> typedef int (*kvm_pgtable_visitor_fn_t)(u64 addr, u64 end, u32 level,
 					kvm_pte_t *ptep, kvm_pte_t *old,
 					enum kvm_pgtable_walk_flags flag,
-- 
2.37.2.672.g94769d06f0-goog


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m51d5975bbb33a8f80342da992689bae320bdb065 id=e51d5975bbb33a8f80342da992689bae320bdb065>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r51d5975bbb33a8f80342da992689bae320bdb065>96+ messages in thread</a></pre><li><pre><a href=#e51d5975bbb33a8f80342da992689bae320bdb065 id=m51d5975bbb33a8f80342da992689bae320bdb065>*</a> <b>[PATCH 07/14] KVM: arm64: Document behavior of pgtable visitor callback</b>
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r51d5975bbb33a8f80342da992689bae320bdb065>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-arm-kernel, kvmarm, kvm, linux-kernel

The argument list to kvm_pgtable_visitor_fn_t has gotten rather long.
Additionally, @old serves as both an input and output parameter, which
isn't easily discerned from the declaration alone.

Document the meaning of the visitor callback arguments and the
conditions under which @old was written to.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-8-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h href=#Z2e.:..:20220830194132.962932-8-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>arch/arm64/include/asm/kvm_pgtable.h</a> | 16 ++++++++++++++++
 1 file <a href=#e51d5975bbb33a8f80342da992689bae320bdb065>changed</a>, 16 insertions(+)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-8-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h id=Z2e.:..:20220830194132.962932-8-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>diff</a> --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index 47920ae3f7e7..78fbb7be1af6 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -194,6 +194,22 @@ enum kvm_pgtable_walk_flags {
</span> 	KVM_PGTABLE_WALK_TABLE_POST		= BIT(2),
 };
 
<span class=add>+/**
+ * kvm_pgtable_visitor_fn_t - Page table traversal callback for visiting a PTE.
+ * @addr:	Input address (IA) mapped by the PTE.
+ * @end:	IA corresponding to the end of the page table traversal range.
+ * @ptep:	Pointer to the PTE.
+ * @old:	Value of the PTE observed by the visitor. Also used as an output
+ *		parameter for returning the new PTE value.
+ * @flag:	Flag identifying the entry type visited.
+ * @arg:	Argument passed to the callback function.
+ *
+ * Callback function signature invoked during page table traversal. Optionally
+ * returns the new value of the PTE via @old if the new value requires further
+ * traversal (i.e. installing a new table).
+ *
+ * Return: 0 on success, negative error code on failure.
+ */
</span> typedef int (*kvm_pgtable_visitor_fn_t)(u64 addr, u64 end, u32 level,
 					kvm_pte_t *ptep, kvm_pte_t *old,
 					enum kvm_pgtable_walk_flags flag,
-- 
2.37.2.672.g94769d06f0-goog


<a href=#m51d5975bbb33a8f80342da992689bae320bdb065 id=e51d5975bbb33a8f80342da992689bae320bdb065>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-8-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r51d5975bbb33a8f80342da992689bae320bdb065>96+ messages in thread</a></pre></ul><li><pre><a href=#e8317ace1fc52c8970cdc9375251ca31c97cef19e id=m8317ace1fc52c8970cdc9375251ca31c97cef19e>*</a> <b>[PATCH 08/14] KVM: arm64: Protect page table traversal with RCU</b>
  2022-08-30 19:41 ` <a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb>Oliver Upton</a>
  (?)
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r8317ace1fc52c8970cdc9375251ca31c97cef19e>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-kernel, kvmarm, linux-arm-kernel, kvm

The use of RCU is necessary to change the paging structures in parallel.
Acquire and release an RCU read lock when traversing the page tables.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h href=#Z2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>arch/arm64/include/asm/kvm_pgtable.h</a> | 19 ++++++++++++++++++-
 <a id=iZ2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a>         |  7 ++++++-
 2 files <a href=#e8317ace1fc52c8970cdc9375251ca31c97cef19e>changed</a>, 24 insertions(+), 2 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h id=Z2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>diff</a> --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index 78fbb7be1af6..7d2de0a98ccb 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -578,9 +578,26 @@ enum kvm_pgtable_prot kvm_pgtable_stage2_pte_prot(kvm_pte_t pte);
</span>  */
 enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte);
 
<span class=add>+#if defined(__KVM_NVHE_HYPERVISOR___)
+
+static inline void kvm_pgtable_walk_begin(void) {}
+static inline void kvm_pgtable_walk_end(void) {}
+
+#define kvm_dereference_ptep rcu_dereference_raw
+
+#else	/* !defined(__KVM_NVHE_HYPERVISOR__) */
+
+#define kvm_pgtable_walk_begin	rcu_read_lock
+#define kvm_pgtable_walk_end	rcu_read_unlock
+#define kvm_dereference_ptep	rcu_dereference
+
+#endif	/* defined(__KVM_NVHE_HYPERVISOR__) */
+
</span> static inline kvm_pte_t kvm_pte_read(kvm_pte_t *ptep)
 {
<span class=del>-	return READ_ONCE(*ptep);
</span><span class=add>+	kvm_pte_t __rcu *p = (kvm_pte_t __rcu *)ptep;
+
+	return READ_ONCE(*kvm_dereference_ptep(p));
</span> }
 
 #endif	/* __ARM64_KVM_PGTABLE_H__ */
<span class=head><a href=#iZ2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index f911509e6512..215a14c434ed 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -284,8 +284,13 @@ int kvm_pgtable_walk(struct kvm_pgtable *pgt, u64 addr, u64 size,
</span> 		.end	= PAGE_ALIGN(walk_data.addr + size),
 		.walker	= walker,
 	};
<span class=add>+	int r;
</span> 
<span class=del>-	return _kvm_pgtable_walk(&amp;walk_data);
</span><span class=add>+	kvm_pgtable_walk_begin();
+	r = _kvm_pgtable_walk(&amp;walk_data);
+	kvm_pgtable_walk_end();
+
+	return r;
</span> }
 
 struct leaf_walk_data {
-- 
2.37.2.672.g94769d06f0-goog

_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m8317ace1fc52c8970cdc9375251ca31c97cef19e id=e8317ace1fc52c8970cdc9375251ca31c97cef19e>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r8317ace1fc52c8970cdc9375251ca31c97cef19e>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e8317ace1fc52c8970cdc9375251ca31c97cef19e id=m8317ace1fc52c8970cdc9375251ca31c97cef19e>*</a> <b>[PATCH 08/14] KVM: arm64: Protect page table traversal with RCU</b>
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r8317ace1fc52c8970cdc9375251ca31c97cef19e>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-arm-kernel, kvmarm, kvm, linux-kernel

The use of RCU is necessary to change the paging structures in parallel.
Acquire and release an RCU read lock when traversing the page tables.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h href=#Z2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>arch/arm64/include/asm/kvm_pgtable.h</a> | 19 ++++++++++++++++++-
 <a id=iZ2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a>         |  7 ++++++-
 2 files <a href=#e8317ace1fc52c8970cdc9375251ca31c97cef19e>changed</a>, 24 insertions(+), 2 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h id=Z2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>diff</a> --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index 78fbb7be1af6..7d2de0a98ccb 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -578,9 +578,26 @@ enum kvm_pgtable_prot kvm_pgtable_stage2_pte_prot(kvm_pte_t pte);
</span>  */
 enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte);
 
<span class=add>+#if defined(__KVM_NVHE_HYPERVISOR___)
+
+static inline void kvm_pgtable_walk_begin(void) {}
+static inline void kvm_pgtable_walk_end(void) {}
+
+#define kvm_dereference_ptep rcu_dereference_raw
+
+#else	/* !defined(__KVM_NVHE_HYPERVISOR__) */
+
+#define kvm_pgtable_walk_begin	rcu_read_lock
+#define kvm_pgtable_walk_end	rcu_read_unlock
+#define kvm_dereference_ptep	rcu_dereference
+
+#endif	/* defined(__KVM_NVHE_HYPERVISOR__) */
+
</span> static inline kvm_pte_t kvm_pte_read(kvm_pte_t *ptep)
 {
<span class=del>-	return READ_ONCE(*ptep);
</span><span class=add>+	kvm_pte_t __rcu *p = (kvm_pte_t __rcu *)ptep;
+
+	return READ_ONCE(*kvm_dereference_ptep(p));
</span> }
 
 #endif	/* __ARM64_KVM_PGTABLE_H__ */
<span class=head><a href=#iZ2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index f911509e6512..215a14c434ed 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -284,8 +284,13 @@ int kvm_pgtable_walk(struct kvm_pgtable *pgt, u64 addr, u64 size,
</span> 		.end	= PAGE_ALIGN(walk_data.addr + size),
 		.walker	= walker,
 	};
<span class=add>+	int r;
</span> 
<span class=del>-	return _kvm_pgtable_walk(&amp;walk_data);
</span><span class=add>+	kvm_pgtable_walk_begin();
+	r = _kvm_pgtable_walk(&amp;walk_data);
+	kvm_pgtable_walk_end();
+
+	return r;
</span> }
 
 struct leaf_walk_data {
-- 
2.37.2.672.g94769d06f0-goog


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m8317ace1fc52c8970cdc9375251ca31c97cef19e id=e8317ace1fc52c8970cdc9375251ca31c97cef19e>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r8317ace1fc52c8970cdc9375251ca31c97cef19e>96+ messages in thread</a></pre><li><pre><a href=#e8317ace1fc52c8970cdc9375251ca31c97cef19e id=m8317ace1fc52c8970cdc9375251ca31c97cef19e>*</a> <b>[PATCH 08/14] KVM: arm64: Protect page table traversal with RCU</b>
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r8317ace1fc52c8970cdc9375251ca31c97cef19e>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-arm-kernel, kvmarm, kvm, linux-kernel

The use of RCU is necessary to change the paging structures in parallel.
Acquire and release an RCU read lock when traversing the page tables.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h href=#Z2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>arch/arm64/include/asm/kvm_pgtable.h</a> | 19 ++++++++++++++++++-
 <a id=iZ2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a>         |  7 ++++++-
 2 files <a href=#e8317ace1fc52c8970cdc9375251ca31c97cef19e>changed</a>, 24 insertions(+), 2 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h id=Z2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>diff</a> --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index 78fbb7be1af6..7d2de0a98ccb 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -578,9 +578,26 @@ enum kvm_pgtable_prot kvm_pgtable_stage2_pte_prot(kvm_pte_t pte);
</span>  */
 enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte);
 
<span class=add>+#if defined(__KVM_NVHE_HYPERVISOR___)
+
+static inline void kvm_pgtable_walk_begin(void) {}
+static inline void kvm_pgtable_walk_end(void) {}
+
+#define kvm_dereference_ptep rcu_dereference_raw
+
+#else	/* !defined(__KVM_NVHE_HYPERVISOR__) */
+
+#define kvm_pgtable_walk_begin	rcu_read_lock
+#define kvm_pgtable_walk_end	rcu_read_unlock
+#define kvm_dereference_ptep	rcu_dereference
+
+#endif	/* defined(__KVM_NVHE_HYPERVISOR__) */
+
</span> static inline kvm_pte_t kvm_pte_read(kvm_pte_t *ptep)
 {
<span class=del>-	return READ_ONCE(*ptep);
</span><span class=add>+	kvm_pte_t __rcu *p = (kvm_pte_t __rcu *)ptep;
+
+	return READ_ONCE(*kvm_dereference_ptep(p));
</span> }
 
 #endif	/* __ARM64_KVM_PGTABLE_H__ */
<span class=head><a href=#iZ2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830194132.962932-9-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index f911509e6512..215a14c434ed 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -284,8 +284,13 @@ int kvm_pgtable_walk(struct kvm_pgtable *pgt, u64 addr, u64 size,
</span> 		.end	= PAGE_ALIGN(walk_data.addr + size),
 		.walker	= walker,
 	};
<span class=add>+	int r;
</span> 
<span class=del>-	return _kvm_pgtable_walk(&amp;walk_data);
</span><span class=add>+	kvm_pgtable_walk_begin();
+	r = _kvm_pgtable_walk(&amp;walk_data);
+	kvm_pgtable_walk_end();
+
+	return r;
</span> }
 
 struct leaf_walk_data {
-- 
2.37.2.672.g94769d06f0-goog


<a href=#m8317ace1fc52c8970cdc9375251ca31c97cef19e id=e8317ace1fc52c8970cdc9375251ca31c97cef19e>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-9-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r8317ace1fc52c8970cdc9375251ca31c97cef19e>96+ messages in thread</a></pre><li><pre><a href=#e937ef34a7f44d4724cc32b00672074194178449e id=m937ef34a7f44d4724cc32b00672074194178449e>*</a> <b>Re: [PATCH 08/14] KVM: arm64: Protect page table traversal with RCU</b>
  2022-08-30 19:41   ` <a href=#m8317ace1fc52c8970cdc9375251ca31c97cef19e>Oliver Upton</a>
  (?)
<b>@ 2022-09-07 21:47     ` David Matlack</b>
  <a href=#r937ef34a7f44d4724cc32b00672074194178449e>-1 siblings, 0 replies; 96+ messages in thread</a>
From: David Matlack @ 2022-09-07 21:47 UTC (<a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

On Tue, Aug 30, 2022 at 07:41:26PM +0000, Oliver Upton wrote:
<span class=q>&gt; The use of RCU is necessary to change the paging structures in parallel.
&gt; Acquire and release an RCU read lock when traversing the page tables.
&gt; 
&gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; ---
&gt;  arch/arm64/include/asm/kvm_pgtable.h | 19 ++++++++++++++++++-
&gt;  arch/arm64/kvm/hyp/pgtable.c         |  7 ++++++-
&gt;  2 files changed, 24 insertions(+), 2 deletions(-)
&gt; 
&gt; diff --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
&gt; index 78fbb7be1af6..7d2de0a98ccb 100644
&gt; --- a/arch/arm64/include/asm/kvm_pgtable.h
&gt; +++ b/arch/arm64/include/asm/kvm_pgtable.h
&gt; @@ -578,9 +578,26 @@ enum kvm_pgtable_prot kvm_pgtable_stage2_pte_prot(kvm_pte_t pte);
&gt;   */
&gt;  enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte);
&gt;  
&gt; +#if defined(__KVM_NVHE_HYPERVISOR___)
&gt; +
</span>
Future readers will wonder why NVHE stubs out RCU support and how that
is even correct. Some comments here would be useful explain it.

<span class=q>&gt; +static inline void kvm_pgtable_walk_begin(void) {}
&gt; +static inline void kvm_pgtable_walk_end(void) {}
&gt; +
&gt; +#define kvm_dereference_ptep rcu_dereference_raw
</span>
How does NVHE have access rcu_dereference_raw()?

<span class=q>&gt; +
&gt; +#else	/* !defined(__KVM_NVHE_HYPERVISOR__) */
&gt; +
&gt; +#define kvm_pgtable_walk_begin	rcu_read_lock
&gt; +#define kvm_pgtable_walk_end	rcu_read_unlock
&gt; +#define kvm_dereference_ptep	rcu_dereference
&gt; +
&gt; +#endif	/* defined(__KVM_NVHE_HYPERVISOR__) */
&gt; +
&gt;  static inline kvm_pte_t kvm_pte_read(kvm_pte_t *ptep)
&gt;  {
&gt; -	return READ_ONCE(*ptep);
&gt; +	kvm_pte_t __rcu *p = (kvm_pte_t __rcu *)ptep;
&gt; +
&gt; +	return READ_ONCE(*kvm_dereference_ptep(p));
</span>
What about all the other places where page table memory is accessed?

If RCU is going to be used to protect page table memory, then all
accesses have to go under an RCU critical section. This means that page
table memory should only be accessed through __rcu annotated pointers
and dereferenced with rcu_dereference().

<span class=q>&gt;  }
&gt;  
&gt;  #endif	/* __ARM64_KVM_PGTABLE_H__ */
&gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; index f911509e6512..215a14c434ed 100644
&gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; @@ -284,8 +284,13 @@ int kvm_pgtable_walk(struct kvm_pgtable *pgt, u64 addr, u64 size,
&gt;  		.end	= PAGE_ALIGN(walk_data.addr + size),
&gt;  		.walker	= walker,
&gt;  	};
&gt; +	int r;
&gt;  
&gt; -	return _kvm_pgtable_walk(&amp;walk_data);
&gt; +	kvm_pgtable_walk_begin();
&gt; +	r = _kvm_pgtable_walk(&amp;walk_data);
&gt; +	kvm_pgtable_walk_end();
&gt; +
&gt; +	return r;
&gt;  }
&gt;  
&gt;  struct leaf_walk_data {
&gt; -- 
&gt; 2.37.2.672.g94769d06f0-goog
&gt; 
</span>
<a href=#m937ef34a7f44d4724cc32b00672074194178449e id=e937ef34a7f44d4724cc32b00672074194178449e>^</a> <a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/t/#u><b>nested</b></a>] <a href=#r937ef34a7f44d4724cc32b00672074194178449e>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e937ef34a7f44d4724cc32b00672074194178449e id=m937ef34a7f44d4724cc32b00672074194178449e>*</a> <b>Re: [PATCH 08/14] KVM: arm64: Protect page table traversal with RCU</b>
<b>@ 2022-09-07 21:47     ` David Matlack</b>
  <a href=#r937ef34a7f44d4724cc32b00672074194178449e>0 siblings, 0 replies; 96+ messages in thread</a>
From: David Matlack @ 2022-09-07 21:47 UTC (<a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: kvm, Marc Zyngier, linux-kernel, Catalin Marinas, Ben Gardon,
	Paolo Bonzini, Will Deacon, kvmarm, linux-arm-kernel

On Tue, Aug 30, 2022 at 07:41:26PM +0000, Oliver Upton wrote:
<span class=q>&gt; The use of RCU is necessary to change the paging structures in parallel.
&gt; Acquire and release an RCU read lock when traversing the page tables.
&gt; 
&gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; ---
&gt;  arch/arm64/include/asm/kvm_pgtable.h | 19 ++++++++++++++++++-
&gt;  arch/arm64/kvm/hyp/pgtable.c         |  7 ++++++-
&gt;  2 files changed, 24 insertions(+), 2 deletions(-)
&gt; 
&gt; diff --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
&gt; index 78fbb7be1af6..7d2de0a98ccb 100644
&gt; --- a/arch/arm64/include/asm/kvm_pgtable.h
&gt; +++ b/arch/arm64/include/asm/kvm_pgtable.h
&gt; @@ -578,9 +578,26 @@ enum kvm_pgtable_prot kvm_pgtable_stage2_pte_prot(kvm_pte_t pte);
&gt;   */
&gt;  enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte);
&gt;  
&gt; +#if defined(__KVM_NVHE_HYPERVISOR___)
&gt; +
</span>
Future readers will wonder why NVHE stubs out RCU support and how that
is even correct. Some comments here would be useful explain it.

<span class=q>&gt; +static inline void kvm_pgtable_walk_begin(void) {}
&gt; +static inline void kvm_pgtable_walk_end(void) {}
&gt; +
&gt; +#define kvm_dereference_ptep rcu_dereference_raw
</span>
How does NVHE have access rcu_dereference_raw()?

<span class=q>&gt; +
&gt; +#else	/* !defined(__KVM_NVHE_HYPERVISOR__) */
&gt; +
&gt; +#define kvm_pgtable_walk_begin	rcu_read_lock
&gt; +#define kvm_pgtable_walk_end	rcu_read_unlock
&gt; +#define kvm_dereference_ptep	rcu_dereference
&gt; +
&gt; +#endif	/* defined(__KVM_NVHE_HYPERVISOR__) */
&gt; +
&gt;  static inline kvm_pte_t kvm_pte_read(kvm_pte_t *ptep)
&gt;  {
&gt; -	return READ_ONCE(*ptep);
&gt; +	kvm_pte_t __rcu *p = (kvm_pte_t __rcu *)ptep;
&gt; +
&gt; +	return READ_ONCE(*kvm_dereference_ptep(p));
</span>
What about all the other places where page table memory is accessed?

If RCU is going to be used to protect page table memory, then all
accesses have to go under an RCU critical section. This means that page
table memory should only be accessed through __rcu annotated pointers
and dereferenced with rcu_dereference().

<span class=q>&gt;  }
&gt;  
&gt;  #endif	/* __ARM64_KVM_PGTABLE_H__ */
&gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; index f911509e6512..215a14c434ed 100644
&gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; @@ -284,8 +284,13 @@ int kvm_pgtable_walk(struct kvm_pgtable *pgt, u64 addr, u64 size,
&gt;  		.end	= PAGE_ALIGN(walk_data.addr + size),
&gt;  		.walker	= walker,
&gt;  	};
&gt; +	int r;
&gt;  
&gt; -	return _kvm_pgtable_walk(&amp;walk_data);
&gt; +	kvm_pgtable_walk_begin();
&gt; +	r = _kvm_pgtable_walk(&amp;walk_data);
&gt; +	kvm_pgtable_walk_end();
&gt; +
&gt; +	return r;
&gt;  }
&gt;  
&gt;  struct leaf_walk_data {
&gt; -- 
&gt; 2.37.2.672.g94769d06f0-goog
&gt; 
</span>_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m937ef34a7f44d4724cc32b00672074194178449e id=e937ef34a7f44d4724cc32b00672074194178449e>^</a> <a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/t/#u><b>nested</b></a>] <a href=#r937ef34a7f44d4724cc32b00672074194178449e>96+ messages in thread</a></pre><li><pre><a href=#e937ef34a7f44d4724cc32b00672074194178449e id=m937ef34a7f44d4724cc32b00672074194178449e>*</a> <b>Re: [PATCH 08/14] KVM: arm64: Protect page table traversal with RCU</b>
<b>@ 2022-09-07 21:47     ` David Matlack</b>
  <a href=#r937ef34a7f44d4724cc32b00672074194178449e>0 siblings, 0 replies; 96+ messages in thread</a>
From: David Matlack @ 2022-09-07 21:47 UTC (<a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

On Tue, Aug 30, 2022 at 07:41:26PM +0000, Oliver Upton wrote:
<span class=q>&gt; The use of RCU is necessary to change the paging structures in parallel.
&gt; Acquire and release an RCU read lock when traversing the page tables.
&gt; 
&gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; ---
&gt;  arch/arm64/include/asm/kvm_pgtable.h | 19 ++++++++++++++++++-
&gt;  arch/arm64/kvm/hyp/pgtable.c         |  7 ++++++-
&gt;  2 files changed, 24 insertions(+), 2 deletions(-)
&gt; 
&gt; diff --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
&gt; index 78fbb7be1af6..7d2de0a98ccb 100644
&gt; --- a/arch/arm64/include/asm/kvm_pgtable.h
&gt; +++ b/arch/arm64/include/asm/kvm_pgtable.h
&gt; @@ -578,9 +578,26 @@ enum kvm_pgtable_prot kvm_pgtable_stage2_pte_prot(kvm_pte_t pte);
&gt;   */
&gt;  enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte);
&gt;  
&gt; +#if defined(__KVM_NVHE_HYPERVISOR___)
&gt; +
</span>
Future readers will wonder why NVHE stubs out RCU support and how that
is even correct. Some comments here would be useful explain it.

<span class=q>&gt; +static inline void kvm_pgtable_walk_begin(void) {}
&gt; +static inline void kvm_pgtable_walk_end(void) {}
&gt; +
&gt; +#define kvm_dereference_ptep rcu_dereference_raw
</span>
How does NVHE have access rcu_dereference_raw()?

<span class=q>&gt; +
&gt; +#else	/* !defined(__KVM_NVHE_HYPERVISOR__) */
&gt; +
&gt; +#define kvm_pgtable_walk_begin	rcu_read_lock
&gt; +#define kvm_pgtable_walk_end	rcu_read_unlock
&gt; +#define kvm_dereference_ptep	rcu_dereference
&gt; +
&gt; +#endif	/* defined(__KVM_NVHE_HYPERVISOR__) */
&gt; +
&gt;  static inline kvm_pte_t kvm_pte_read(kvm_pte_t *ptep)
&gt;  {
&gt; -	return READ_ONCE(*ptep);
&gt; +	kvm_pte_t __rcu *p = (kvm_pte_t __rcu *)ptep;
&gt; +
&gt; +	return READ_ONCE(*kvm_dereference_ptep(p));
</span>
What about all the other places where page table memory is accessed?

If RCU is going to be used to protect page table memory, then all
accesses have to go under an RCU critical section. This means that page
table memory should only be accessed through __rcu annotated pointers
and dereferenced with rcu_dereference().

<span class=q>&gt;  }
&gt;  
&gt;  #endif	/* __ARM64_KVM_PGTABLE_H__ */
&gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; index f911509e6512..215a14c434ed 100644
&gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; @@ -284,8 +284,13 @@ int kvm_pgtable_walk(struct kvm_pgtable *pgt, u64 addr, u64 size,
&gt;  		.end	= PAGE_ALIGN(walk_data.addr + size),
&gt;  		.walker	= walker,
&gt;  	};
&gt; +	int r;
&gt;  
&gt; -	return _kvm_pgtable_walk(&amp;walk_data);
&gt; +	kvm_pgtable_walk_begin();
&gt; +	r = _kvm_pgtable_walk(&amp;walk_data);
&gt; +	kvm_pgtable_walk_end();
&gt; +
&gt; +	return r;
&gt;  }
&gt;  
&gt;  struct leaf_walk_data {
&gt; -- 
&gt; 2.37.2.672.g94769d06f0-goog
&gt; 
</span>
_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m937ef34a7f44d4724cc32b00672074194178449e id=e937ef34a7f44d4724cc32b00672074194178449e>^</a> <a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxkRXLsLuhjBNanT@google.com/t/#u><b>nested</b></a>] <a href=#r937ef34a7f44d4724cc32b00672074194178449e>96+ messages in thread</a></pre><li><pre><a href=#ea09f4a08fe867eda2557c47d2ab6c40f959980d8 id=ma09f4a08fe867eda2557c47d2ab6c40f959980d8>*</a> <b>Re: [PATCH 08/14] KVM: arm64: Protect page table traversal with RCU</b>
  2022-09-07 21:47     ` <a href=#m937ef34a7f44d4724cc32b00672074194178449e>David Matlack</a>
  (?)
<b>@ 2022-09-09  9:55       ` Oliver Upton</b>
  <a href=#ra09f4a08fe867eda2557c47d2ab6c40f959980d8>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-09-09  9:55 UTC (<a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/raw>raw</a>)
  To: David Matlack
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

On Wed, Sep 07, 2022 at 02:47:08PM -0700, David Matlack wrote:
<span class=q>&gt; On Tue, Aug 30, 2022 at 07:41:26PM +0000, Oliver Upton wrote:
&gt; &gt; The use of RCU is necessary to change the paging structures in parallel.
&gt; &gt; Acquire and release an RCU read lock when traversing the page tables.
&gt; &gt; 
&gt; &gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; &gt; ---
&gt; &gt;  arch/arm64/include/asm/kvm_pgtable.h | 19 ++++++++++++++++++-
&gt; &gt;  arch/arm64/kvm/hyp/pgtable.c         |  7 ++++++-
&gt; &gt;  2 files changed, 24 insertions(+), 2 deletions(-)
&gt; &gt; 
&gt; &gt; diff --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
&gt; &gt; index 78fbb7be1af6..7d2de0a98ccb 100644
&gt; &gt; --- a/arch/arm64/include/asm/kvm_pgtable.h
&gt; &gt; +++ b/arch/arm64/include/asm/kvm_pgtable.h
&gt; &gt; @@ -578,9 +578,26 @@ enum kvm_pgtable_prot kvm_pgtable_stage2_pte_prot(kvm_pte_t pte);
&gt; &gt;   */
&gt; &gt;  enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte);
&gt; &gt;  
&gt; &gt; +#if defined(__KVM_NVHE_HYPERVISOR___)
&gt; &gt; +
&gt; 
&gt; Future readers will wonder why NVHE stubs out RCU support and how that
&gt; is even correct. Some comments here would be useful explain it.
</span>
Good point.

<span class=q>&gt; &gt; +static inline void kvm_pgtable_walk_begin(void) {}
&gt; &gt; +static inline void kvm_pgtable_walk_end(void) {}
&gt; &gt; +
&gt; &gt; +#define kvm_dereference_ptep rcu_dereference_raw
&gt; 
&gt; How does NVHE have access rcu_dereference_raw()?
</span>
rcu_dereference_raw() is inlined and simply recasts the pointer into the
kernel address space.

Perhaps it is less confusing to template this on kvm_pte_read() to avoid
polluting nVHE with an otherwise benign reference to RCU.

<span class=q>&gt; &gt; +
&gt; &gt; +#else	/* !defined(__KVM_NVHE_HYPERVISOR__) */
&gt; &gt; +
&gt; &gt; +#define kvm_pgtable_walk_begin	rcu_read_lock
&gt; &gt; +#define kvm_pgtable_walk_end	rcu_read_unlock
&gt; &gt; +#define kvm_dereference_ptep	rcu_dereference
&gt; &gt; +
&gt; &gt; +#endif	/* defined(__KVM_NVHE_HYPERVISOR__) */
&gt; &gt; +
&gt; &gt;  static inline kvm_pte_t kvm_pte_read(kvm_pte_t *ptep)
&gt; &gt;  {
&gt; &gt; -	return READ_ONCE(*ptep);
&gt; &gt; +	kvm_pte_t __rcu *p = (kvm_pte_t __rcu *)ptep;
&gt; &gt; +
&gt; &gt; +	return READ_ONCE(*kvm_dereference_ptep(p));
&gt; 
&gt; What about all the other places where page table memory is accessed?
&gt; 
&gt; If RCU is going to be used to protect page table memory, then all
&gt; accesses have to go under an RCU critical section. This means that page
&gt; table memory should only be accessed through __rcu annotated pointers
&gt; and dereferenced with rcu_dereference().
</span>
Let me play around with this a bit, as the annoying part is trying to
sprinkle in RCU annotations w/o messing with nVHE. 

--
Thanks,
Oliver

<a href=#ma09f4a08fe867eda2557c47d2ab6c40f959980d8 id=ea09f4a08fe867eda2557c47d2ab6c40f959980d8>^</a> <a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/t/#u><b>nested</b></a>] <a href=#ra09f4a08fe867eda2557c47d2ab6c40f959980d8>96+ messages in thread</a></pre><li><ul><li><pre><a href=#ea09f4a08fe867eda2557c47d2ab6c40f959980d8 id=ma09f4a08fe867eda2557c47d2ab6c40f959980d8>*</a> <b>Re: [PATCH 08/14] KVM: arm64: Protect page table traversal with RCU</b>
<b>@ 2022-09-09  9:55       ` Oliver Upton</b>
  <a href=#ra09f4a08fe867eda2557c47d2ab6c40f959980d8>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-09-09  9:55 UTC (<a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/raw>raw</a>)
  To: David Matlack
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

On Wed, Sep 07, 2022 at 02:47:08PM -0700, David Matlack wrote:
<span class=q>&gt; On Tue, Aug 30, 2022 at 07:41:26PM +0000, Oliver Upton wrote:
&gt; &gt; The use of RCU is necessary to change the paging structures in parallel.
&gt; &gt; Acquire and release an RCU read lock when traversing the page tables.
&gt; &gt; 
&gt; &gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; &gt; ---
&gt; &gt;  arch/arm64/include/asm/kvm_pgtable.h | 19 ++++++++++++++++++-
&gt; &gt;  arch/arm64/kvm/hyp/pgtable.c         |  7 ++++++-
&gt; &gt;  2 files changed, 24 insertions(+), 2 deletions(-)
&gt; &gt; 
&gt; &gt; diff --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
&gt; &gt; index 78fbb7be1af6..7d2de0a98ccb 100644
&gt; &gt; --- a/arch/arm64/include/asm/kvm_pgtable.h
&gt; &gt; +++ b/arch/arm64/include/asm/kvm_pgtable.h
&gt; &gt; @@ -578,9 +578,26 @@ enum kvm_pgtable_prot kvm_pgtable_stage2_pte_prot(kvm_pte_t pte);
&gt; &gt;   */
&gt; &gt;  enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte);
&gt; &gt;  
&gt; &gt; +#if defined(__KVM_NVHE_HYPERVISOR___)
&gt; &gt; +
&gt; 
&gt; Future readers will wonder why NVHE stubs out RCU support and how that
&gt; is even correct. Some comments here would be useful explain it.
</span>
Good point.

<span class=q>&gt; &gt; +static inline void kvm_pgtable_walk_begin(void) {}
&gt; &gt; +static inline void kvm_pgtable_walk_end(void) {}
&gt; &gt; +
&gt; &gt; +#define kvm_dereference_ptep rcu_dereference_raw
&gt; 
&gt; How does NVHE have access rcu_dereference_raw()?
</span>
rcu_dereference_raw() is inlined and simply recasts the pointer into the
kernel address space.

Perhaps it is less confusing to template this on kvm_pte_read() to avoid
polluting nVHE with an otherwise benign reference to RCU.

<span class=q>&gt; &gt; +
&gt; &gt; +#else	/* !defined(__KVM_NVHE_HYPERVISOR__) */
&gt; &gt; +
&gt; &gt; +#define kvm_pgtable_walk_begin	rcu_read_lock
&gt; &gt; +#define kvm_pgtable_walk_end	rcu_read_unlock
&gt; &gt; +#define kvm_dereference_ptep	rcu_dereference
&gt; &gt; +
&gt; &gt; +#endif	/* defined(__KVM_NVHE_HYPERVISOR__) */
&gt; &gt; +
&gt; &gt;  static inline kvm_pte_t kvm_pte_read(kvm_pte_t *ptep)
&gt; &gt;  {
&gt; &gt; -	return READ_ONCE(*ptep);
&gt; &gt; +	kvm_pte_t __rcu *p = (kvm_pte_t __rcu *)ptep;
&gt; &gt; +
&gt; &gt; +	return READ_ONCE(*kvm_dereference_ptep(p));
&gt; 
&gt; What about all the other places where page table memory is accessed?
&gt; 
&gt; If RCU is going to be used to protect page table memory, then all
&gt; accesses have to go under an RCU critical section. This means that page
&gt; table memory should only be accessed through __rcu annotated pointers
&gt; and dereferenced with rcu_dereference().
</span>
Let me play around with this a bit, as the annoying part is trying to
sprinkle in RCU annotations w/o messing with nVHE. 

--
Thanks,
Oliver

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#ma09f4a08fe867eda2557c47d2ab6c40f959980d8 id=ea09f4a08fe867eda2557c47d2ab6c40f959980d8>^</a> <a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/t/#u><b>nested</b></a>] <a href=#ra09f4a08fe867eda2557c47d2ab6c40f959980d8>96+ messages in thread</a></pre><li><pre><a href=#ea09f4a08fe867eda2557c47d2ab6c40f959980d8 id=ma09f4a08fe867eda2557c47d2ab6c40f959980d8>*</a> <b>Re: [PATCH 08/14] KVM: arm64: Protect page table traversal with RCU</b>
<b>@ 2022-09-09  9:55       ` Oliver Upton</b>
  <a href=#ra09f4a08fe867eda2557c47d2ab6c40f959980d8>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-09-09  9:55 UTC (<a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/raw>raw</a>)
  To: David Matlack
  Cc: kvm, Marc Zyngier, linux-kernel, Catalin Marinas, Ben Gardon,
	Paolo Bonzini, Will Deacon, kvmarm, linux-arm-kernel

On Wed, Sep 07, 2022 at 02:47:08PM -0700, David Matlack wrote:
<span class=q>&gt; On Tue, Aug 30, 2022 at 07:41:26PM +0000, Oliver Upton wrote:
&gt; &gt; The use of RCU is necessary to change the paging structures in parallel.
&gt; &gt; Acquire and release an RCU read lock when traversing the page tables.
&gt; &gt; 
&gt; &gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; &gt; ---
&gt; &gt;  arch/arm64/include/asm/kvm_pgtable.h | 19 ++++++++++++++++++-
&gt; &gt;  arch/arm64/kvm/hyp/pgtable.c         |  7 ++++++-
&gt; &gt;  2 files changed, 24 insertions(+), 2 deletions(-)
&gt; &gt; 
&gt; &gt; diff --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
&gt; &gt; index 78fbb7be1af6..7d2de0a98ccb 100644
&gt; &gt; --- a/arch/arm64/include/asm/kvm_pgtable.h
&gt; &gt; +++ b/arch/arm64/include/asm/kvm_pgtable.h
&gt; &gt; @@ -578,9 +578,26 @@ enum kvm_pgtable_prot kvm_pgtable_stage2_pte_prot(kvm_pte_t pte);
&gt; &gt;   */
&gt; &gt;  enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte);
&gt; &gt;  
&gt; &gt; +#if defined(__KVM_NVHE_HYPERVISOR___)
&gt; &gt; +
&gt; 
&gt; Future readers will wonder why NVHE stubs out RCU support and how that
&gt; is even correct. Some comments here would be useful explain it.
</span>
Good point.

<span class=q>&gt; &gt; +static inline void kvm_pgtable_walk_begin(void) {}
&gt; &gt; +static inline void kvm_pgtable_walk_end(void) {}
&gt; &gt; +
&gt; &gt; +#define kvm_dereference_ptep rcu_dereference_raw
&gt; 
&gt; How does NVHE have access rcu_dereference_raw()?
</span>
rcu_dereference_raw() is inlined and simply recasts the pointer into the
kernel address space.

Perhaps it is less confusing to template this on kvm_pte_read() to avoid
polluting nVHE with an otherwise benign reference to RCU.

<span class=q>&gt; &gt; +
&gt; &gt; +#else	/* !defined(__KVM_NVHE_HYPERVISOR__) */
&gt; &gt; +
&gt; &gt; +#define kvm_pgtable_walk_begin	rcu_read_lock
&gt; &gt; +#define kvm_pgtable_walk_end	rcu_read_unlock
&gt; &gt; +#define kvm_dereference_ptep	rcu_dereference
&gt; &gt; +
&gt; &gt; +#endif	/* defined(__KVM_NVHE_HYPERVISOR__) */
&gt; &gt; +
&gt; &gt;  static inline kvm_pte_t kvm_pte_read(kvm_pte_t *ptep)
&gt; &gt;  {
&gt; &gt; -	return READ_ONCE(*ptep);
&gt; &gt; +	kvm_pte_t __rcu *p = (kvm_pte_t __rcu *)ptep;
&gt; &gt; +
&gt; &gt; +	return READ_ONCE(*kvm_dereference_ptep(p));
&gt; 
&gt; What about all the other places where page table memory is accessed?
&gt; 
&gt; If RCU is going to be used to protect page table memory, then all
&gt; accesses have to go under an RCU critical section. This means that page
&gt; table memory should only be accessed through __rcu annotated pointers
&gt; and dereferenced with rcu_dereference().
</span>
Let me play around with this a bit, as the annoying part is trying to
sprinkle in RCU annotations w/o messing with nVHE. 

--
Thanks,
Oliver
_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#ma09f4a08fe867eda2557c47d2ab6c40f959980d8 id=ea09f4a08fe867eda2557c47d2ab6c40f959980d8>^</a> <a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxsNr+79UUm5Go9x@google.com/t/#u><b>nested</b></a>] <a href=#ra09f4a08fe867eda2557c47d2ab6c40f959980d8>96+ messages in thread</a></pre></ul></ul></ul><li><pre><a href=#e10409b482b1166e85de5a4cc2b432e6de2e770c8 id=m10409b482b1166e85de5a4cc2b432e6de2e770c8>*</a> <b>[PATCH 09/14] KVM: arm64: Free removed stage-2 tables in RCU callback</b>
  2022-08-30 19:41 ` <a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb>Oliver Upton</a>
  (?)
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r10409b482b1166e85de5a4cc2b432e6de2e770c8>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-kernel, kvmarm, linux-arm-kernel, kvm

There is no real urgency to free a stage-2 subtree that was pruned.
Nonetheless, KVM does the tear down in the stage-2 fault path while
holding the MMU lock.

Free removed stage-2 subtrees after an RCU grace period. To guarantee
all stage-2 table pages are freed before killing a VM, add an
rcu_barrier() to the flush path.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-10-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c href=#Z2e.:..:20220830194132.962932-10-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c>arch/arm64/kvm/mmu.c</a> | 35 ++++++++++++++++++++++++++++++++++-
 1 file <a href=#e10409b482b1166e85de5a4cc2b432e6de2e770c8>changed</a>, 34 insertions(+), 1 deletion(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-10-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c id=Z2e.:..:20220830194132.962932-10-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c>diff</a> --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index 91521f4aab97..265951c05879 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
</span><span class=hunk>@@ -97,6 +97,38 @@ static void *stage2_memcache_zalloc_page(void *arg)
</span> 	return kvm_mmu_memory_cache_alloc(mc);
 }
 
<span class=add>+#define STAGE2_PAGE_PRIVATE_LEVEL_MASK	GENMASK_ULL(2, 0)
+
+static inline unsigned long stage2_page_private(u32 level, void *arg)
+{
+	unsigned long pvt = (unsigned long)arg;
+
+	BUILD_BUG_ON(KVM_PGTABLE_MAX_LEVELS &gt; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
+	WARN_ON_ONCE(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
+
+	return pvt | level;
+}
+
+static void stage2_free_removed_table_rcu_cb(struct rcu_head *head)
+{
+	struct page *page = container_of(head, struct page, rcu_head);
+	unsigned long pvt = page_private(page);
+	void *arg = (void *)(pvt &amp; ~STAGE2_PAGE_PRIVATE_LEVEL_MASK);
+	u32 level = (u32)(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
+	void *pgtable = page_to_virt(page);
+
+	kvm_pgtable_stage2_free_removed(pgtable, level, arg);
+}
+
+static void stage2_free_removed_table(void *pgtable, u32 level, void *arg)
+{
+	unsigned long pvt = stage2_page_private(level, arg);
+	struct page *page = virt_to_page(pgtable);
+
+	set_page_private(page, (unsigned long)pvt);
+	call_rcu(&amp;page-&gt;rcu_head, stage2_free_removed_table_rcu_cb);
+}
+
</span> static void *kvm_host_zalloc_pages_exact(size_t size)
 {
 	return alloc_pages_exact(size, GFP_KERNEL_ACCOUNT | __GFP_ZERO);
<span class=hunk>@@ -627,7 +659,7 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
</span> 	.zalloc_page		= stage2_memcache_zalloc_page,
 	.zalloc_pages_exact	= kvm_host_zalloc_pages_exact,
 	.free_pages_exact	= free_pages_exact,
<span class=del>-	.free_removed_table	= kvm_pgtable_stage2_free_removed,
</span><span class=add>+	.free_removed_table	= stage2_free_removed_table,
</span> 	.get_page		= kvm_host_get_page,
 	.put_page		= kvm_host_put_page,
 	.page_count		= kvm_host_page_count,
<span class=hunk>@@ -770,6 +802,7 @@ void kvm_free_stage2_pgd(struct kvm_s2_mmu *mmu)
</span> 	if (pgt) {
 		kvm_pgtable_stage2_destroy(pgt);
 		kfree(pgt);
<span class=add>+		rcu_barrier();
</span> 	}
 }
 
-- 
2.37.2.672.g94769d06f0-goog

_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m10409b482b1166e85de5a4cc2b432e6de2e770c8 id=e10409b482b1166e85de5a4cc2b432e6de2e770c8>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r10409b482b1166e85de5a4cc2b432e6de2e770c8>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e10409b482b1166e85de5a4cc2b432e6de2e770c8 id=m10409b482b1166e85de5a4cc2b432e6de2e770c8>*</a> <b>[PATCH 09/14] KVM: arm64: Free removed stage-2 tables in RCU callback</b>
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r10409b482b1166e85de5a4cc2b432e6de2e770c8>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-arm-kernel, kvmarm, kvm, linux-kernel

There is no real urgency to free a stage-2 subtree that was pruned.
Nonetheless, KVM does the tear down in the stage-2 fault path while
holding the MMU lock.

Free removed stage-2 subtrees after an RCU grace period. To guarantee
all stage-2 table pages are freed before killing a VM, add an
rcu_barrier() to the flush path.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-10-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c href=#Z2e.:..:20220830194132.962932-10-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c>arch/arm64/kvm/mmu.c</a> | 35 ++++++++++++++++++++++++++++++++++-
 1 file <a href=#e10409b482b1166e85de5a4cc2b432e6de2e770c8>changed</a>, 34 insertions(+), 1 deletion(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-10-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c id=Z2e.:..:20220830194132.962932-10-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c>diff</a> --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index 91521f4aab97..265951c05879 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
</span><span class=hunk>@@ -97,6 +97,38 @@ static void *stage2_memcache_zalloc_page(void *arg)
</span> 	return kvm_mmu_memory_cache_alloc(mc);
 }
 
<span class=add>+#define STAGE2_PAGE_PRIVATE_LEVEL_MASK	GENMASK_ULL(2, 0)
+
+static inline unsigned long stage2_page_private(u32 level, void *arg)
+{
+	unsigned long pvt = (unsigned long)arg;
+
+	BUILD_BUG_ON(KVM_PGTABLE_MAX_LEVELS &gt; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
+	WARN_ON_ONCE(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
+
+	return pvt | level;
+}
+
+static void stage2_free_removed_table_rcu_cb(struct rcu_head *head)
+{
+	struct page *page = container_of(head, struct page, rcu_head);
+	unsigned long pvt = page_private(page);
+	void *arg = (void *)(pvt &amp; ~STAGE2_PAGE_PRIVATE_LEVEL_MASK);
+	u32 level = (u32)(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
+	void *pgtable = page_to_virt(page);
+
+	kvm_pgtable_stage2_free_removed(pgtable, level, arg);
+}
+
+static void stage2_free_removed_table(void *pgtable, u32 level, void *arg)
+{
+	unsigned long pvt = stage2_page_private(level, arg);
+	struct page *page = virt_to_page(pgtable);
+
+	set_page_private(page, (unsigned long)pvt);
+	call_rcu(&amp;page-&gt;rcu_head, stage2_free_removed_table_rcu_cb);
+}
+
</span> static void *kvm_host_zalloc_pages_exact(size_t size)
 {
 	return alloc_pages_exact(size, GFP_KERNEL_ACCOUNT | __GFP_ZERO);
<span class=hunk>@@ -627,7 +659,7 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
</span> 	.zalloc_page		= stage2_memcache_zalloc_page,
 	.zalloc_pages_exact	= kvm_host_zalloc_pages_exact,
 	.free_pages_exact	= free_pages_exact,
<span class=del>-	.free_removed_table	= kvm_pgtable_stage2_free_removed,
</span><span class=add>+	.free_removed_table	= stage2_free_removed_table,
</span> 	.get_page		= kvm_host_get_page,
 	.put_page		= kvm_host_put_page,
 	.page_count		= kvm_host_page_count,
<span class=hunk>@@ -770,6 +802,7 @@ void kvm_free_stage2_pgd(struct kvm_s2_mmu *mmu)
</span> 	if (pgt) {
 		kvm_pgtable_stage2_destroy(pgt);
 		kfree(pgt);
<span class=add>+		rcu_barrier();
</span> 	}
 }
 
-- 
2.37.2.672.g94769d06f0-goog


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m10409b482b1166e85de5a4cc2b432e6de2e770c8 id=e10409b482b1166e85de5a4cc2b432e6de2e770c8>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r10409b482b1166e85de5a4cc2b432e6de2e770c8>96+ messages in thread</a></pre><li><pre><a href=#e10409b482b1166e85de5a4cc2b432e6de2e770c8 id=m10409b482b1166e85de5a4cc2b432e6de2e770c8>*</a> <b>[PATCH 09/14] KVM: arm64: Free removed stage-2 tables in RCU callback</b>
<b>@ 2022-08-30 19:41   ` Oliver Upton</b>
  <a href=#r10409b482b1166e85de5a4cc2b432e6de2e770c8>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:41 UTC (<a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, Oliver Upton
  Cc: linux-arm-kernel, kvmarm, kvm, linux-kernel

There is no real urgency to free a stage-2 subtree that was pruned.
Nonetheless, KVM does the tear down in the stage-2 fault path while
holding the MMU lock.

Free removed stage-2 subtrees after an RCU grace period. To guarantee
all stage-2 table pages are freed before killing a VM, add an
rcu_barrier() to the flush path.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830194132.962932-10-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c href=#Z2e.:..:20220830194132.962932-10-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c>arch/arm64/kvm/mmu.c</a> | 35 ++++++++++++++++++++++++++++++++++-
 1 file <a href=#e10409b482b1166e85de5a4cc2b432e6de2e770c8>changed</a>, 34 insertions(+), 1 deletion(-)

<span class=head><a href=#iZ2e.:..:20220830194132.962932-10-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c id=Z2e.:..:20220830194132.962932-10-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c>diff</a> --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index 91521f4aab97..265951c05879 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
</span><span class=hunk>@@ -97,6 +97,38 @@ static void *stage2_memcache_zalloc_page(void *arg)
</span> 	return kvm_mmu_memory_cache_alloc(mc);
 }
 
<span class=add>+#define STAGE2_PAGE_PRIVATE_LEVEL_MASK	GENMASK_ULL(2, 0)
+
+static inline unsigned long stage2_page_private(u32 level, void *arg)
+{
+	unsigned long pvt = (unsigned long)arg;
+
+	BUILD_BUG_ON(KVM_PGTABLE_MAX_LEVELS &gt; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
+	WARN_ON_ONCE(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
+
+	return pvt | level;
+}
+
+static void stage2_free_removed_table_rcu_cb(struct rcu_head *head)
+{
+	struct page *page = container_of(head, struct page, rcu_head);
+	unsigned long pvt = page_private(page);
+	void *arg = (void *)(pvt &amp; ~STAGE2_PAGE_PRIVATE_LEVEL_MASK);
+	u32 level = (u32)(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
+	void *pgtable = page_to_virt(page);
+
+	kvm_pgtable_stage2_free_removed(pgtable, level, arg);
+}
+
+static void stage2_free_removed_table(void *pgtable, u32 level, void *arg)
+{
+	unsigned long pvt = stage2_page_private(level, arg);
+	struct page *page = virt_to_page(pgtable);
+
+	set_page_private(page, (unsigned long)pvt);
+	call_rcu(&amp;page-&gt;rcu_head, stage2_free_removed_table_rcu_cb);
+}
+
</span> static void *kvm_host_zalloc_pages_exact(size_t size)
 {
 	return alloc_pages_exact(size, GFP_KERNEL_ACCOUNT | __GFP_ZERO);
<span class=hunk>@@ -627,7 +659,7 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
</span> 	.zalloc_page		= stage2_memcache_zalloc_page,
 	.zalloc_pages_exact	= kvm_host_zalloc_pages_exact,
 	.free_pages_exact	= free_pages_exact,
<span class=del>-	.free_removed_table	= kvm_pgtable_stage2_free_removed,
</span><span class=add>+	.free_removed_table	= stage2_free_removed_table,
</span> 	.get_page		= kvm_host_get_page,
 	.put_page		= kvm_host_put_page,
 	.page_count		= kvm_host_page_count,
<span class=hunk>@@ -770,6 +802,7 @@ void kvm_free_stage2_pgd(struct kvm_s2_mmu *mmu)
</span> 	if (pgt) {
 		kvm_pgtable_stage2_destroy(pgt);
 		kfree(pgt);
<span class=add>+		rcu_barrier();
</span> 	}
 }
 
-- 
2.37.2.672.g94769d06f0-goog


<a href=#m10409b482b1166e85de5a4cc2b432e6de2e770c8 id=e10409b482b1166e85de5a4cc2b432e6de2e770c8>^</a> <a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830194132.962932-10-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r10409b482b1166e85de5a4cc2b432e6de2e770c8>96+ messages in thread</a></pre><li><pre><a href=#e6886d4365210102a4cbe3b8bf47c108818ab632a id=m6886d4365210102a4cbe3b8bf47c108818ab632a>*</a> <b>Re: [PATCH 09/14] KVM: arm64: Free removed stage-2 tables in RCU callback</b>
  2022-08-30 19:41   ` <a href=#m10409b482b1166e85de5a4cc2b432e6de2e770c8>Oliver Upton</a>
  (?)
<b>@ 2022-09-07 22:00     ` David Matlack</b>
  <a href=#r6886d4365210102a4cbe3b8bf47c108818ab632a>-1 siblings, 0 replies; 96+ messages in thread</a>
From: David Matlack @ 2022-09-07 22:00 UTC (<a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

On Tue, Aug 30, 2022 at 07:41:27PM +0000, Oliver Upton wrote:
<span class=q>&gt; There is no real urgency to free a stage-2 subtree that was pruned.
&gt; Nonetheless, KVM does the tear down in the stage-2 fault path while
&gt; holding the MMU lock.
&gt; 
&gt; Free removed stage-2 subtrees after an RCU grace period. To guarantee
&gt; all stage-2 table pages are freed before killing a VM, add an
&gt; rcu_barrier() to the flush path.
&gt; 
&gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; ---
&gt;  arch/arm64/kvm/mmu.c | 35 ++++++++++++++++++++++++++++++++++-
&gt;  1 file changed, 34 insertions(+), 1 deletion(-)
&gt; 
&gt; diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
&gt; index 91521f4aab97..265951c05879 100644
&gt; --- a/arch/arm64/kvm/mmu.c
&gt; +++ b/arch/arm64/kvm/mmu.c
&gt; @@ -97,6 +97,38 @@ static void *stage2_memcache_zalloc_page(void *arg)
&gt;  	return kvm_mmu_memory_cache_alloc(mc);
&gt;  }
&gt;  
&gt; +#define STAGE2_PAGE_PRIVATE_LEVEL_MASK	GENMASK_ULL(2, 0)
&gt; +
&gt; +static inline unsigned long stage2_page_private(u32 level, void *arg)
&gt; +{
&gt; +	unsigned long pvt = (unsigned long)arg;
&gt; +
&gt; +	BUILD_BUG_ON(KVM_PGTABLE_MAX_LEVELS &gt; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +	WARN_ON_ONCE(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +
&gt; +	return pvt | level;
&gt; +}
&gt; +
&gt; +static void stage2_free_removed_table_rcu_cb(struct rcu_head *head)
&gt; +{
&gt; +	struct page *page = container_of(head, struct page, rcu_head);
&gt; +	unsigned long pvt = page_private(page);
&gt; +	void *arg = (void *)(pvt &amp; ~STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +	u32 level = (u32)(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +	void *pgtable = page_to_virt(page);
&gt; +
&gt; +	kvm_pgtable_stage2_free_removed(pgtable, level, arg);
&gt; +}
&gt; +
&gt; +static void stage2_free_removed_table(void *pgtable, u32 level, void *arg)
&gt; +{
&gt; +	unsigned long pvt = stage2_page_private(level, arg);
&gt; +	struct page *page = virt_to_page(pgtable);
&gt; +
&gt; +	set_page_private(page, (unsigned long)pvt);
&gt; +	call_rcu(&amp;page-&gt;rcu_head, stage2_free_removed_table_rcu_cb);
&gt; +}
&gt; +
&gt;  static void *kvm_host_zalloc_pages_exact(size_t size)
&gt;  {
&gt;  	return alloc_pages_exact(size, GFP_KERNEL_ACCOUNT | __GFP_ZERO);
&gt; @@ -627,7 +659,7 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
&gt;  	.zalloc_page		= stage2_memcache_zalloc_page,
&gt;  	.zalloc_pages_exact	= kvm_host_zalloc_pages_exact,
&gt;  	.free_pages_exact	= free_pages_exact,
&gt; -	.free_removed_table	= kvm_pgtable_stage2_free_removed,
&gt; +	.free_removed_table	= stage2_free_removed_table,
&gt;  	.get_page		= kvm_host_get_page,
&gt;  	.put_page		= kvm_host_put_page,
&gt;  	.page_count		= kvm_host_page_count,
&gt; @@ -770,6 +802,7 @@ void kvm_free_stage2_pgd(struct kvm_s2_mmu *mmu)
&gt;  	if (pgt) {
&gt;  		kvm_pgtable_stage2_destroy(pgt);
&gt;  		kfree(pgt);
&gt; +		rcu_barrier();
</span>
A comment here would be useful to document the behavior. e.g.

        /*
         * Wait for all stage-2 page tables that are being freed
         * asynchronously via RCU callback because ...
         */

Speaking of, what's the reason for this rcu_barrier()? Is there any
reason why KVM can't let in-flight stage-2 freeing RCU callbacks run at
the end of the next grace period?

<span class=q>&gt;  	}
&gt;  }
&gt;  
&gt; -- 
&gt; 2.37.2.672.g94769d06f0-goog
&gt; 
</span>
<a href=#m6886d4365210102a4cbe3b8bf47c108818ab632a id=e6886d4365210102a4cbe3b8bf47c108818ab632a>^</a> <a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/t/#u><b>nested</b></a>] <a href=#r6886d4365210102a4cbe3b8bf47c108818ab632a>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e6886d4365210102a4cbe3b8bf47c108818ab632a id=m6886d4365210102a4cbe3b8bf47c108818ab632a>*</a> <b>Re: [PATCH 09/14] KVM: arm64: Free removed stage-2 tables in RCU callback</b>
<b>@ 2022-09-07 22:00     ` David Matlack</b>
  <a href=#r6886d4365210102a4cbe3b8bf47c108818ab632a>0 siblings, 0 replies; 96+ messages in thread</a>
From: David Matlack @ 2022-09-07 22:00 UTC (<a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: kvm, Marc Zyngier, linux-kernel, Catalin Marinas, Ben Gardon,
	Paolo Bonzini, Will Deacon, kvmarm, linux-arm-kernel

On Tue, Aug 30, 2022 at 07:41:27PM +0000, Oliver Upton wrote:
<span class=q>&gt; There is no real urgency to free a stage-2 subtree that was pruned.
&gt; Nonetheless, KVM does the tear down in the stage-2 fault path while
&gt; holding the MMU lock.
&gt; 
&gt; Free removed stage-2 subtrees after an RCU grace period. To guarantee
&gt; all stage-2 table pages are freed before killing a VM, add an
&gt; rcu_barrier() to the flush path.
&gt; 
&gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; ---
&gt;  arch/arm64/kvm/mmu.c | 35 ++++++++++++++++++++++++++++++++++-
&gt;  1 file changed, 34 insertions(+), 1 deletion(-)
&gt; 
&gt; diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
&gt; index 91521f4aab97..265951c05879 100644
&gt; --- a/arch/arm64/kvm/mmu.c
&gt; +++ b/arch/arm64/kvm/mmu.c
&gt; @@ -97,6 +97,38 @@ static void *stage2_memcache_zalloc_page(void *arg)
&gt;  	return kvm_mmu_memory_cache_alloc(mc);
&gt;  }
&gt;  
&gt; +#define STAGE2_PAGE_PRIVATE_LEVEL_MASK	GENMASK_ULL(2, 0)
&gt; +
&gt; +static inline unsigned long stage2_page_private(u32 level, void *arg)
&gt; +{
&gt; +	unsigned long pvt = (unsigned long)arg;
&gt; +
&gt; +	BUILD_BUG_ON(KVM_PGTABLE_MAX_LEVELS &gt; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +	WARN_ON_ONCE(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +
&gt; +	return pvt | level;
&gt; +}
&gt; +
&gt; +static void stage2_free_removed_table_rcu_cb(struct rcu_head *head)
&gt; +{
&gt; +	struct page *page = container_of(head, struct page, rcu_head);
&gt; +	unsigned long pvt = page_private(page);
&gt; +	void *arg = (void *)(pvt &amp; ~STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +	u32 level = (u32)(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +	void *pgtable = page_to_virt(page);
&gt; +
&gt; +	kvm_pgtable_stage2_free_removed(pgtable, level, arg);
&gt; +}
&gt; +
&gt; +static void stage2_free_removed_table(void *pgtable, u32 level, void *arg)
&gt; +{
&gt; +	unsigned long pvt = stage2_page_private(level, arg);
&gt; +	struct page *page = virt_to_page(pgtable);
&gt; +
&gt; +	set_page_private(page, (unsigned long)pvt);
&gt; +	call_rcu(&amp;page-&gt;rcu_head, stage2_free_removed_table_rcu_cb);
&gt; +}
&gt; +
&gt;  static void *kvm_host_zalloc_pages_exact(size_t size)
&gt;  {
&gt;  	return alloc_pages_exact(size, GFP_KERNEL_ACCOUNT | __GFP_ZERO);
&gt; @@ -627,7 +659,7 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
&gt;  	.zalloc_page		= stage2_memcache_zalloc_page,
&gt;  	.zalloc_pages_exact	= kvm_host_zalloc_pages_exact,
&gt;  	.free_pages_exact	= free_pages_exact,
&gt; -	.free_removed_table	= kvm_pgtable_stage2_free_removed,
&gt; +	.free_removed_table	= stage2_free_removed_table,
&gt;  	.get_page		= kvm_host_get_page,
&gt;  	.put_page		= kvm_host_put_page,
&gt;  	.page_count		= kvm_host_page_count,
&gt; @@ -770,6 +802,7 @@ void kvm_free_stage2_pgd(struct kvm_s2_mmu *mmu)
&gt;  	if (pgt) {
&gt;  		kvm_pgtable_stage2_destroy(pgt);
&gt;  		kfree(pgt);
&gt; +		rcu_barrier();
</span>
A comment here would be useful to document the behavior. e.g.

        /*
         * Wait for all stage-2 page tables that are being freed
         * asynchronously via RCU callback because ...
         */

Speaking of, what's the reason for this rcu_barrier()? Is there any
reason why KVM can't let in-flight stage-2 freeing RCU callbacks run at
the end of the next grace period?

<span class=q>&gt;  	}
&gt;  }
&gt;  
&gt; -- 
&gt; 2.37.2.672.g94769d06f0-goog
&gt; 
</span>_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m6886d4365210102a4cbe3b8bf47c108818ab632a id=e6886d4365210102a4cbe3b8bf47c108818ab632a>^</a> <a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/t/#u><b>nested</b></a>] <a href=#r6886d4365210102a4cbe3b8bf47c108818ab632a>96+ messages in thread</a></pre><li><pre><a href=#e6886d4365210102a4cbe3b8bf47c108818ab632a id=m6886d4365210102a4cbe3b8bf47c108818ab632a>*</a> <b>Re: [PATCH 09/14] KVM: arm64: Free removed stage-2 tables in RCU callback</b>
<b>@ 2022-09-07 22:00     ` David Matlack</b>
  <a href=#r6886d4365210102a4cbe3b8bf47c108818ab632a>0 siblings, 0 replies; 96+ messages in thread</a>
From: David Matlack @ 2022-09-07 22:00 UTC (<a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

On Tue, Aug 30, 2022 at 07:41:27PM +0000, Oliver Upton wrote:
<span class=q>&gt; There is no real urgency to free a stage-2 subtree that was pruned.
&gt; Nonetheless, KVM does the tear down in the stage-2 fault path while
&gt; holding the MMU lock.
&gt; 
&gt; Free removed stage-2 subtrees after an RCU grace period. To guarantee
&gt; all stage-2 table pages are freed before killing a VM, add an
&gt; rcu_barrier() to the flush path.
&gt; 
&gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; ---
&gt;  arch/arm64/kvm/mmu.c | 35 ++++++++++++++++++++++++++++++++++-
&gt;  1 file changed, 34 insertions(+), 1 deletion(-)
&gt; 
&gt; diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
&gt; index 91521f4aab97..265951c05879 100644
&gt; --- a/arch/arm64/kvm/mmu.c
&gt; +++ b/arch/arm64/kvm/mmu.c
&gt; @@ -97,6 +97,38 @@ static void *stage2_memcache_zalloc_page(void *arg)
&gt;  	return kvm_mmu_memory_cache_alloc(mc);
&gt;  }
&gt;  
&gt; +#define STAGE2_PAGE_PRIVATE_LEVEL_MASK	GENMASK_ULL(2, 0)
&gt; +
&gt; +static inline unsigned long stage2_page_private(u32 level, void *arg)
&gt; +{
&gt; +	unsigned long pvt = (unsigned long)arg;
&gt; +
&gt; +	BUILD_BUG_ON(KVM_PGTABLE_MAX_LEVELS &gt; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +	WARN_ON_ONCE(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +
&gt; +	return pvt | level;
&gt; +}
&gt; +
&gt; +static void stage2_free_removed_table_rcu_cb(struct rcu_head *head)
&gt; +{
&gt; +	struct page *page = container_of(head, struct page, rcu_head);
&gt; +	unsigned long pvt = page_private(page);
&gt; +	void *arg = (void *)(pvt &amp; ~STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +	u32 level = (u32)(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +	void *pgtable = page_to_virt(page);
&gt; +
&gt; +	kvm_pgtable_stage2_free_removed(pgtable, level, arg);
&gt; +}
&gt; +
&gt; +static void stage2_free_removed_table(void *pgtable, u32 level, void *arg)
&gt; +{
&gt; +	unsigned long pvt = stage2_page_private(level, arg);
&gt; +	struct page *page = virt_to_page(pgtable);
&gt; +
&gt; +	set_page_private(page, (unsigned long)pvt);
&gt; +	call_rcu(&amp;page-&gt;rcu_head, stage2_free_removed_table_rcu_cb);
&gt; +}
&gt; +
&gt;  static void *kvm_host_zalloc_pages_exact(size_t size)
&gt;  {
&gt;  	return alloc_pages_exact(size, GFP_KERNEL_ACCOUNT | __GFP_ZERO);
&gt; @@ -627,7 +659,7 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
&gt;  	.zalloc_page		= stage2_memcache_zalloc_page,
&gt;  	.zalloc_pages_exact	= kvm_host_zalloc_pages_exact,
&gt;  	.free_pages_exact	= free_pages_exact,
&gt; -	.free_removed_table	= kvm_pgtable_stage2_free_removed,
&gt; +	.free_removed_table	= stage2_free_removed_table,
&gt;  	.get_page		= kvm_host_get_page,
&gt;  	.put_page		= kvm_host_put_page,
&gt;  	.page_count		= kvm_host_page_count,
&gt; @@ -770,6 +802,7 @@ void kvm_free_stage2_pgd(struct kvm_s2_mmu *mmu)
&gt;  	if (pgt) {
&gt;  		kvm_pgtable_stage2_destroy(pgt);
&gt;  		kfree(pgt);
&gt; +		rcu_barrier();
</span>
A comment here would be useful to document the behavior. e.g.

        /*
         * Wait for all stage-2 page tables that are being freed
         * asynchronously via RCU callback because ...
         */

Speaking of, what's the reason for this rcu_barrier()? Is there any
reason why KVM can't let in-flight stage-2 freeing RCU callbacks run at
the end of the next grace period?

<span class=q>&gt;  	}
&gt;  }
&gt;  
&gt; -- 
&gt; 2.37.2.672.g94769d06f0-goog
&gt; 
</span>
_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m6886d4365210102a4cbe3b8bf47c108818ab632a id=e6886d4365210102a4cbe3b8bf47c108818ab632a>^</a> <a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxkUciuwLFvByLOu@google.com/t/#u><b>nested</b></a>] <a href=#r6886d4365210102a4cbe3b8bf47c108818ab632a>96+ messages in thread</a></pre><li><pre><a href=#edde2e85c111e7bf38027f66ddceab79cb9619dc0 id=mdde2e85c111e7bf38027f66ddceab79cb9619dc0>*</a> <b>Re: [PATCH 09/14] KVM: arm64: Free removed stage-2 tables in RCU callback</b>
  2022-09-07 22:00     ` <a href=#m6886d4365210102a4cbe3b8bf47c108818ab632a>David Matlack</a>
  (?)
<b>@ 2022-09-08 16:40       ` David Matlack</b>
  <a href=#rdde2e85c111e7bf38027f66ddceab79cb9619dc0>-1 siblings, 0 replies; 96+ messages in thread</a>
From: David Matlack @ 2022-09-08 16:40 UTC (<a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

On Wed, Sep 07, 2022 at 03:00:18PM -0700, David Matlack wrote:
<span class=q>&gt; On Tue, Aug 30, 2022 at 07:41:27PM +0000, Oliver Upton wrote:
&gt; &gt; There is no real urgency to free a stage-2 subtree that was pruned.
&gt; &gt; Nonetheless, KVM does the tear down in the stage-2 fault path while
&gt; &gt; holding the MMU lock.
&gt; &gt; 
&gt; &gt; Free removed stage-2 subtrees after an RCU grace period. To guarantee
&gt; &gt; all stage-2 table pages are freed before killing a VM, add an
&gt; &gt; rcu_barrier() to the flush path.
&gt; &gt; 
&gt; &gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; &gt; ---
&gt; &gt;  arch/arm64/kvm/mmu.c | 35 ++++++++++++++++++++++++++++++++++-
&gt; &gt;  1 file changed, 34 insertions(+), 1 deletion(-)
&gt; &gt; 
&gt; &gt; diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
&gt; &gt; index 91521f4aab97..265951c05879 100644
&gt; &gt; --- a/arch/arm64/kvm/mmu.c
&gt; &gt; +++ b/arch/arm64/kvm/mmu.c
&gt; &gt; @@ -97,6 +97,38 @@ static void *stage2_memcache_zalloc_page(void *arg)
&gt; &gt;  	return kvm_mmu_memory_cache_alloc(mc);
&gt; &gt;  }
&gt; &gt;  
&gt; &gt; +#define STAGE2_PAGE_PRIVATE_LEVEL_MASK	GENMASK_ULL(2, 0)
&gt; &gt; +
&gt; &gt; +static inline unsigned long stage2_page_private(u32 level, void *arg)
&gt; &gt; +{
&gt; &gt; +	unsigned long pvt = (unsigned long)arg;
&gt; &gt; +
&gt; &gt; +	BUILD_BUG_ON(KVM_PGTABLE_MAX_LEVELS &gt; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; &gt; +	WARN_ON_ONCE(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; &gt; +
&gt; &gt; +	return pvt | level;
&gt; &gt; +}
&gt; &gt; +
&gt; &gt; +static void stage2_free_removed_table_rcu_cb(struct rcu_head *head)
&gt; &gt; +{
&gt; &gt; +	struct page *page = container_of(head, struct page, rcu_head);
&gt; &gt; +	unsigned long pvt = page_private(page);
&gt; &gt; +	void *arg = (void *)(pvt &amp; ~STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; &gt; +	u32 level = (u32)(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; &gt; +	void *pgtable = page_to_virt(page);
&gt; &gt; +
&gt; &gt; +	kvm_pgtable_stage2_free_removed(pgtable, level, arg);
&gt; &gt; +}
&gt; &gt; +
&gt; &gt; +static void stage2_free_removed_table(void *pgtable, u32 level, void *arg)
&gt; &gt; +{
&gt; &gt; +	unsigned long pvt = stage2_page_private(level, arg);
&gt; &gt; +	struct page *page = virt_to_page(pgtable);
&gt; &gt; +
&gt; &gt; +	set_page_private(page, (unsigned long)pvt);
&gt; &gt; +	call_rcu(&amp;page-&gt;rcu_head, stage2_free_removed_table_rcu_cb);
&gt; &gt; +}
&gt; &gt; +
&gt; &gt;  static void *kvm_host_zalloc_pages_exact(size_t size)
&gt; &gt;  {
&gt; &gt;  	return alloc_pages_exact(size, GFP_KERNEL_ACCOUNT | __GFP_ZERO);
&gt; &gt; @@ -627,7 +659,7 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
&gt; &gt;  	.zalloc_page		= stage2_memcache_zalloc_page,
&gt; &gt;  	.zalloc_pages_exact	= kvm_host_zalloc_pages_exact,
&gt; &gt;  	.free_pages_exact	= free_pages_exact,
&gt; &gt; -	.free_removed_table	= kvm_pgtable_stage2_free_removed,
&gt; &gt; +	.free_removed_table	= stage2_free_removed_table,
&gt; &gt;  	.get_page		= kvm_host_get_page,
&gt; &gt;  	.put_page		= kvm_host_put_page,
&gt; &gt;  	.page_count		= kvm_host_page_count,
&gt; &gt; @@ -770,6 +802,7 @@ void kvm_free_stage2_pgd(struct kvm_s2_mmu *mmu)
&gt; &gt;  	if (pgt) {
&gt; &gt;  		kvm_pgtable_stage2_destroy(pgt);
&gt; &gt;  		kfree(pgt);
&gt; &gt; +		rcu_barrier();
&gt; 
&gt; A comment here would be useful to document the behavior. e.g.
&gt; 
&gt;         /*
&gt;          * Wait for all stage-2 page tables that are being freed
&gt;          * asynchronously via RCU callback because ...
&gt;          */
&gt; 
&gt; Speaking of, what's the reason for this rcu_barrier()? Is there any
&gt; reason why KVM can't let in-flight stage-2 freeing RCU callbacks run at
&gt; the end of the next grace period?
</span>
After thinking about this more I have 2 follow-up questions:

1. Should the RCU barrier come before kvm_pgtable_stage2_destroy() and
   kfree(pgt)? Otherwise an RCU callback running
   kvm_pgtable_stage2_free_removed() could access the pgt after it has
   been freed?

2. In general, is it safe for kvm_pgtable_stage2_free_removed() to run
   outside of the MMU lock? Yes the page tables have already been
   disconnected from the tree, but kvm_pgtable_stage2_free_removed()
   also accesses shared data structures likstruct kvm_pgtable. I *think*
   it might be safe after you fix (1.) but it would be more robust to
   avoid accessing shared data structures at all outside of the MMU lock
   and just do the page table freeing in the RCU callback.

<span class=q>&gt; 
&gt; &gt;  	}
&gt; &gt;  }
&gt; &gt;  
&gt; &gt; -- 
&gt; &gt; 2.37.2.672.g94769d06f0-goog
&gt; &gt; 
</span>
<a href=#mdde2e85c111e7bf38027f66ddceab79cb9619dc0 id=edde2e85c111e7bf38027f66ddceab79cb9619dc0>^</a> <a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/>permalink</a> <a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/t/#u><b>nested</b></a>] <a href=#rdde2e85c111e7bf38027f66ddceab79cb9619dc0>96+ messages in thread</a></pre><li><ul><li><pre><a href=#edde2e85c111e7bf38027f66ddceab79cb9619dc0 id=mdde2e85c111e7bf38027f66ddceab79cb9619dc0>*</a> <b>Re: [PATCH 09/14] KVM: arm64: Free removed stage-2 tables in RCU callback</b>
<b>@ 2022-09-08 16:40       ` David Matlack</b>
  <a href=#rdde2e85c111e7bf38027f66ddceab79cb9619dc0>0 siblings, 0 replies; 96+ messages in thread</a>
From: David Matlack @ 2022-09-08 16:40 UTC (<a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

On Wed, Sep 07, 2022 at 03:00:18PM -0700, David Matlack wrote:
<span class=q>&gt; On Tue, Aug 30, 2022 at 07:41:27PM +0000, Oliver Upton wrote:
&gt; &gt; There is no real urgency to free a stage-2 subtree that was pruned.
&gt; &gt; Nonetheless, KVM does the tear down in the stage-2 fault path while
&gt; &gt; holding the MMU lock.
&gt; &gt; 
&gt; &gt; Free removed stage-2 subtrees after an RCU grace period. To guarantee
&gt; &gt; all stage-2 table pages are freed before killing a VM, add an
&gt; &gt; rcu_barrier() to the flush path.
&gt; &gt; 
&gt; &gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; &gt; ---
&gt; &gt;  arch/arm64/kvm/mmu.c | 35 ++++++++++++++++++++++++++++++++++-
&gt; &gt;  1 file changed, 34 insertions(+), 1 deletion(-)
&gt; &gt; 
&gt; &gt; diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
&gt; &gt; index 91521f4aab97..265951c05879 100644
&gt; &gt; --- a/arch/arm64/kvm/mmu.c
&gt; &gt; +++ b/arch/arm64/kvm/mmu.c
&gt; &gt; @@ -97,6 +97,38 @@ static void *stage2_memcache_zalloc_page(void *arg)
&gt; &gt;  	return kvm_mmu_memory_cache_alloc(mc);
&gt; &gt;  }
&gt; &gt;  
&gt; &gt; +#define STAGE2_PAGE_PRIVATE_LEVEL_MASK	GENMASK_ULL(2, 0)
&gt; &gt; +
&gt; &gt; +static inline unsigned long stage2_page_private(u32 level, void *arg)
&gt; &gt; +{
&gt; &gt; +	unsigned long pvt = (unsigned long)arg;
&gt; &gt; +
&gt; &gt; +	BUILD_BUG_ON(KVM_PGTABLE_MAX_LEVELS &gt; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; &gt; +	WARN_ON_ONCE(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; &gt; +
&gt; &gt; +	return pvt | level;
&gt; &gt; +}
&gt; &gt; +
&gt; &gt; +static void stage2_free_removed_table_rcu_cb(struct rcu_head *head)
&gt; &gt; +{
&gt; &gt; +	struct page *page = container_of(head, struct page, rcu_head);
&gt; &gt; +	unsigned long pvt = page_private(page);
&gt; &gt; +	void *arg = (void *)(pvt &amp; ~STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; &gt; +	u32 level = (u32)(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; &gt; +	void *pgtable = page_to_virt(page);
&gt; &gt; +
&gt; &gt; +	kvm_pgtable_stage2_free_removed(pgtable, level, arg);
&gt; &gt; +}
&gt; &gt; +
&gt; &gt; +static void stage2_free_removed_table(void *pgtable, u32 level, void *arg)
&gt; &gt; +{
&gt; &gt; +	unsigned long pvt = stage2_page_private(level, arg);
&gt; &gt; +	struct page *page = virt_to_page(pgtable);
&gt; &gt; +
&gt; &gt; +	set_page_private(page, (unsigned long)pvt);
&gt; &gt; +	call_rcu(&amp;page-&gt;rcu_head, stage2_free_removed_table_rcu_cb);
&gt; &gt; +}
&gt; &gt; +
&gt; &gt;  static void *kvm_host_zalloc_pages_exact(size_t size)
&gt; &gt;  {
&gt; &gt;  	return alloc_pages_exact(size, GFP_KERNEL_ACCOUNT | __GFP_ZERO);
&gt; &gt; @@ -627,7 +659,7 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
&gt; &gt;  	.zalloc_page		= stage2_memcache_zalloc_page,
&gt; &gt;  	.zalloc_pages_exact	= kvm_host_zalloc_pages_exact,
&gt; &gt;  	.free_pages_exact	= free_pages_exact,
&gt; &gt; -	.free_removed_table	= kvm_pgtable_stage2_free_removed,
&gt; &gt; +	.free_removed_table	= stage2_free_removed_table,
&gt; &gt;  	.get_page		= kvm_host_get_page,
&gt; &gt;  	.put_page		= kvm_host_put_page,
&gt; &gt;  	.page_count		= kvm_host_page_count,
&gt; &gt; @@ -770,6 +802,7 @@ void kvm_free_stage2_pgd(struct kvm_s2_mmu *mmu)
&gt; &gt;  	if (pgt) {
&gt; &gt;  		kvm_pgtable_stage2_destroy(pgt);
&gt; &gt;  		kfree(pgt);
&gt; &gt; +		rcu_barrier();
&gt; 
&gt; A comment here would be useful to document the behavior. e.g.
&gt; 
&gt;         /*
&gt;          * Wait for all stage-2 page tables that are being freed
&gt;          * asynchronously via RCU callback because ...
&gt;          */
&gt; 
&gt; Speaking of, what's the reason for this rcu_barrier()? Is there any
&gt; reason why KVM can't let in-flight stage-2 freeing RCU callbacks run at
&gt; the end of the next grace period?
</span>
After thinking about this more I have 2 follow-up questions:

1. Should the RCU barrier come before kvm_pgtable_stage2_destroy() and
   kfree(pgt)? Otherwise an RCU callback running
   kvm_pgtable_stage2_free_removed() could access the pgt after it has
   been freed?

2. In general, is it safe for kvm_pgtable_stage2_free_removed() to run
   outside of the MMU lock? Yes the page tables have already been
   disconnected from the tree, but kvm_pgtable_stage2_free_removed()
   also accesses shared data structures likstruct kvm_pgtable. I *think*
   it might be safe after you fix (1.) but it would be more robust to
   avoid accessing shared data structures at all outside of the MMU lock
   and just do the page table freeing in the RCU callback.

<span class=q>&gt; 
&gt; &gt;  	}
&gt; &gt;  }
&gt; &gt;  
&gt; &gt; -- 
&gt; &gt; 2.37.2.672.g94769d06f0-goog
&gt; &gt; 
</span>
_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#mdde2e85c111e7bf38027f66ddceab79cb9619dc0 id=edde2e85c111e7bf38027f66ddceab79cb9619dc0>^</a> <a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/>permalink</a> <a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/t/#u><b>nested</b></a>] <a href=#rdde2e85c111e7bf38027f66ddceab79cb9619dc0>96+ messages in thread</a></pre><li><pre><a href=#edde2e85c111e7bf38027f66ddceab79cb9619dc0 id=mdde2e85c111e7bf38027f66ddceab79cb9619dc0>*</a> <b>Re: [PATCH 09/14] KVM: arm64: Free removed stage-2 tables in RCU callback</b>
<b>@ 2022-09-08 16:40       ` David Matlack</b>
  <a href=#rdde2e85c111e7bf38027f66ddceab79cb9619dc0>0 siblings, 0 replies; 96+ messages in thread</a>
From: David Matlack @ 2022-09-08 16:40 UTC (<a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: kvm, Marc Zyngier, linux-kernel, Catalin Marinas, Ben Gardon,
	Paolo Bonzini, Will Deacon, kvmarm, linux-arm-kernel

On Wed, Sep 07, 2022 at 03:00:18PM -0700, David Matlack wrote:
<span class=q>&gt; On Tue, Aug 30, 2022 at 07:41:27PM +0000, Oliver Upton wrote:
&gt; &gt; There is no real urgency to free a stage-2 subtree that was pruned.
&gt; &gt; Nonetheless, KVM does the tear down in the stage-2 fault path while
&gt; &gt; holding the MMU lock.
&gt; &gt; 
&gt; &gt; Free removed stage-2 subtrees after an RCU grace period. To guarantee
&gt; &gt; all stage-2 table pages are freed before killing a VM, add an
&gt; &gt; rcu_barrier() to the flush path.
&gt; &gt; 
&gt; &gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; &gt; ---
&gt; &gt;  arch/arm64/kvm/mmu.c | 35 ++++++++++++++++++++++++++++++++++-
&gt; &gt;  1 file changed, 34 insertions(+), 1 deletion(-)
&gt; &gt; 
&gt; &gt; diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
&gt; &gt; index 91521f4aab97..265951c05879 100644
&gt; &gt; --- a/arch/arm64/kvm/mmu.c
&gt; &gt; +++ b/arch/arm64/kvm/mmu.c
&gt; &gt; @@ -97,6 +97,38 @@ static void *stage2_memcache_zalloc_page(void *arg)
&gt; &gt;  	return kvm_mmu_memory_cache_alloc(mc);
&gt; &gt;  }
&gt; &gt;  
&gt; &gt; +#define STAGE2_PAGE_PRIVATE_LEVEL_MASK	GENMASK_ULL(2, 0)
&gt; &gt; +
&gt; &gt; +static inline unsigned long stage2_page_private(u32 level, void *arg)
&gt; &gt; +{
&gt; &gt; +	unsigned long pvt = (unsigned long)arg;
&gt; &gt; +
&gt; &gt; +	BUILD_BUG_ON(KVM_PGTABLE_MAX_LEVELS &gt; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; &gt; +	WARN_ON_ONCE(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; &gt; +
&gt; &gt; +	return pvt | level;
&gt; &gt; +}
&gt; &gt; +
&gt; &gt; +static void stage2_free_removed_table_rcu_cb(struct rcu_head *head)
&gt; &gt; +{
&gt; &gt; +	struct page *page = container_of(head, struct page, rcu_head);
&gt; &gt; +	unsigned long pvt = page_private(page);
&gt; &gt; +	void *arg = (void *)(pvt &amp; ~STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; &gt; +	u32 level = (u32)(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; &gt; +	void *pgtable = page_to_virt(page);
&gt; &gt; +
&gt; &gt; +	kvm_pgtable_stage2_free_removed(pgtable, level, arg);
&gt; &gt; +}
&gt; &gt; +
&gt; &gt; +static void stage2_free_removed_table(void *pgtable, u32 level, void *arg)
&gt; &gt; +{
&gt; &gt; +	unsigned long pvt = stage2_page_private(level, arg);
&gt; &gt; +	struct page *page = virt_to_page(pgtable);
&gt; &gt; +
&gt; &gt; +	set_page_private(page, (unsigned long)pvt);
&gt; &gt; +	call_rcu(&amp;page-&gt;rcu_head, stage2_free_removed_table_rcu_cb);
&gt; &gt; +}
&gt; &gt; +
&gt; &gt;  static void *kvm_host_zalloc_pages_exact(size_t size)
&gt; &gt;  {
&gt; &gt;  	return alloc_pages_exact(size, GFP_KERNEL_ACCOUNT | __GFP_ZERO);
&gt; &gt; @@ -627,7 +659,7 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
&gt; &gt;  	.zalloc_page		= stage2_memcache_zalloc_page,
&gt; &gt;  	.zalloc_pages_exact	= kvm_host_zalloc_pages_exact,
&gt; &gt;  	.free_pages_exact	= free_pages_exact,
&gt; &gt; -	.free_removed_table	= kvm_pgtable_stage2_free_removed,
&gt; &gt; +	.free_removed_table	= stage2_free_removed_table,
&gt; &gt;  	.get_page		= kvm_host_get_page,
&gt; &gt;  	.put_page		= kvm_host_put_page,
&gt; &gt;  	.page_count		= kvm_host_page_count,
&gt; &gt; @@ -770,6 +802,7 @@ void kvm_free_stage2_pgd(struct kvm_s2_mmu *mmu)
&gt; &gt;  	if (pgt) {
&gt; &gt;  		kvm_pgtable_stage2_destroy(pgt);
&gt; &gt;  		kfree(pgt);
&gt; &gt; +		rcu_barrier();
&gt; 
&gt; A comment here would be useful to document the behavior. e.g.
&gt; 
&gt;         /*
&gt;          * Wait for all stage-2 page tables that are being freed
&gt;          * asynchronously via RCU callback because ...
&gt;          */
&gt; 
&gt; Speaking of, what's the reason for this rcu_barrier()? Is there any
&gt; reason why KVM can't let in-flight stage-2 freeing RCU callbacks run at
&gt; the end of the next grace period?
</span>
After thinking about this more I have 2 follow-up questions:

1. Should the RCU barrier come before kvm_pgtable_stage2_destroy() and
   kfree(pgt)? Otherwise an RCU callback running
   kvm_pgtable_stage2_free_removed() could access the pgt after it has
   been freed?

2. In general, is it safe for kvm_pgtable_stage2_free_removed() to run
   outside of the MMU lock? Yes the page tables have already been
   disconnected from the tree, but kvm_pgtable_stage2_free_removed()
   also accesses shared data structures likstruct kvm_pgtable. I *think*
   it might be safe after you fix (1.) but it would be more robust to
   avoid accessing shared data structures at all outside of the MMU lock
   and just do the page table freeing in the RCU callback.

<span class=q>&gt; 
&gt; &gt;  	}
&gt; &gt;  }
&gt; &gt;  
&gt; &gt; -- 
&gt; &gt; 2.37.2.672.g94769d06f0-goog
&gt; &gt; 
</span>_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#mdde2e85c111e7bf38027f66ddceab79cb9619dc0 id=edde2e85c111e7bf38027f66ddceab79cb9619dc0>^</a> <a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/>permalink</a> <a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/Yxoa8AQozMWuayTD@google.com/t/#u><b>nested</b></a>] <a href=#rdde2e85c111e7bf38027f66ddceab79cb9619dc0>96+ messages in thread</a></pre></ul></ul><li><pre><a href=#e0151ebce75fd307e534ce3cf4350fe1455739219 id=m0151ebce75fd307e534ce3cf4350fe1455739219>*</a> <b>Re: [PATCH 09/14] KVM: arm64: Free removed stage-2 tables in RCU callback</b>
  2022-08-30 19:41   ` <a href=#m10409b482b1166e85de5a4cc2b432e6de2e770c8>Oliver Upton</a>
  (?)
<b>@ 2022-09-14  0:49     ` Ricardo Koller</b>
  <a href=#r0151ebce75fd307e534ce3cf4350fe1455739219>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Ricardo Koller @ 2022-09-14  0:49 UTC (<a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Reiji Watanabe,
	David Matlack, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

Hi Oliver,

On Tue, Aug 30, 2022 at 07:41:27PM +0000, Oliver Upton wrote:
<span class=q>&gt; There is no real urgency to free a stage-2 subtree that was pruned.
&gt; Nonetheless, KVM does the tear down in the stage-2 fault path while
&gt; holding the MMU lock.
&gt; 
&gt; Free removed stage-2 subtrees after an RCU grace period. To guarantee
&gt; all stage-2 table pages are freed before killing a VM, add an
&gt; rcu_barrier() to the flush path.
&gt; 
&gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; ---
&gt;  arch/arm64/kvm/mmu.c | 35 ++++++++++++++++++++++++++++++++++-
&gt;  1 file changed, 34 insertions(+), 1 deletion(-)
&gt; 
&gt; diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
&gt; index 91521f4aab97..265951c05879 100644
&gt; --- a/arch/arm64/kvm/mmu.c
&gt; +++ b/arch/arm64/kvm/mmu.c
&gt; @@ -97,6 +97,38 @@ static void *stage2_memcache_zalloc_page(void *arg)
&gt;  	return kvm_mmu_memory_cache_alloc(mc);
&gt;  }
&gt;  
&gt; +#define STAGE2_PAGE_PRIVATE_LEVEL_MASK	GENMASK_ULL(2, 0)
&gt; +
&gt; +static inline unsigned long stage2_page_private(u32 level, void *arg)
&gt; +{
&gt; +	unsigned long pvt = (unsigned long)arg;
&gt; +
&gt; +	BUILD_BUG_ON(KVM_PGTABLE_MAX_LEVELS &gt; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +	WARN_ON_ONCE(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
</span>
If the pgt pointer (arg) is not aligned for some reason, I think it
might be better to BUG_ON(). Alternatively, why not trying to pass a new
struct (with level and arg) that's freed by the rcu callback.

<span class=q>&gt; +
&gt; +	return pvt | level;
&gt; +}
&gt; +
&gt; +static void stage2_free_removed_table_rcu_cb(struct rcu_head *head)
&gt; +{
&gt; +	struct page *page = container_of(head, struct page, rcu_head);
&gt; +	unsigned long pvt = page_private(page);
&gt; +	void *arg = (void *)(pvt &amp; ~STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +	u32 level = (u32)(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +	void *pgtable = page_to_virt(page);
&gt; +
&gt; +	kvm_pgtable_stage2_free_removed(pgtable, level, arg);
&gt; +}
&gt; +
&gt; +static void stage2_free_removed_table(void *pgtable, u32 level, void *arg)
&gt; +{
&gt; +	unsigned long pvt = stage2_page_private(level, arg);
&gt; +	struct page *page = virt_to_page(pgtable);
&gt; +
&gt; +	set_page_private(page, (unsigned long)pvt);
&gt; +	call_rcu(&amp;page-&gt;rcu_head, stage2_free_removed_table_rcu_cb);
&gt; +}
&gt; +
&gt;  static void *kvm_host_zalloc_pages_exact(size_t size)
&gt;  {
&gt;  	return alloc_pages_exact(size, GFP_KERNEL_ACCOUNT | __GFP_ZERO);
&gt; @@ -627,7 +659,7 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
&gt;  	.zalloc_page		= stage2_memcache_zalloc_page,
&gt;  	.zalloc_pages_exact	= kvm_host_zalloc_pages_exact,
&gt;  	.free_pages_exact	= free_pages_exact,
&gt; -	.free_removed_table	= kvm_pgtable_stage2_free_removed,
&gt; +	.free_removed_table	= stage2_free_removed_table,
&gt;  	.get_page		= kvm_host_get_page,
&gt;  	.put_page		= kvm_host_put_page,
&gt;  	.page_count		= kvm_host_page_count,
&gt; @@ -770,6 +802,7 @@ void kvm_free_stage2_pgd(struct kvm_s2_mmu *mmu)
&gt;  	if (pgt) {
&gt;  		kvm_pgtable_stage2_destroy(pgt);
&gt;  		kfree(pgt);
&gt; +		rcu_barrier();
&gt;  	}
&gt;  }
&gt;  
&gt; -- 
&gt; 2.37.2.672.g94769d06f0-goog
&gt; 
</span>
<a href=#m0151ebce75fd307e534ce3cf4350fe1455739219 id=e0151ebce75fd307e534ce3cf4350fe1455739219>^</a> <a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/t/#u><b>nested</b></a>] <a href=#r0151ebce75fd307e534ce3cf4350fe1455739219>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e0151ebce75fd307e534ce3cf4350fe1455739219 id=m0151ebce75fd307e534ce3cf4350fe1455739219>*</a> <b>Re: [PATCH 09/14] KVM: arm64: Free removed stage-2 tables in RCU callback</b>
<b>@ 2022-09-14  0:49     ` Ricardo Koller</b>
  <a href=#r0151ebce75fd307e534ce3cf4350fe1455739219>0 siblings, 0 replies; 96+ messages in thread</a>
From: Ricardo Koller @ 2022-09-14  0:49 UTC (<a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, Quentin Perret, Reiji Watanabe,
	David Matlack, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm, linux-kernel

Hi Oliver,

On Tue, Aug 30, 2022 at 07:41:27PM +0000, Oliver Upton wrote:
<span class=q>&gt; There is no real urgency to free a stage-2 subtree that was pruned.
&gt; Nonetheless, KVM does the tear down in the stage-2 fault path while
&gt; holding the MMU lock.
&gt; 
&gt; Free removed stage-2 subtrees after an RCU grace period. To guarantee
&gt; all stage-2 table pages are freed before killing a VM, add an
&gt; rcu_barrier() to the flush path.
&gt; 
&gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; ---
&gt;  arch/arm64/kvm/mmu.c | 35 ++++++++++++++++++++++++++++++++++-
&gt;  1 file changed, 34 insertions(+), 1 deletion(-)
&gt; 
&gt; diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
&gt; index 91521f4aab97..265951c05879 100644
&gt; --- a/arch/arm64/kvm/mmu.c
&gt; +++ b/arch/arm64/kvm/mmu.c
&gt; @@ -97,6 +97,38 @@ static void *stage2_memcache_zalloc_page(void *arg)
&gt;  	return kvm_mmu_memory_cache_alloc(mc);
&gt;  }
&gt;  
&gt; +#define STAGE2_PAGE_PRIVATE_LEVEL_MASK	GENMASK_ULL(2, 0)
&gt; +
&gt; +static inline unsigned long stage2_page_private(u32 level, void *arg)
&gt; +{
&gt; +	unsigned long pvt = (unsigned long)arg;
&gt; +
&gt; +	BUILD_BUG_ON(KVM_PGTABLE_MAX_LEVELS &gt; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +	WARN_ON_ONCE(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
</span>
If the pgt pointer (arg) is not aligned for some reason, I think it
might be better to BUG_ON(). Alternatively, why not trying to pass a new
struct (with level and arg) that's freed by the rcu callback.

<span class=q>&gt; +
&gt; +	return pvt | level;
&gt; +}
&gt; +
&gt; +static void stage2_free_removed_table_rcu_cb(struct rcu_head *head)
&gt; +{
&gt; +	struct page *page = container_of(head, struct page, rcu_head);
&gt; +	unsigned long pvt = page_private(page);
&gt; +	void *arg = (void *)(pvt &amp; ~STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +	u32 level = (u32)(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +	void *pgtable = page_to_virt(page);
&gt; +
&gt; +	kvm_pgtable_stage2_free_removed(pgtable, level, arg);
&gt; +}
&gt; +
&gt; +static void stage2_free_removed_table(void *pgtable, u32 level, void *arg)
&gt; +{
&gt; +	unsigned long pvt = stage2_page_private(level, arg);
&gt; +	struct page *page = virt_to_page(pgtable);
&gt; +
&gt; +	set_page_private(page, (unsigned long)pvt);
&gt; +	call_rcu(&amp;page-&gt;rcu_head, stage2_free_removed_table_rcu_cb);
&gt; +}
&gt; +
&gt;  static void *kvm_host_zalloc_pages_exact(size_t size)
&gt;  {
&gt;  	return alloc_pages_exact(size, GFP_KERNEL_ACCOUNT | __GFP_ZERO);
&gt; @@ -627,7 +659,7 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
&gt;  	.zalloc_page		= stage2_memcache_zalloc_page,
&gt;  	.zalloc_pages_exact	= kvm_host_zalloc_pages_exact,
&gt;  	.free_pages_exact	= free_pages_exact,
&gt; -	.free_removed_table	= kvm_pgtable_stage2_free_removed,
&gt; +	.free_removed_table	= stage2_free_removed_table,
&gt;  	.get_page		= kvm_host_get_page,
&gt;  	.put_page		= kvm_host_put_page,
&gt;  	.page_count		= kvm_host_page_count,
&gt; @@ -770,6 +802,7 @@ void kvm_free_stage2_pgd(struct kvm_s2_mmu *mmu)
&gt;  	if (pgt) {
&gt;  		kvm_pgtable_stage2_destroy(pgt);
&gt;  		kfree(pgt);
&gt; +		rcu_barrier();
&gt;  	}
&gt;  }
&gt;  
&gt; -- 
&gt; 2.37.2.672.g94769d06f0-goog
&gt; 
</span>
_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m0151ebce75fd307e534ce3cf4350fe1455739219 id=e0151ebce75fd307e534ce3cf4350fe1455739219>^</a> <a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/t/#u><b>nested</b></a>] <a href=#r0151ebce75fd307e534ce3cf4350fe1455739219>96+ messages in thread</a></pre><li><pre><a href=#e0151ebce75fd307e534ce3cf4350fe1455739219 id=m0151ebce75fd307e534ce3cf4350fe1455739219>*</a> <b>Re: [PATCH 09/14] KVM: arm64: Free removed stage-2 tables in RCU callback</b>
<b>@ 2022-09-14  0:49     ` Ricardo Koller</b>
  <a href=#r0151ebce75fd307e534ce3cf4350fe1455739219>0 siblings, 0 replies; 96+ messages in thread</a>
From: Ricardo Koller @ 2022-09-14  0:49 UTC (<a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: kvm, Marc Zyngier, linux-kernel, Ben Gardon, Catalin Marinas,
	David Matlack, Paolo Bonzini, Will Deacon, kvmarm,
	linux-arm-kernel

Hi Oliver,

On Tue, Aug 30, 2022 at 07:41:27PM +0000, Oliver Upton wrote:
<span class=q>&gt; There is no real urgency to free a stage-2 subtree that was pruned.
&gt; Nonetheless, KVM does the tear down in the stage-2 fault path while
&gt; holding the MMU lock.
&gt; 
&gt; Free removed stage-2 subtrees after an RCU grace period. To guarantee
&gt; all stage-2 table pages are freed before killing a VM, add an
&gt; rcu_barrier() to the flush path.
&gt; 
&gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; ---
&gt;  arch/arm64/kvm/mmu.c | 35 ++++++++++++++++++++++++++++++++++-
&gt;  1 file changed, 34 insertions(+), 1 deletion(-)
&gt; 
&gt; diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
&gt; index 91521f4aab97..265951c05879 100644
&gt; --- a/arch/arm64/kvm/mmu.c
&gt; +++ b/arch/arm64/kvm/mmu.c
&gt; @@ -97,6 +97,38 @@ static void *stage2_memcache_zalloc_page(void *arg)
&gt;  	return kvm_mmu_memory_cache_alloc(mc);
&gt;  }
&gt;  
&gt; +#define STAGE2_PAGE_PRIVATE_LEVEL_MASK	GENMASK_ULL(2, 0)
&gt; +
&gt; +static inline unsigned long stage2_page_private(u32 level, void *arg)
&gt; +{
&gt; +	unsigned long pvt = (unsigned long)arg;
&gt; +
&gt; +	BUILD_BUG_ON(KVM_PGTABLE_MAX_LEVELS &gt; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +	WARN_ON_ONCE(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
</span>
If the pgt pointer (arg) is not aligned for some reason, I think it
might be better to BUG_ON(). Alternatively, why not trying to pass a new
struct (with level and arg) that's freed by the rcu callback.

<span class=q>&gt; +
&gt; +	return pvt | level;
&gt; +}
&gt; +
&gt; +static void stage2_free_removed_table_rcu_cb(struct rcu_head *head)
&gt; +{
&gt; +	struct page *page = container_of(head, struct page, rcu_head);
&gt; +	unsigned long pvt = page_private(page);
&gt; +	void *arg = (void *)(pvt &amp; ~STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +	u32 level = (u32)(pvt &amp; STAGE2_PAGE_PRIVATE_LEVEL_MASK);
&gt; +	void *pgtable = page_to_virt(page);
&gt; +
&gt; +	kvm_pgtable_stage2_free_removed(pgtable, level, arg);
&gt; +}
&gt; +
&gt; +static void stage2_free_removed_table(void *pgtable, u32 level, void *arg)
&gt; +{
&gt; +	unsigned long pvt = stage2_page_private(level, arg);
&gt; +	struct page *page = virt_to_page(pgtable);
&gt; +
&gt; +	set_page_private(page, (unsigned long)pvt);
&gt; +	call_rcu(&amp;page-&gt;rcu_head, stage2_free_removed_table_rcu_cb);
&gt; +}
&gt; +
&gt;  static void *kvm_host_zalloc_pages_exact(size_t size)
&gt;  {
&gt;  	return alloc_pages_exact(size, GFP_KERNEL_ACCOUNT | __GFP_ZERO);
&gt; @@ -627,7 +659,7 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
&gt;  	.zalloc_page		= stage2_memcache_zalloc_page,
&gt;  	.zalloc_pages_exact	= kvm_host_zalloc_pages_exact,
&gt;  	.free_pages_exact	= free_pages_exact,
&gt; -	.free_removed_table	= kvm_pgtable_stage2_free_removed,
&gt; +	.free_removed_table	= stage2_free_removed_table,
&gt;  	.get_page		= kvm_host_get_page,
&gt;  	.put_page		= kvm_host_put_page,
&gt;  	.page_count		= kvm_host_page_count,
&gt; @@ -770,6 +802,7 @@ void kvm_free_stage2_pgd(struct kvm_s2_mmu *mmu)
&gt;  	if (pgt) {
&gt;  		kvm_pgtable_stage2_destroy(pgt);
&gt;  		kfree(pgt);
&gt; +		rcu_barrier();
&gt;  	}
&gt;  }
&gt;  
&gt; -- 
&gt; 2.37.2.672.g94769d06f0-goog
&gt; 
</span>_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m0151ebce75fd307e534ce3cf4350fe1455739219 id=e0151ebce75fd307e534ce3cf4350fe1455739219>^</a> <a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YyElF+MMXSumzszf@google.com/t/#u><b>nested</b></a>] <a href=#r0151ebce75fd307e534ce3cf4350fe1455739219>96+ messages in thread</a></pre></ul></ul><li><pre><a href=#edeed2e1f6386a9a9fb93754ec5655a1618c425e1 id=mdeed2e1f6386a9a9fb93754ec5655a1618c425e1>*</a> <u id=u><b>[PATCH 10/14] KVM: arm64: Atomically update stage 2 leaf attributes in parallel walks</b></u>
  2022-08-30 19:41 ` <a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb>Oliver Upton</a>
  (?)
<b>@ 2022-08-30 19:50   ` Oliver Upton</b>
  <a href=#rdeed2e1f6386a9a9fb93754ec5655a1618c425e1>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:50 UTC (<a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Oliver Upton, Catalin Marinas, Will Deacon
  Cc: linux-arm-kernel, kvmarm, kvm, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, linux-kernel

The stage2 attr walker is already used for parallel walks. Since commit
f783ef1c0e82 ("KVM: arm64: Add fast path to handle permission relaxation
during dirty logging"), KVM acquires the read lock when
write-unprotecting a PTE. However, the walker only uses a simple store
to update the PTE. This is safe as the only possible race is with
hardware updates to the access flag, which is benign.

However, a subsequent change to KVM will allow more changes to the stage
2 page tables to be done in parallel. Prepare the stage 2 attribute
walker by performing atomic updates to the PTE when walking in parallel.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830195036.964607-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830195036.964607-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 28 +++++++++++++++++++++-------
 1 file <a href=#edeed2e1f6386a9a9fb93754ec5655a1618c425e1>changed</a>, 21 insertions(+), 7 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830195036.964607-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830195036.964607-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 215a14c434ed..61a4437c8c16 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -691,6 +691,16 @@ static bool stage2_pte_is_counted(kvm_pte_t pte)
</span> 	return kvm_pte_valid(pte) || kvm_invalid_pte_owner(pte);
 }
 
<span class=add>+static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bool shared)
+{
+	if (!shared) {
+		WRITE_ONCE(*ptep, new);
+		return true;
+	}
+
+	return cmpxchg(ptep, old, new) == old;
+}
+
</span> static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_s2_mmu *mmu, u64 addr,
 			   u32 level, struct kvm_pgtable_mm_ops *mm_ops)
 {
<span class=hunk>@@ -985,6 +995,7 @@ struct stage2_attr_data {
</span> 	kvm_pte_t			pte;
 	u32				level;
 	struct kvm_pgtable_mm_ops	*mm_ops;
<span class=add>+	bool				shared;
</span> };
 
 static int stage2_attr_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=hunk>@@ -1017,7 +1028,9 @@ static int stage2_attr_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 		    stage2_pte_executable(pte) &amp;&amp; !stage2_pte_executable(data-&gt;pte))
 			mm_ops-&gt;icache_inval_pou(kvm_pte_follow(pte, mm_ops),
 						  kvm_granule_size(level));
<span class=del>-		WRITE_ONCE(*ptep, pte);
</span><span class=add>+
+		if (!stage2_try_set_pte(ptep, data-&gt;pte, pte, data-&gt;shared))
+			return -EAGAIN;
</span> 	}
 
 	return 0;
<span class=hunk>@@ -1026,7 +1039,7 @@ static int stage2_attr_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> static int stage2_update_leaf_attrs(struct kvm_pgtable *pgt, u64 addr,
 				    u64 size, kvm_pte_t attr_set,
 				    kvm_pte_t attr_clr, kvm_pte_t *orig_pte,
<span class=del>-				    u32 *level)
</span><span class=add>+				    u32 *level, bool shared)
</span> {
 	int ret;
 	kvm_pte_t attr_mask = KVM_PTE_LEAF_ATTR_LO | KVM_PTE_LEAF_ATTR_HI;
<span class=hunk>@@ -1034,6 +1047,7 @@ static int stage2_update_leaf_attrs(struct kvm_pgtable *pgt, u64 addr,
</span> 		.attr_set	= attr_set &amp; attr_mask,
 		.attr_clr	= attr_clr &amp; attr_mask,
 		.mm_ops		= pgt-&gt;mm_ops,
<span class=add>+		.shared		= shared,
</span> 	};
 	struct kvm_pgtable_walker walker = {
 		.cb		= stage2_attr_walker,
<span class=hunk>@@ -1057,14 +1071,14 @@ int kvm_pgtable_stage2_wrprotect(struct kvm_pgtable *pgt, u64 addr, u64 size)
</span> {
 	return stage2_update_leaf_attrs(pgt, addr, size, 0,
 					KVM_PTE_LEAF_ATTR_LO_S2_S2AP_W,
<span class=del>-					NULL, NULL);
</span><span class=add>+					NULL, NULL, false);
</span> }
 
 kvm_pte_t kvm_pgtable_stage2_mkyoung(struct kvm_pgtable *pgt, u64 addr)
 {
 	kvm_pte_t pte = 0;
 	stage2_update_leaf_attrs(pgt, addr, 1, KVM_PTE_LEAF_ATTR_LO_S2_AF, 0,
<span class=del>-				 &amp;pte, NULL);
</span><span class=add>+				 &amp;pte, NULL, false);
</span> 	dsb(ishst);
 	return pte;
 }
<span class=hunk>@@ -1073,7 +1087,7 @@ kvm_pte_t kvm_pgtable_stage2_mkold(struct kvm_pgtable *pgt, u64 addr)
</span> {
 	kvm_pte_t pte = 0;
 	stage2_update_leaf_attrs(pgt, addr, 1, 0, KVM_PTE_LEAF_ATTR_LO_S2_AF,
<span class=del>-				 &amp;pte, NULL);
</span><span class=add>+				 &amp;pte, NULL, false);
</span> 	/*
 	 * "But where's the TLBI?!", you scream.
 	 * "Over in the core code", I sigh.
<span class=hunk>@@ -1086,7 +1100,7 @@ kvm_pte_t kvm_pgtable_stage2_mkold(struct kvm_pgtable *pgt, u64 addr)
</span> bool kvm_pgtable_stage2_is_young(struct kvm_pgtable *pgt, u64 addr)
 {
 	kvm_pte_t pte = 0;
<span class=del>-	stage2_update_leaf_attrs(pgt, addr, 1, 0, 0, &amp;pte, NULL);
</span><span class=add>+	stage2_update_leaf_attrs(pgt, addr, 1, 0, 0, &amp;pte, NULL, false);
</span> 	return pte &amp; KVM_PTE_LEAF_ATTR_LO_S2_AF;
 }
 
<span class=hunk>@@ -1109,7 +1123,7 @@ int kvm_pgtable_stage2_relax_perms(struct kvm_pgtable *pgt, u64 addr,
</span> 	if (prot &amp; KVM_PGTABLE_PROT_X)
 		clr |= KVM_PTE_LEAF_ATTR_HI_S2_XN;
 
<span class=del>-	ret = stage2_update_leaf_attrs(pgt, addr, 1, set, clr, NULL, &amp;level);
</span><span class=add>+	ret = stage2_update_leaf_attrs(pgt, addr, 1, set, clr, NULL, &amp;level, true);
</span> 	if (!ret)
 		kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, pgt-&gt;mmu, addr, level);
 	return ret;
-- 
2.37.2.672.g94769d06f0-goog


<a href=#mdeed2e1f6386a9a9fb93754ec5655a1618c425e1 id=edeed2e1f6386a9a9fb93754ec5655a1618c425e1>^</a> <a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/T/#u>flat</a>|<a href=#u><b>nested</b></a>] <a href=#rdeed2e1f6386a9a9fb93754ec5655a1618c425e1>96+ messages in thread</a></pre><li><ul><li><pre><a href=#edeed2e1f6386a9a9fb93754ec5655a1618c425e1 id=mdeed2e1f6386a9a9fb93754ec5655a1618c425e1>*</a> <u id=u><b>[PATCH 10/14] KVM: arm64: Atomically update stage 2 leaf attributes in parallel walks</b></u>
<b>@ 2022-08-30 19:50   ` Oliver Upton</b>
  <a href=#rdeed2e1f6386a9a9fb93754ec5655a1618c425e1>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:50 UTC (<a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Oliver Upton, Catalin Marinas, Will Deacon
  Cc: linux-arm-kernel, kvmarm, kvm, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, linux-kernel

The stage2 attr walker is already used for parallel walks. Since commit
f783ef1c0e82 ("KVM: arm64: Add fast path to handle permission relaxation
during dirty logging"), KVM acquires the read lock when
write-unprotecting a PTE. However, the walker only uses a simple store
to update the PTE. This is safe as the only possible race is with
hardware updates to the access flag, which is benign.

However, a subsequent change to KVM will allow more changes to the stage
2 page tables to be done in parallel. Prepare the stage 2 attribute
walker by performing atomic updates to the PTE when walking in parallel.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830195036.964607-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830195036.964607-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 28 +++++++++++++++++++++-------
 1 file <a href=#edeed2e1f6386a9a9fb93754ec5655a1618c425e1>changed</a>, 21 insertions(+), 7 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830195036.964607-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830195036.964607-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 215a14c434ed..61a4437c8c16 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -691,6 +691,16 @@ static bool stage2_pte_is_counted(kvm_pte_t pte)
</span> 	return kvm_pte_valid(pte) || kvm_invalid_pte_owner(pte);
 }
 
<span class=add>+static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bool shared)
+{
+	if (!shared) {
+		WRITE_ONCE(*ptep, new);
+		return true;
+	}
+
+	return cmpxchg(ptep, old, new) == old;
+}
+
</span> static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_s2_mmu *mmu, u64 addr,
 			   u32 level, struct kvm_pgtable_mm_ops *mm_ops)
 {
<span class=hunk>@@ -985,6 +995,7 @@ struct stage2_attr_data {
</span> 	kvm_pte_t			pte;
 	u32				level;
 	struct kvm_pgtable_mm_ops	*mm_ops;
<span class=add>+	bool				shared;
</span> };
 
 static int stage2_attr_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=hunk>@@ -1017,7 +1028,9 @@ static int stage2_attr_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 		    stage2_pte_executable(pte) &amp;&amp; !stage2_pte_executable(data-&gt;pte))
 			mm_ops-&gt;icache_inval_pou(kvm_pte_follow(pte, mm_ops),
 						  kvm_granule_size(level));
<span class=del>-		WRITE_ONCE(*ptep, pte);
</span><span class=add>+
+		if (!stage2_try_set_pte(ptep, data-&gt;pte, pte, data-&gt;shared))
+			return -EAGAIN;
</span> 	}
 
 	return 0;
<span class=hunk>@@ -1026,7 +1039,7 @@ static int stage2_attr_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> static int stage2_update_leaf_attrs(struct kvm_pgtable *pgt, u64 addr,
 				    u64 size, kvm_pte_t attr_set,
 				    kvm_pte_t attr_clr, kvm_pte_t *orig_pte,
<span class=del>-				    u32 *level)
</span><span class=add>+				    u32 *level, bool shared)
</span> {
 	int ret;
 	kvm_pte_t attr_mask = KVM_PTE_LEAF_ATTR_LO | KVM_PTE_LEAF_ATTR_HI;
<span class=hunk>@@ -1034,6 +1047,7 @@ static int stage2_update_leaf_attrs(struct kvm_pgtable *pgt, u64 addr,
</span> 		.attr_set	= attr_set &amp; attr_mask,
 		.attr_clr	= attr_clr &amp; attr_mask,
 		.mm_ops		= pgt-&gt;mm_ops,
<span class=add>+		.shared		= shared,
</span> 	};
 	struct kvm_pgtable_walker walker = {
 		.cb		= stage2_attr_walker,
<span class=hunk>@@ -1057,14 +1071,14 @@ int kvm_pgtable_stage2_wrprotect(struct kvm_pgtable *pgt, u64 addr, u64 size)
</span> {
 	return stage2_update_leaf_attrs(pgt, addr, size, 0,
 					KVM_PTE_LEAF_ATTR_LO_S2_S2AP_W,
<span class=del>-					NULL, NULL);
</span><span class=add>+					NULL, NULL, false);
</span> }
 
 kvm_pte_t kvm_pgtable_stage2_mkyoung(struct kvm_pgtable *pgt, u64 addr)
 {
 	kvm_pte_t pte = 0;
 	stage2_update_leaf_attrs(pgt, addr, 1, KVM_PTE_LEAF_ATTR_LO_S2_AF, 0,
<span class=del>-				 &amp;pte, NULL);
</span><span class=add>+				 &amp;pte, NULL, false);
</span> 	dsb(ishst);
 	return pte;
 }
<span class=hunk>@@ -1073,7 +1087,7 @@ kvm_pte_t kvm_pgtable_stage2_mkold(struct kvm_pgtable *pgt, u64 addr)
</span> {
 	kvm_pte_t pte = 0;
 	stage2_update_leaf_attrs(pgt, addr, 1, 0, KVM_PTE_LEAF_ATTR_LO_S2_AF,
<span class=del>-				 &amp;pte, NULL);
</span><span class=add>+				 &amp;pte, NULL, false);
</span> 	/*
 	 * "But where's the TLBI?!", you scream.
 	 * "Over in the core code", I sigh.
<span class=hunk>@@ -1086,7 +1100,7 @@ kvm_pte_t kvm_pgtable_stage2_mkold(struct kvm_pgtable *pgt, u64 addr)
</span> bool kvm_pgtable_stage2_is_young(struct kvm_pgtable *pgt, u64 addr)
 {
 	kvm_pte_t pte = 0;
<span class=del>-	stage2_update_leaf_attrs(pgt, addr, 1, 0, 0, &amp;pte, NULL);
</span><span class=add>+	stage2_update_leaf_attrs(pgt, addr, 1, 0, 0, &amp;pte, NULL, false);
</span> 	return pte &amp; KVM_PTE_LEAF_ATTR_LO_S2_AF;
 }
 
<span class=hunk>@@ -1109,7 +1123,7 @@ int kvm_pgtable_stage2_relax_perms(struct kvm_pgtable *pgt, u64 addr,
</span> 	if (prot &amp; KVM_PGTABLE_PROT_X)
 		clr |= KVM_PTE_LEAF_ATTR_HI_S2_XN;
 
<span class=del>-	ret = stage2_update_leaf_attrs(pgt, addr, 1, set, clr, NULL, &amp;level);
</span><span class=add>+	ret = stage2_update_leaf_attrs(pgt, addr, 1, set, clr, NULL, &amp;level, true);
</span> 	if (!ret)
 		kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, pgt-&gt;mmu, addr, level);
 	return ret;
-- 
2.37.2.672.g94769d06f0-goog


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#mdeed2e1f6386a9a9fb93754ec5655a1618c425e1 id=edeed2e1f6386a9a9fb93754ec5655a1618c425e1>^</a> <a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/T/#u>flat</a>|<a href=#u><b>nested</b></a>] <a href=#rdeed2e1f6386a9a9fb93754ec5655a1618c425e1>96+ messages in thread</a></pre><li><pre><a href=#edeed2e1f6386a9a9fb93754ec5655a1618c425e1 id=mdeed2e1f6386a9a9fb93754ec5655a1618c425e1>*</a> <u id=u><b>[PATCH 10/14] KVM: arm64: Atomically update stage 2 leaf attributes in parallel walks</b></u>
<b>@ 2022-08-30 19:50   ` Oliver Upton</b>
  <a href=#rdeed2e1f6386a9a9fb93754ec5655a1618c425e1>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:50 UTC (<a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Oliver Upton, Catalin Marinas, Will Deacon
  Cc: kvm, linux-kernel, Ben Gardon, David Matlack, Paolo Bonzini,
	kvmarm, linux-arm-kernel

The stage2 attr walker is already used for parallel walks. Since commit
f783ef1c0e82 ("KVM: arm64: Add fast path to handle permission relaxation
during dirty logging"), KVM acquires the read lock when
write-unprotecting a PTE. However, the walker only uses a simple store
to update the PTE. This is safe as the only possible race is with
hardware updates to the access flag, which is benign.

However, a subsequent change to KVM will allow more changes to the stage
2 page tables to be done in parallel. Prepare the stage 2 attribute
walker by performing atomic updates to the PTE when walking in parallel.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830195036.964607-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830195036.964607-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 28 +++++++++++++++++++++-------
 1 file <a href=#edeed2e1f6386a9a9fb93754ec5655a1618c425e1>changed</a>, 21 insertions(+), 7 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830195036.964607-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830195036.964607-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 215a14c434ed..61a4437c8c16 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -691,6 +691,16 @@ static bool stage2_pte_is_counted(kvm_pte_t pte)
</span> 	return kvm_pte_valid(pte) || kvm_invalid_pte_owner(pte);
 }
 
<span class=add>+static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bool shared)
+{
+	if (!shared) {
+		WRITE_ONCE(*ptep, new);
+		return true;
+	}
+
+	return cmpxchg(ptep, old, new) == old;
+}
+
</span> static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_s2_mmu *mmu, u64 addr,
 			   u32 level, struct kvm_pgtable_mm_ops *mm_ops)
 {
<span class=hunk>@@ -985,6 +995,7 @@ struct stage2_attr_data {
</span> 	kvm_pte_t			pte;
 	u32				level;
 	struct kvm_pgtable_mm_ops	*mm_ops;
<span class=add>+	bool				shared;
</span> };
 
 static int stage2_attr_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
<span class=hunk>@@ -1017,7 +1028,9 @@ static int stage2_attr_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 		    stage2_pte_executable(pte) &amp;&amp; !stage2_pte_executable(data-&gt;pte))
 			mm_ops-&gt;icache_inval_pou(kvm_pte_follow(pte, mm_ops),
 						  kvm_granule_size(level));
<span class=del>-		WRITE_ONCE(*ptep, pte);
</span><span class=add>+
+		if (!stage2_try_set_pte(ptep, data-&gt;pte, pte, data-&gt;shared))
+			return -EAGAIN;
</span> 	}
 
 	return 0;
<span class=hunk>@@ -1026,7 +1039,7 @@ static int stage2_attr_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> static int stage2_update_leaf_attrs(struct kvm_pgtable *pgt, u64 addr,
 				    u64 size, kvm_pte_t attr_set,
 				    kvm_pte_t attr_clr, kvm_pte_t *orig_pte,
<span class=del>-				    u32 *level)
</span><span class=add>+				    u32 *level, bool shared)
</span> {
 	int ret;
 	kvm_pte_t attr_mask = KVM_PTE_LEAF_ATTR_LO | KVM_PTE_LEAF_ATTR_HI;
<span class=hunk>@@ -1034,6 +1047,7 @@ static int stage2_update_leaf_attrs(struct kvm_pgtable *pgt, u64 addr,
</span> 		.attr_set	= attr_set &amp; attr_mask,
 		.attr_clr	= attr_clr &amp; attr_mask,
 		.mm_ops		= pgt-&gt;mm_ops,
<span class=add>+		.shared		= shared,
</span> 	};
 	struct kvm_pgtable_walker walker = {
 		.cb		= stage2_attr_walker,
<span class=hunk>@@ -1057,14 +1071,14 @@ int kvm_pgtable_stage2_wrprotect(struct kvm_pgtable *pgt, u64 addr, u64 size)
</span> {
 	return stage2_update_leaf_attrs(pgt, addr, size, 0,
 					KVM_PTE_LEAF_ATTR_LO_S2_S2AP_W,
<span class=del>-					NULL, NULL);
</span><span class=add>+					NULL, NULL, false);
</span> }
 
 kvm_pte_t kvm_pgtable_stage2_mkyoung(struct kvm_pgtable *pgt, u64 addr)
 {
 	kvm_pte_t pte = 0;
 	stage2_update_leaf_attrs(pgt, addr, 1, KVM_PTE_LEAF_ATTR_LO_S2_AF, 0,
<span class=del>-				 &amp;pte, NULL);
</span><span class=add>+				 &amp;pte, NULL, false);
</span> 	dsb(ishst);
 	return pte;
 }
<span class=hunk>@@ -1073,7 +1087,7 @@ kvm_pte_t kvm_pgtable_stage2_mkold(struct kvm_pgtable *pgt, u64 addr)
</span> {
 	kvm_pte_t pte = 0;
 	stage2_update_leaf_attrs(pgt, addr, 1, 0, KVM_PTE_LEAF_ATTR_LO_S2_AF,
<span class=del>-				 &amp;pte, NULL);
</span><span class=add>+				 &amp;pte, NULL, false);
</span> 	/*
 	 * "But where's the TLBI?!", you scream.
 	 * "Over in the core code", I sigh.
<span class=hunk>@@ -1086,7 +1100,7 @@ kvm_pte_t kvm_pgtable_stage2_mkold(struct kvm_pgtable *pgt, u64 addr)
</span> bool kvm_pgtable_stage2_is_young(struct kvm_pgtable *pgt, u64 addr)
 {
 	kvm_pte_t pte = 0;
<span class=del>-	stage2_update_leaf_attrs(pgt, addr, 1, 0, 0, &amp;pte, NULL);
</span><span class=add>+	stage2_update_leaf_attrs(pgt, addr, 1, 0, 0, &amp;pte, NULL, false);
</span> 	return pte &amp; KVM_PTE_LEAF_ATTR_LO_S2_AF;
 }
 
<span class=hunk>@@ -1109,7 +1123,7 @@ int kvm_pgtable_stage2_relax_perms(struct kvm_pgtable *pgt, u64 addr,
</span> 	if (prot &amp; KVM_PGTABLE_PROT_X)
 		clr |= KVM_PTE_LEAF_ATTR_HI_S2_XN;
 
<span class=del>-	ret = stage2_update_leaf_attrs(pgt, addr, 1, set, clr, NULL, &amp;level);
</span><span class=add>+	ret = stage2_update_leaf_attrs(pgt, addr, 1, set, clr, NULL, &amp;level, true);
</span> 	if (!ret)
 		kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, pgt-&gt;mmu, addr, level);
 	return ret;
-- 
2.37.2.672.g94769d06f0-goog

_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#mdeed2e1f6386a9a9fb93754ec5655a1618c425e1 id=edeed2e1f6386a9a9fb93754ec5655a1618c425e1>^</a> <a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/T/#u>flat</a>|<a href=#u><b>nested</b></a>] <a href=#rdeed2e1f6386a9a9fb93754ec5655a1618c425e1>96+ messages in thread</a></pre></ul><li><pre><a href=#e12c4d59838e62fa01806ebbbd99b6882ebdbbcc2 id=m12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>*</a> <b>[PATCH 11/14] KVM: arm64: Make changes block-&gt;table to leaf PTEs parallel-aware</b>
  2022-08-30 19:41 ` <a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb>Oliver Upton</a>
  (?)
<b>@ 2022-08-30 19:51   ` Oliver Upton</b>
  <a href=#r12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:51 UTC (<a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Oliver Upton, Catalin Marinas, Will Deacon
  Cc: linux-arm-kernel, kvmarm, kvm, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, linux-kernel

In order to service stage-2 faults in parallel, stage-2 table walkers
must take exclusive ownership of the PTE being worked on. An additional
requirement of the architecture is that software must perform a
'break-before-make' operation when changing the block size used for
mapping memory.

Roll these two concepts together into helpers for performing a
'break-before-make' sequence. Use a special PTE value to indicate a PTE
has been locked by a software walker. Additionally, use an atomic
compare-exchange to 'break' the PTE when the stage-2 page tables are
possibly shared with another software walker. Elide the DSB + TLBI if
the evicted PTE was invalid (and thus not subject to break-before-make).

All of the atomics do nothing for now, as the stage-2 walker isn't fully
ready to perform parallel walks.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830195102.964724-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830195102.964724-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 87 +++++++++++++++++++++++++++++++++---
 1 file <a href=#e12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>changed</a>, 82 insertions(+), 5 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830195102.964724-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830195102.964724-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 61a4437c8c16..71ae96608752 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -49,6 +49,12 @@
</span> #define KVM_INVALID_PTE_OWNER_MASK	GENMASK(9, 2)
 #define KVM_MAX_OWNER_ID		1
 
<span class=add>+/*
+ * Used to indicate a pte for which a 'break-before-make' sequence is in
+ * progress.
+ */
+#define KVM_INVALID_PTE_LOCKED		BIT(10)
+
</span> struct kvm_pgtable_walk_data {
 	struct kvm_pgtable		*pgt;
 	struct kvm_pgtable_walker	*walker;
<span class=hunk>@@ -586,6 +592,8 @@ struct stage2_map_data {
</span> 
 	/* Force mappings to page granularity */
 	bool				force_pte;
<span class=add>+
+	bool				shared;
</span> };
 
 u64 kvm_get_vtcr(u64 mmfr0, u64 mmfr1, u32 phys_shift)
<span class=hunk>@@ -691,6 +699,11 @@ static bool stage2_pte_is_counted(kvm_pte_t pte)
</span> 	return kvm_pte_valid(pte) || kvm_invalid_pte_owner(pte);
 }
 
<span class=add>+static bool stage2_pte_is_locked(kvm_pte_t pte)
+{
+	return !kvm_pte_valid(pte) &amp;&amp; (pte &amp; KVM_INVALID_PTE_LOCKED);
+}
+
</span> static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bool shared)
 {
 	if (!shared) {
<span class=hunk>@@ -701,6 +714,69 @@ static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bo
</span> 	return cmpxchg(ptep, old, new) == old;
 }
 
<span class=add>+/**
+ * stage2_try_break_pte() - Invalidates a pte according to the
+ *			    'break-before-make' requirements of the
+ *			    architecture.
+ *
+ * @ptep: Pointer to the pte to break
+ * @old: The previously observed value of the pte
+ * @addr: IPA corresponding to the pte
+ * @level: Table level of the pte
+ * @shared: true if the stage-2 page tables could be shared by multiple software
+ *	    walkers
+ *
+ * Returns: true if the pte was successfully broken.
+ *
+ * If the removed pte was valid, performs the necessary serialization and TLB
+ * invalidation for the old value. For counted ptes, drops the reference count
+ * on the containing table page.
+ */
+static bool stage2_try_break_pte(kvm_pte_t *ptep, kvm_pte_t old, u64 addr, u32 level,
+				 struct stage2_map_data *data)
+{
+	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
+
+	if (stage2_pte_is_locked(old)) {
+		/*
+		 * Should never occur if this walker has exclusive access to the
+		 * page tables.
+		 */
+		WARN_ON(!data-&gt;shared);
+		return false;
+	}
+
+	if (!stage2_try_set_pte(ptep, old, KVM_INVALID_PTE_LOCKED, data-&gt;shared))
+		return false;
+
+	/*
+	 * Perform the appropriate TLB invalidation based on the evicted pte
+	 * value (if any).
+	 */
+	if (kvm_pte_table(old, level))
+		kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
+	else if (kvm_pte_valid(old))
+		kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, data-&gt;mmu, addr, level);
+
+	if (stage2_pte_is_counted(old))
+		mm_ops-&gt;put_page(ptep);
+
+	return true;
+}
+
+static void stage2_make_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new,
+			    struct stage2_map_data *data)
+{
+	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
+
+	WARN_ON(!stage2_pte_is_locked(*ptep));
+
+	if (stage2_pte_is_counted(new))
+		mm_ops-&gt;get_page(ptep);
+
+	smp_store_release(ptep, new);
+}
+
</span> static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_s2_mmu *mmu, u64 addr,
 			   u32 level, struct kvm_pgtable_mm_ops *mm_ops)
 {
<span class=hunk>@@ -836,17 +912,18 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 	if (!childp)
 		return -ENOMEM;
 
<span class=add>+	if (!stage2_try_break_pte(ptep, *old, addr, level, data)) {
+		mm_ops-&gt;put_page(childp);
+		return -EAGAIN;
+	}
+
</span> 	/*
 	 * If we've run into an existing block mapping then replace it with
 	 * a table. Accesses beyond 'end' that fall within the new table
 	 * will be mapped lazily.
 	 */
<span class=del>-	if (stage2_pte_is_counted(pte))
-		stage2_put_pte(ptep, data-&gt;mmu, addr, level, mm_ops);
-
</span> 	new = kvm_init_table_pte(childp, mm_ops);
<span class=del>-	mm_ops-&gt;get_page(ptep);
-	smp_store_release(ptep, new);
</span><span class=add>+	stage2_make_pte(ptep, *old, new, data);
</span> 	*old = new;
 
 	return 0;
-- 
2.37.2.672.g94769d06f0-goog


<a href=#m12c4d59838e62fa01806ebbbd99b6882ebdbbcc2 id=e12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>^</a> <a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e12c4d59838e62fa01806ebbbd99b6882ebdbbcc2 id=m12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>*</a> <b>[PATCH 11/14] KVM: arm64: Make changes block-&gt;table to leaf PTEs parallel-aware</b>
<b>@ 2022-08-30 19:51   ` Oliver Upton</b>
  <a href=#r12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:51 UTC (<a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Oliver Upton, Catalin Marinas, Will Deacon
  Cc: linux-arm-kernel, kvmarm, kvm, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, linux-kernel

In order to service stage-2 faults in parallel, stage-2 table walkers
must take exclusive ownership of the PTE being worked on. An additional
requirement of the architecture is that software must perform a
'break-before-make' operation when changing the block size used for
mapping memory.

Roll these two concepts together into helpers for performing a
'break-before-make' sequence. Use a special PTE value to indicate a PTE
has been locked by a software walker. Additionally, use an atomic
compare-exchange to 'break' the PTE when the stage-2 page tables are
possibly shared with another software walker. Elide the DSB + TLBI if
the evicted PTE was invalid (and thus not subject to break-before-make).

All of the atomics do nothing for now, as the stage-2 walker isn't fully
ready to perform parallel walks.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830195102.964724-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830195102.964724-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 87 +++++++++++++++++++++++++++++++++---
 1 file <a href=#e12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>changed</a>, 82 insertions(+), 5 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830195102.964724-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830195102.964724-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 61a4437c8c16..71ae96608752 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -49,6 +49,12 @@
</span> #define KVM_INVALID_PTE_OWNER_MASK	GENMASK(9, 2)
 #define KVM_MAX_OWNER_ID		1
 
<span class=add>+/*
+ * Used to indicate a pte for which a 'break-before-make' sequence is in
+ * progress.
+ */
+#define KVM_INVALID_PTE_LOCKED		BIT(10)
+
</span> struct kvm_pgtable_walk_data {
 	struct kvm_pgtable		*pgt;
 	struct kvm_pgtable_walker	*walker;
<span class=hunk>@@ -586,6 +592,8 @@ struct stage2_map_data {
</span> 
 	/* Force mappings to page granularity */
 	bool				force_pte;
<span class=add>+
+	bool				shared;
</span> };
 
 u64 kvm_get_vtcr(u64 mmfr0, u64 mmfr1, u32 phys_shift)
<span class=hunk>@@ -691,6 +699,11 @@ static bool stage2_pte_is_counted(kvm_pte_t pte)
</span> 	return kvm_pte_valid(pte) || kvm_invalid_pte_owner(pte);
 }
 
<span class=add>+static bool stage2_pte_is_locked(kvm_pte_t pte)
+{
+	return !kvm_pte_valid(pte) &amp;&amp; (pte &amp; KVM_INVALID_PTE_LOCKED);
+}
+
</span> static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bool shared)
 {
 	if (!shared) {
<span class=hunk>@@ -701,6 +714,69 @@ static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bo
</span> 	return cmpxchg(ptep, old, new) == old;
 }
 
<span class=add>+/**
+ * stage2_try_break_pte() - Invalidates a pte according to the
+ *			    'break-before-make' requirements of the
+ *			    architecture.
+ *
+ * @ptep: Pointer to the pte to break
+ * @old: The previously observed value of the pte
+ * @addr: IPA corresponding to the pte
+ * @level: Table level of the pte
+ * @shared: true if the stage-2 page tables could be shared by multiple software
+ *	    walkers
+ *
+ * Returns: true if the pte was successfully broken.
+ *
+ * If the removed pte was valid, performs the necessary serialization and TLB
+ * invalidation for the old value. For counted ptes, drops the reference count
+ * on the containing table page.
+ */
+static bool stage2_try_break_pte(kvm_pte_t *ptep, kvm_pte_t old, u64 addr, u32 level,
+				 struct stage2_map_data *data)
+{
+	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
+
+	if (stage2_pte_is_locked(old)) {
+		/*
+		 * Should never occur if this walker has exclusive access to the
+		 * page tables.
+		 */
+		WARN_ON(!data-&gt;shared);
+		return false;
+	}
+
+	if (!stage2_try_set_pte(ptep, old, KVM_INVALID_PTE_LOCKED, data-&gt;shared))
+		return false;
+
+	/*
+	 * Perform the appropriate TLB invalidation based on the evicted pte
+	 * value (if any).
+	 */
+	if (kvm_pte_table(old, level))
+		kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
+	else if (kvm_pte_valid(old))
+		kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, data-&gt;mmu, addr, level);
+
+	if (stage2_pte_is_counted(old))
+		mm_ops-&gt;put_page(ptep);
+
+	return true;
+}
+
+static void stage2_make_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new,
+			    struct stage2_map_data *data)
+{
+	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
+
+	WARN_ON(!stage2_pte_is_locked(*ptep));
+
+	if (stage2_pte_is_counted(new))
+		mm_ops-&gt;get_page(ptep);
+
+	smp_store_release(ptep, new);
+}
+
</span> static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_s2_mmu *mmu, u64 addr,
 			   u32 level, struct kvm_pgtable_mm_ops *mm_ops)
 {
<span class=hunk>@@ -836,17 +912,18 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 	if (!childp)
 		return -ENOMEM;
 
<span class=add>+	if (!stage2_try_break_pte(ptep, *old, addr, level, data)) {
+		mm_ops-&gt;put_page(childp);
+		return -EAGAIN;
+	}
+
</span> 	/*
 	 * If we've run into an existing block mapping then replace it with
 	 * a table. Accesses beyond 'end' that fall within the new table
 	 * will be mapped lazily.
 	 */
<span class=del>-	if (stage2_pte_is_counted(pte))
-		stage2_put_pte(ptep, data-&gt;mmu, addr, level, mm_ops);
-
</span> 	new = kvm_init_table_pte(childp, mm_ops);
<span class=del>-	mm_ops-&gt;get_page(ptep);
-	smp_store_release(ptep, new);
</span><span class=add>+	stage2_make_pte(ptep, *old, new, data);
</span> 	*old = new;
 
 	return 0;
-- 
2.37.2.672.g94769d06f0-goog


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m12c4d59838e62fa01806ebbbd99b6882ebdbbcc2 id=e12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>^</a> <a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>96+ messages in thread</a></pre><li><pre><a href=#e12c4d59838e62fa01806ebbbd99b6882ebdbbcc2 id=m12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>*</a> <b>[PATCH 11/14] KVM: arm64: Make changes block-&gt;table to leaf PTEs parallel-aware</b>
<b>@ 2022-08-30 19:51   ` Oliver Upton</b>
  <a href=#r12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:51 UTC (<a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Oliver Upton, Catalin Marinas, Will Deacon
  Cc: kvm, linux-kernel, Ben Gardon, David Matlack, Paolo Bonzini,
	kvmarm, linux-arm-kernel

In order to service stage-2 faults in parallel, stage-2 table walkers
must take exclusive ownership of the PTE being worked on. An additional
requirement of the architecture is that software must perform a
'break-before-make' operation when changing the block size used for
mapping memory.

Roll these two concepts together into helpers for performing a
'break-before-make' sequence. Use a special PTE value to indicate a PTE
has been locked by a software walker. Additionally, use an atomic
compare-exchange to 'break' the PTE when the stage-2 page tables are
possibly shared with another software walker. Elide the DSB + TLBI if
the evicted PTE was invalid (and thus not subject to break-before-make).

All of the atomics do nothing for now, as the stage-2 walker isn't fully
ready to perform parallel walks.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830195102.964724-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830195102.964724-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 87 +++++++++++++++++++++++++++++++++---
 1 file <a href=#e12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>changed</a>, 82 insertions(+), 5 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830195102.964724-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830195102.964724-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 61a4437c8c16..71ae96608752 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -49,6 +49,12 @@
</span> #define KVM_INVALID_PTE_OWNER_MASK	GENMASK(9, 2)
 #define KVM_MAX_OWNER_ID		1
 
<span class=add>+/*
+ * Used to indicate a pte for which a 'break-before-make' sequence is in
+ * progress.
+ */
+#define KVM_INVALID_PTE_LOCKED		BIT(10)
+
</span> struct kvm_pgtable_walk_data {
 	struct kvm_pgtable		*pgt;
 	struct kvm_pgtable_walker	*walker;
<span class=hunk>@@ -586,6 +592,8 @@ struct stage2_map_data {
</span> 
 	/* Force mappings to page granularity */
 	bool				force_pte;
<span class=add>+
+	bool				shared;
</span> };
 
 u64 kvm_get_vtcr(u64 mmfr0, u64 mmfr1, u32 phys_shift)
<span class=hunk>@@ -691,6 +699,11 @@ static bool stage2_pte_is_counted(kvm_pte_t pte)
</span> 	return kvm_pte_valid(pte) || kvm_invalid_pte_owner(pte);
 }
 
<span class=add>+static bool stage2_pte_is_locked(kvm_pte_t pte)
+{
+	return !kvm_pte_valid(pte) &amp;&amp; (pte &amp; KVM_INVALID_PTE_LOCKED);
+}
+
</span> static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bool shared)
 {
 	if (!shared) {
<span class=hunk>@@ -701,6 +714,69 @@ static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bo
</span> 	return cmpxchg(ptep, old, new) == old;
 }
 
<span class=add>+/**
+ * stage2_try_break_pte() - Invalidates a pte according to the
+ *			    'break-before-make' requirements of the
+ *			    architecture.
+ *
+ * @ptep: Pointer to the pte to break
+ * @old: The previously observed value of the pte
+ * @addr: IPA corresponding to the pte
+ * @level: Table level of the pte
+ * @shared: true if the stage-2 page tables could be shared by multiple software
+ *	    walkers
+ *
+ * Returns: true if the pte was successfully broken.
+ *
+ * If the removed pte was valid, performs the necessary serialization and TLB
+ * invalidation for the old value. For counted ptes, drops the reference count
+ * on the containing table page.
+ */
+static bool stage2_try_break_pte(kvm_pte_t *ptep, kvm_pte_t old, u64 addr, u32 level,
+				 struct stage2_map_data *data)
+{
+	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
+
+	if (stage2_pte_is_locked(old)) {
+		/*
+		 * Should never occur if this walker has exclusive access to the
+		 * page tables.
+		 */
+		WARN_ON(!data-&gt;shared);
+		return false;
+	}
+
+	if (!stage2_try_set_pte(ptep, old, KVM_INVALID_PTE_LOCKED, data-&gt;shared))
+		return false;
+
+	/*
+	 * Perform the appropriate TLB invalidation based on the evicted pte
+	 * value (if any).
+	 */
+	if (kvm_pte_table(old, level))
+		kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
+	else if (kvm_pte_valid(old))
+		kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, data-&gt;mmu, addr, level);
+
+	if (stage2_pte_is_counted(old))
+		mm_ops-&gt;put_page(ptep);
+
+	return true;
+}
+
+static void stage2_make_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new,
+			    struct stage2_map_data *data)
+{
+	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
+
+	WARN_ON(!stage2_pte_is_locked(*ptep));
+
+	if (stage2_pte_is_counted(new))
+		mm_ops-&gt;get_page(ptep);
+
+	smp_store_release(ptep, new);
+}
+
</span> static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_s2_mmu *mmu, u64 addr,
 			   u32 level, struct kvm_pgtable_mm_ops *mm_ops)
 {
<span class=hunk>@@ -836,17 +912,18 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
</span> 	if (!childp)
 		return -ENOMEM;
 
<span class=add>+	if (!stage2_try_break_pte(ptep, *old, addr, level, data)) {
+		mm_ops-&gt;put_page(childp);
+		return -EAGAIN;
+	}
+
</span> 	/*
 	 * If we've run into an existing block mapping then replace it with
 	 * a table. Accesses beyond 'end' that fall within the new table
 	 * will be mapped lazily.
 	 */
<span class=del>-	if (stage2_pte_is_counted(pte))
-		stage2_put_pte(ptep, data-&gt;mmu, addr, level, mm_ops);
-
</span> 	new = kvm_init_table_pte(childp, mm_ops);
<span class=del>-	mm_ops-&gt;get_page(ptep);
-	smp_store_release(ptep, new);
</span><span class=add>+	stage2_make_pte(ptep, *old, new, data);
</span> 	*old = new;
 
 	return 0;
-- 
2.37.2.672.g94769d06f0-goog

_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m12c4d59838e62fa01806ebbbd99b6882ebdbbcc2 id=e12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>^</a> <a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830195102.964724-1-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>96+ messages in thread</a></pre><li><pre><a href=#eee4bebe450c8627f2ff70393c2bd5400869ff940 id=mee4bebe450c8627f2ff70393c2bd5400869ff940>*</a> <b>Re: [PATCH 11/14] KVM: arm64: Make changes block-&gt;table to leaf PTEs parallel-aware</b>
  2022-08-30 19:51   ` <a href=#m12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>Oliver Upton</a>
  (?)
<b>@ 2022-09-14  0:51     ` Ricardo Koller</b>
  <a href=#ree4bebe450c8627f2ff70393c2bd5400869ff940>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Ricardo Koller @ 2022-09-14  0:51 UTC (<a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, linux-arm-kernel, kvmarm, kvm,
	Quentin Perret, Reiji Watanabe, David Matlack, Ben Gardon,
	Paolo Bonzini, Gavin Shan, Peter Xu, Sean Christopherson,
	linux-kernel

On Tue, Aug 30, 2022 at 07:51:01PM +0000, Oliver Upton wrote:
<span class=q>&gt; In order to service stage-2 faults in parallel, stage-2 table walkers
&gt; must take exclusive ownership of the PTE being worked on. An additional
&gt; requirement of the architecture is that software must perform a
&gt; 'break-before-make' operation when changing the block size used for
&gt; mapping memory.
&gt; 
&gt; Roll these two concepts together into helpers for performing a
&gt; 'break-before-make' sequence. Use a special PTE value to indicate a PTE
&gt; has been locked by a software walker. Additionally, use an atomic
&gt; compare-exchange to 'break' the PTE when the stage-2 page tables are
&gt; possibly shared with another software walker. Elide the DSB + TLBI if
&gt; the evicted PTE was invalid (and thus not subject to break-before-make).
&gt; 
&gt; All of the atomics do nothing for now, as the stage-2 walker isn't fully
&gt; ready to perform parallel walks.
&gt; 
&gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; ---
&gt;  arch/arm64/kvm/hyp/pgtable.c | 87 +++++++++++++++++++++++++++++++++---
&gt;  1 file changed, 82 insertions(+), 5 deletions(-)
&gt; 
&gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; index 61a4437c8c16..71ae96608752 100644
&gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; @@ -49,6 +49,12 @@
&gt;  #define KVM_INVALID_PTE_OWNER_MASK	GENMASK(9, 2)
&gt;  #define KVM_MAX_OWNER_ID		1
&gt;  
&gt; +/*
&gt; + * Used to indicate a pte for which a 'break-before-make' sequence is in
&gt; + * progress.
&gt; + */
&gt; +#define KVM_INVALID_PTE_LOCKED		BIT(10)
&gt; +
&gt;  struct kvm_pgtable_walk_data {
&gt;  	struct kvm_pgtable		*pgt;
&gt;  	struct kvm_pgtable_walker	*walker;
&gt; @@ -586,6 +592,8 @@ struct stage2_map_data {
&gt;  
&gt;  	/* Force mappings to page granularity */
&gt;  	bool				force_pte;
&gt; +
&gt; +	bool				shared;
&gt;  };
&gt;  
&gt;  u64 kvm_get_vtcr(u64 mmfr0, u64 mmfr1, u32 phys_shift)
&gt; @@ -691,6 +699,11 @@ static bool stage2_pte_is_counted(kvm_pte_t pte)
&gt;  	return kvm_pte_valid(pte) || kvm_invalid_pte_owner(pte);
&gt;  }
&gt;  
&gt; +static bool stage2_pte_is_locked(kvm_pte_t pte)
&gt; +{
&gt; +	return !kvm_pte_valid(pte) &amp;&amp; (pte &amp; KVM_INVALID_PTE_LOCKED);
&gt; +}
&gt; +
&gt;  static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bool shared)
&gt;  {
&gt;  	if (!shared) {
&gt; @@ -701,6 +714,69 @@ static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bo
&gt;  	return cmpxchg(ptep, old, new) == old;
&gt;  }
&gt;  
&gt; +/**
&gt; + * stage2_try_break_pte() - Invalidates a pte according to the
&gt; + *			    'break-before-make' requirements of the
&gt; + *			    architecture.
&gt; + *
&gt; + * @ptep: Pointer to the pte to break
&gt; + * @old: The previously observed value of the pte
&gt; + * @addr: IPA corresponding to the pte
&gt; + * @level: Table level of the pte
&gt; + * @shared: true if the stage-2 page tables could be shared by multiple software
&gt; + *	    walkers
&gt; + *
&gt; + * Returns: true if the pte was successfully broken.
&gt; + *
&gt; + * If the removed pte was valid, performs the necessary serialization and TLB
&gt; + * invalidation for the old value. For counted ptes, drops the reference count
&gt; + * on the containing table page.
&gt; + */
&gt; +static bool stage2_try_break_pte(kvm_pte_t *ptep, kvm_pte_t old, u64 addr, u32 level,
&gt; +				 struct stage2_map_data *data)
&gt; +{
&gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; +
&gt; +	if (stage2_pte_is_locked(old)) {
&gt; +		/*
&gt; +		 * Should never occur if this walker has exclusive access to the
&gt; +		 * page tables.
&gt; +		 */
&gt; +		WARN_ON(!data-&gt;shared);
&gt; +		return false;
&gt; +	}
</span>
The above check is not needed as the cmpxchg() will return false if the
old pte is equal to "new" (KVM_INVALID_PTE_LOCKED).

<span class=q>&gt; +
&gt; +	if (!stage2_try_set_pte(ptep, old, KVM_INVALID_PTE_LOCKED, data-&gt;shared))
&gt; +		return false;
&gt; +
&gt; +	/*
&gt; +	 * Perform the appropriate TLB invalidation based on the evicted pte
&gt; +	 * value (if any).
&gt; +	 */
&gt; +	if (kvm_pte_table(old, level))
&gt; +		kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt; +	else if (kvm_pte_valid(old))
&gt; +		kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, data-&gt;mmu, addr, level);
&gt; +
&gt; +	if (stage2_pte_is_counted(old))
&gt; +		mm_ops-&gt;put_page(ptep);
&gt; +
&gt; +	return true;
&gt; +}
&gt; +
&gt; +static void stage2_make_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new,
&gt; +			    struct stage2_map_data *data)
&gt; +{
&gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; +
&gt; +	WARN_ON(!stage2_pte_is_locked(*ptep));
&gt; +
&gt; +	if (stage2_pte_is_counted(new))
&gt; +		mm_ops-&gt;get_page(ptep);
&gt; +
&gt; +	smp_store_release(ptep, new);
&gt; +}
&gt; +
&gt;  static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_s2_mmu *mmu, u64 addr,
&gt;  			   u32 level, struct kvm_pgtable_mm_ops *mm_ops)
&gt;  {
&gt; @@ -836,17 +912,18 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  	if (!childp)
&gt;  		return -ENOMEM;
&gt;  
&gt; +	if (!stage2_try_break_pte(ptep, *old, addr, level, data)) {
&gt; +		mm_ops-&gt;put_page(childp);
&gt; +		return -EAGAIN;
&gt; +	}
&gt; +
&gt;  	/*
&gt;  	 * If we've run into an existing block mapping then replace it with
&gt;  	 * a table. Accesses beyond 'end' that fall within the new table
&gt;  	 * will be mapped lazily.
&gt;  	 */
&gt; -	if (stage2_pte_is_counted(pte))
&gt; -		stage2_put_pte(ptep, data-&gt;mmu, addr, level, mm_ops);
&gt; -
&gt;  	new = kvm_init_table_pte(childp, mm_ops);
&gt; -	mm_ops-&gt;get_page(ptep);
&gt; -	smp_store_release(ptep, new);
&gt; +	stage2_make_pte(ptep, *old, new, data);
&gt;  	*old = new;
&gt;  
&gt;  	return 0;
&gt; -- 
&gt; 2.37.2.672.g94769d06f0-goog
&gt; 
</span>
<a href=#mee4bebe450c8627f2ff70393c2bd5400869ff940 id=eee4bebe450c8627f2ff70393c2bd5400869ff940>^</a> <a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/t/#u><b>nested</b></a>] <a href=#ree4bebe450c8627f2ff70393c2bd5400869ff940>96+ messages in thread</a></pre><li><ul><li><pre><a href=#eee4bebe450c8627f2ff70393c2bd5400869ff940 id=mee4bebe450c8627f2ff70393c2bd5400869ff940>*</a> <b>Re: [PATCH 11/14] KVM: arm64: Make changes block-&gt;table to leaf PTEs parallel-aware</b>
<b>@ 2022-09-14  0:51     ` Ricardo Koller</b>
  <a href=#ree4bebe450c8627f2ff70393c2bd5400869ff940>0 siblings, 0 replies; 96+ messages in thread</a>
From: Ricardo Koller @ 2022-09-14  0:51 UTC (<a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, linux-arm-kernel, kvmarm, kvm,
	Quentin Perret, Reiji Watanabe, David Matlack, Ben Gardon,
	Paolo Bonzini, Gavin Shan, Peter Xu, Sean Christopherson,
	linux-kernel

On Tue, Aug 30, 2022 at 07:51:01PM +0000, Oliver Upton wrote:
<span class=q>&gt; In order to service stage-2 faults in parallel, stage-2 table walkers
&gt; must take exclusive ownership of the PTE being worked on. An additional
&gt; requirement of the architecture is that software must perform a
&gt; 'break-before-make' operation when changing the block size used for
&gt; mapping memory.
&gt; 
&gt; Roll these two concepts together into helpers for performing a
&gt; 'break-before-make' sequence. Use a special PTE value to indicate a PTE
&gt; has been locked by a software walker. Additionally, use an atomic
&gt; compare-exchange to 'break' the PTE when the stage-2 page tables are
&gt; possibly shared with another software walker. Elide the DSB + TLBI if
&gt; the evicted PTE was invalid (and thus not subject to break-before-make).
&gt; 
&gt; All of the atomics do nothing for now, as the stage-2 walker isn't fully
&gt; ready to perform parallel walks.
&gt; 
&gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; ---
&gt;  arch/arm64/kvm/hyp/pgtable.c | 87 +++++++++++++++++++++++++++++++++---
&gt;  1 file changed, 82 insertions(+), 5 deletions(-)
&gt; 
&gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; index 61a4437c8c16..71ae96608752 100644
&gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; @@ -49,6 +49,12 @@
&gt;  #define KVM_INVALID_PTE_OWNER_MASK	GENMASK(9, 2)
&gt;  #define KVM_MAX_OWNER_ID		1
&gt;  
&gt; +/*
&gt; + * Used to indicate a pte for which a 'break-before-make' sequence is in
&gt; + * progress.
&gt; + */
&gt; +#define KVM_INVALID_PTE_LOCKED		BIT(10)
&gt; +
&gt;  struct kvm_pgtable_walk_data {
&gt;  	struct kvm_pgtable		*pgt;
&gt;  	struct kvm_pgtable_walker	*walker;
&gt; @@ -586,6 +592,8 @@ struct stage2_map_data {
&gt;  
&gt;  	/* Force mappings to page granularity */
&gt;  	bool				force_pte;
&gt; +
&gt; +	bool				shared;
&gt;  };
&gt;  
&gt;  u64 kvm_get_vtcr(u64 mmfr0, u64 mmfr1, u32 phys_shift)
&gt; @@ -691,6 +699,11 @@ static bool stage2_pte_is_counted(kvm_pte_t pte)
&gt;  	return kvm_pte_valid(pte) || kvm_invalid_pte_owner(pte);
&gt;  }
&gt;  
&gt; +static bool stage2_pte_is_locked(kvm_pte_t pte)
&gt; +{
&gt; +	return !kvm_pte_valid(pte) &amp;&amp; (pte &amp; KVM_INVALID_PTE_LOCKED);
&gt; +}
&gt; +
&gt;  static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bool shared)
&gt;  {
&gt;  	if (!shared) {
&gt; @@ -701,6 +714,69 @@ static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bo
&gt;  	return cmpxchg(ptep, old, new) == old;
&gt;  }
&gt;  
&gt; +/**
&gt; + * stage2_try_break_pte() - Invalidates a pte according to the
&gt; + *			    'break-before-make' requirements of the
&gt; + *			    architecture.
&gt; + *
&gt; + * @ptep: Pointer to the pte to break
&gt; + * @old: The previously observed value of the pte
&gt; + * @addr: IPA corresponding to the pte
&gt; + * @level: Table level of the pte
&gt; + * @shared: true if the stage-2 page tables could be shared by multiple software
&gt; + *	    walkers
&gt; + *
&gt; + * Returns: true if the pte was successfully broken.
&gt; + *
&gt; + * If the removed pte was valid, performs the necessary serialization and TLB
&gt; + * invalidation for the old value. For counted ptes, drops the reference count
&gt; + * on the containing table page.
&gt; + */
&gt; +static bool stage2_try_break_pte(kvm_pte_t *ptep, kvm_pte_t old, u64 addr, u32 level,
&gt; +				 struct stage2_map_data *data)
&gt; +{
&gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; +
&gt; +	if (stage2_pte_is_locked(old)) {
&gt; +		/*
&gt; +		 * Should never occur if this walker has exclusive access to the
&gt; +		 * page tables.
&gt; +		 */
&gt; +		WARN_ON(!data-&gt;shared);
&gt; +		return false;
&gt; +	}
</span>
The above check is not needed as the cmpxchg() will return false if the
old pte is equal to "new" (KVM_INVALID_PTE_LOCKED).

<span class=q>&gt; +
&gt; +	if (!stage2_try_set_pte(ptep, old, KVM_INVALID_PTE_LOCKED, data-&gt;shared))
&gt; +		return false;
&gt; +
&gt; +	/*
&gt; +	 * Perform the appropriate TLB invalidation based on the evicted pte
&gt; +	 * value (if any).
&gt; +	 */
&gt; +	if (kvm_pte_table(old, level))
&gt; +		kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt; +	else if (kvm_pte_valid(old))
&gt; +		kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, data-&gt;mmu, addr, level);
&gt; +
&gt; +	if (stage2_pte_is_counted(old))
&gt; +		mm_ops-&gt;put_page(ptep);
&gt; +
&gt; +	return true;
&gt; +}
&gt; +
&gt; +static void stage2_make_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new,
&gt; +			    struct stage2_map_data *data)
&gt; +{
&gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; +
&gt; +	WARN_ON(!stage2_pte_is_locked(*ptep));
&gt; +
&gt; +	if (stage2_pte_is_counted(new))
&gt; +		mm_ops-&gt;get_page(ptep);
&gt; +
&gt; +	smp_store_release(ptep, new);
&gt; +}
&gt; +
&gt;  static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_s2_mmu *mmu, u64 addr,
&gt;  			   u32 level, struct kvm_pgtable_mm_ops *mm_ops)
&gt;  {
&gt; @@ -836,17 +912,18 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  	if (!childp)
&gt;  		return -ENOMEM;
&gt;  
&gt; +	if (!stage2_try_break_pte(ptep, *old, addr, level, data)) {
&gt; +		mm_ops-&gt;put_page(childp);
&gt; +		return -EAGAIN;
&gt; +	}
&gt; +
&gt;  	/*
&gt;  	 * If we've run into an existing block mapping then replace it with
&gt;  	 * a table. Accesses beyond 'end' that fall within the new table
&gt;  	 * will be mapped lazily.
&gt;  	 */
&gt; -	if (stage2_pte_is_counted(pte))
&gt; -		stage2_put_pte(ptep, data-&gt;mmu, addr, level, mm_ops);
&gt; -
&gt;  	new = kvm_init_table_pte(childp, mm_ops);
&gt; -	mm_ops-&gt;get_page(ptep);
&gt; -	smp_store_release(ptep, new);
&gt; +	stage2_make_pte(ptep, *old, new, data);
&gt;  	*old = new;
&gt;  
&gt;  	return 0;
&gt; -- 
&gt; 2.37.2.672.g94769d06f0-goog
&gt; 
</span>
_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#mee4bebe450c8627f2ff70393c2bd5400869ff940 id=eee4bebe450c8627f2ff70393c2bd5400869ff940>^</a> <a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/t/#u><b>nested</b></a>] <a href=#ree4bebe450c8627f2ff70393c2bd5400869ff940>96+ messages in thread</a></pre><li><pre><a href=#eee4bebe450c8627f2ff70393c2bd5400869ff940 id=mee4bebe450c8627f2ff70393c2bd5400869ff940>*</a> <b>Re: [PATCH 11/14] KVM: arm64: Make changes block-&gt;table to leaf PTEs parallel-aware</b>
<b>@ 2022-09-14  0:51     ` Ricardo Koller</b>
  <a href=#ree4bebe450c8627f2ff70393c2bd5400869ff940>0 siblings, 0 replies; 96+ messages in thread</a>
From: Ricardo Koller @ 2022-09-14  0:51 UTC (<a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: kvm, Marc Zyngier, linux-kernel, Ben Gardon, Catalin Marinas,
	David Matlack, Paolo Bonzini, Will Deacon, kvmarm,
	linux-arm-kernel

On Tue, Aug 30, 2022 at 07:51:01PM +0000, Oliver Upton wrote:
<span class=q>&gt; In order to service stage-2 faults in parallel, stage-2 table walkers
&gt; must take exclusive ownership of the PTE being worked on. An additional
&gt; requirement of the architecture is that software must perform a
&gt; 'break-before-make' operation when changing the block size used for
&gt; mapping memory.
&gt; 
&gt; Roll these two concepts together into helpers for performing a
&gt; 'break-before-make' sequence. Use a special PTE value to indicate a PTE
&gt; has been locked by a software walker. Additionally, use an atomic
&gt; compare-exchange to 'break' the PTE when the stage-2 page tables are
&gt; possibly shared with another software walker. Elide the DSB + TLBI if
&gt; the evicted PTE was invalid (and thus not subject to break-before-make).
&gt; 
&gt; All of the atomics do nothing for now, as the stage-2 walker isn't fully
&gt; ready to perform parallel walks.
&gt; 
&gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; ---
&gt;  arch/arm64/kvm/hyp/pgtable.c | 87 +++++++++++++++++++++++++++++++++---
&gt;  1 file changed, 82 insertions(+), 5 deletions(-)
&gt; 
&gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; index 61a4437c8c16..71ae96608752 100644
&gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; @@ -49,6 +49,12 @@
&gt;  #define KVM_INVALID_PTE_OWNER_MASK	GENMASK(9, 2)
&gt;  #define KVM_MAX_OWNER_ID		1
&gt;  
&gt; +/*
&gt; + * Used to indicate a pte for which a 'break-before-make' sequence is in
&gt; + * progress.
&gt; + */
&gt; +#define KVM_INVALID_PTE_LOCKED		BIT(10)
&gt; +
&gt;  struct kvm_pgtable_walk_data {
&gt;  	struct kvm_pgtable		*pgt;
&gt;  	struct kvm_pgtable_walker	*walker;
&gt; @@ -586,6 +592,8 @@ struct stage2_map_data {
&gt;  
&gt;  	/* Force mappings to page granularity */
&gt;  	bool				force_pte;
&gt; +
&gt; +	bool				shared;
&gt;  };
&gt;  
&gt;  u64 kvm_get_vtcr(u64 mmfr0, u64 mmfr1, u32 phys_shift)
&gt; @@ -691,6 +699,11 @@ static bool stage2_pte_is_counted(kvm_pte_t pte)
&gt;  	return kvm_pte_valid(pte) || kvm_invalid_pte_owner(pte);
&gt;  }
&gt;  
&gt; +static bool stage2_pte_is_locked(kvm_pte_t pte)
&gt; +{
&gt; +	return !kvm_pte_valid(pte) &amp;&amp; (pte &amp; KVM_INVALID_PTE_LOCKED);
&gt; +}
&gt; +
&gt;  static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bool shared)
&gt;  {
&gt;  	if (!shared) {
&gt; @@ -701,6 +714,69 @@ static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bo
&gt;  	return cmpxchg(ptep, old, new) == old;
&gt;  }
&gt;  
&gt; +/**
&gt; + * stage2_try_break_pte() - Invalidates a pte according to the
&gt; + *			    'break-before-make' requirements of the
&gt; + *			    architecture.
&gt; + *
&gt; + * @ptep: Pointer to the pte to break
&gt; + * @old: The previously observed value of the pte
&gt; + * @addr: IPA corresponding to the pte
&gt; + * @level: Table level of the pte
&gt; + * @shared: true if the stage-2 page tables could be shared by multiple software
&gt; + *	    walkers
&gt; + *
&gt; + * Returns: true if the pte was successfully broken.
&gt; + *
&gt; + * If the removed pte was valid, performs the necessary serialization and TLB
&gt; + * invalidation for the old value. For counted ptes, drops the reference count
&gt; + * on the containing table page.
&gt; + */
&gt; +static bool stage2_try_break_pte(kvm_pte_t *ptep, kvm_pte_t old, u64 addr, u32 level,
&gt; +				 struct stage2_map_data *data)
&gt; +{
&gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; +
&gt; +	if (stage2_pte_is_locked(old)) {
&gt; +		/*
&gt; +		 * Should never occur if this walker has exclusive access to the
&gt; +		 * page tables.
&gt; +		 */
&gt; +		WARN_ON(!data-&gt;shared);
&gt; +		return false;
&gt; +	}
</span>
The above check is not needed as the cmpxchg() will return false if the
old pte is equal to "new" (KVM_INVALID_PTE_LOCKED).

<span class=q>&gt; +
&gt; +	if (!stage2_try_set_pte(ptep, old, KVM_INVALID_PTE_LOCKED, data-&gt;shared))
&gt; +		return false;
&gt; +
&gt; +	/*
&gt; +	 * Perform the appropriate TLB invalidation based on the evicted pte
&gt; +	 * value (if any).
&gt; +	 */
&gt; +	if (kvm_pte_table(old, level))
&gt; +		kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt; +	else if (kvm_pte_valid(old))
&gt; +		kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, data-&gt;mmu, addr, level);
&gt; +
&gt; +	if (stage2_pte_is_counted(old))
&gt; +		mm_ops-&gt;put_page(ptep);
&gt; +
&gt; +	return true;
&gt; +}
&gt; +
&gt; +static void stage2_make_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new,
&gt; +			    struct stage2_map_data *data)
&gt; +{
&gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; +
&gt; +	WARN_ON(!stage2_pte_is_locked(*ptep));
&gt; +
&gt; +	if (stage2_pte_is_counted(new))
&gt; +		mm_ops-&gt;get_page(ptep);
&gt; +
&gt; +	smp_store_release(ptep, new);
&gt; +}
&gt; +
&gt;  static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_s2_mmu *mmu, u64 addr,
&gt;  			   u32 level, struct kvm_pgtable_mm_ops *mm_ops)
&gt;  {
&gt; @@ -836,17 +912,18 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt;  	if (!childp)
&gt;  		return -ENOMEM;
&gt;  
&gt; +	if (!stage2_try_break_pte(ptep, *old, addr, level, data)) {
&gt; +		mm_ops-&gt;put_page(childp);
&gt; +		return -EAGAIN;
&gt; +	}
&gt; +
&gt;  	/*
&gt;  	 * If we've run into an existing block mapping then replace it with
&gt;  	 * a table. Accesses beyond 'end' that fall within the new table
&gt;  	 * will be mapped lazily.
&gt;  	 */
&gt; -	if (stage2_pte_is_counted(pte))
&gt; -		stage2_put_pte(ptep, data-&gt;mmu, addr, level, mm_ops);
&gt; -
&gt;  	new = kvm_init_table_pte(childp, mm_ops);
&gt; -	mm_ops-&gt;get_page(ptep);
&gt; -	smp_store_release(ptep, new);
&gt; +	stage2_make_pte(ptep, *old, new, data);
&gt;  	*old = new;
&gt;  
&gt;  	return 0;
&gt; -- 
&gt; 2.37.2.672.g94769d06f0-goog
&gt; 
</span>_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#mee4bebe450c8627f2ff70393c2bd5400869ff940 id=eee4bebe450c8627f2ff70393c2bd5400869ff940>^</a> <a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YyElq0c6WD1zh7Lu@google.com/t/#u><b>nested</b></a>] <a href=#ree4bebe450c8627f2ff70393c2bd5400869ff940>96+ messages in thread</a></pre><li><pre><a href=#e77ade85e72dafda18c138e2b7a41ffd92979d92e id=m77ade85e72dafda18c138e2b7a41ffd92979d92e>*</a> <b>Re: [PATCH 11/14] KVM: arm64: Make changes block-&gt;table to leaf PTEs parallel-aware</b>
  2022-09-14  0:51     ` <a href=#mee4bebe450c8627f2ff70393c2bd5400869ff940>Ricardo Koller</a>
  (?)
<b>@ 2022-09-14  0:53       ` Ricardo Koller</b>
  <a href=#r77ade85e72dafda18c138e2b7a41ffd92979d92e>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Ricardo Koller @ 2022-09-14  0:53 UTC (<a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, linux-arm-kernel, kvmarm, kvm,
	Quentin Perret, Reiji Watanabe, David Matlack, Ben Gardon,
	Paolo Bonzini, Gavin Shan, Peter Xu, Sean Christopherson,
	linux-kernel

On Tue, Sep 13, 2022 at 05:51:55PM -0700, Ricardo Koller wrote:
<span class=q>&gt; On Tue, Aug 30, 2022 at 07:51:01PM +0000, Oliver Upton wrote:
&gt; &gt; In order to service stage-2 faults in parallel, stage-2 table walkers
&gt; &gt; must take exclusive ownership of the PTE being worked on. An additional
&gt; &gt; requirement of the architecture is that software must perform a
&gt; &gt; 'break-before-make' operation when changing the block size used for
&gt; &gt; mapping memory.
&gt; &gt; 
&gt; &gt; Roll these two concepts together into helpers for performing a
&gt; &gt; 'break-before-make' sequence. Use a special PTE value to indicate a PTE
&gt; &gt; has been locked by a software walker. Additionally, use an atomic
&gt; &gt; compare-exchange to 'break' the PTE when the stage-2 page tables are
&gt; &gt; possibly shared with another software walker. Elide the DSB + TLBI if
&gt; &gt; the evicted PTE was invalid (and thus not subject to break-before-make).
&gt; &gt; 
&gt; &gt; All of the atomics do nothing for now, as the stage-2 walker isn't fully
&gt; &gt; ready to perform parallel walks.
&gt; &gt; 
&gt; &gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; &gt; ---
&gt; &gt;  arch/arm64/kvm/hyp/pgtable.c | 87 +++++++++++++++++++++++++++++++++---
&gt; &gt;  1 file changed, 82 insertions(+), 5 deletions(-)
&gt; &gt; 
&gt; &gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; index 61a4437c8c16..71ae96608752 100644
&gt; &gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; @@ -49,6 +49,12 @@
&gt; &gt;  #define KVM_INVALID_PTE_OWNER_MASK	GENMASK(9, 2)
&gt; &gt;  #define KVM_MAX_OWNER_ID		1
&gt; &gt;  
&gt; &gt; +/*
&gt; &gt; + * Used to indicate a pte for which a 'break-before-make' sequence is in
&gt; &gt; + * progress.
&gt; &gt; + */
&gt; &gt; +#define KVM_INVALID_PTE_LOCKED		BIT(10)
&gt; &gt; +
&gt; &gt;  struct kvm_pgtable_walk_data {
&gt; &gt;  	struct kvm_pgtable		*pgt;
&gt; &gt;  	struct kvm_pgtable_walker	*walker;
&gt; &gt; @@ -586,6 +592,8 @@ struct stage2_map_data {
&gt; &gt;  
&gt; &gt;  	/* Force mappings to page granularity */
&gt; &gt;  	bool				force_pte;
&gt; &gt; +
&gt; &gt; +	bool				shared;
&gt; &gt;  };
&gt; &gt;  
&gt; &gt;  u64 kvm_get_vtcr(u64 mmfr0, u64 mmfr1, u32 phys_shift)
&gt; &gt; @@ -691,6 +699,11 @@ static bool stage2_pte_is_counted(kvm_pte_t pte)
&gt; &gt;  	return kvm_pte_valid(pte) || kvm_invalid_pte_owner(pte);
&gt; &gt;  }
&gt; &gt;  
&gt; &gt; +static bool stage2_pte_is_locked(kvm_pte_t pte)
&gt; &gt; +{
&gt; &gt; +	return !kvm_pte_valid(pte) &amp;&amp; (pte &amp; KVM_INVALID_PTE_LOCKED);
&gt; &gt; +}
&gt; &gt; +
&gt; &gt;  static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bool shared)
&gt; &gt;  {
&gt; &gt;  	if (!shared) {
&gt; &gt; @@ -701,6 +714,69 @@ static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bo
&gt; &gt;  	return cmpxchg(ptep, old, new) == old;
&gt; &gt;  }
&gt; &gt;  
&gt; &gt; +/**
&gt; &gt; + * stage2_try_break_pte() - Invalidates a pte according to the
&gt; &gt; + *			    'break-before-make' requirements of the
&gt; &gt; + *			    architecture.
&gt; &gt; + *
&gt; &gt; + * @ptep: Pointer to the pte to break
&gt; &gt; + * @old: The previously observed value of the pte
&gt; &gt; + * @addr: IPA corresponding to the pte
&gt; &gt; + * @level: Table level of the pte
&gt; &gt; + * @shared: true if the stage-2 page tables could be shared by multiple software
&gt; &gt; + *	    walkers
&gt; &gt; + *
&gt; &gt; + * Returns: true if the pte was successfully broken.
&gt; &gt; + *
&gt; &gt; + * If the removed pte was valid, performs the necessary serialization and TLB
&gt; &gt; + * invalidation for the old value. For counted ptes, drops the reference count
&gt; &gt; + * on the containing table page.
&gt; &gt; + */
&gt; &gt; +static bool stage2_try_break_pte(kvm_pte_t *ptep, kvm_pte_t old, u64 addr, u32 level,
&gt; &gt; +				 struct stage2_map_data *data)
&gt; &gt; +{
&gt; &gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; &gt; +
&gt; &gt; +	if (stage2_pte_is_locked(old)) {
&gt; &gt; +		/*
&gt; &gt; +		 * Should never occur if this walker has exclusive access to the
&gt; &gt; +		 * page tables.
&gt; &gt; +		 */
&gt; &gt; +		WARN_ON(!data-&gt;shared);
&gt; &gt; +		return false;
&gt; &gt; +	}
&gt; 
&gt; The above check is not needed as the cmpxchg() will return false if the
&gt; old pte is equal to "new" (KVM_INVALID_PTE_LOCKED).
&gt; 
&gt; &gt; +
&gt; &gt; +	if (!stage2_try_set_pte(ptep, old, KVM_INVALID_PTE_LOCKED, data-&gt;shared))
&gt; &gt; +		return false;
&gt; &gt; +
&gt; &gt; +	/*
&gt; &gt; +	 * Perform the appropriate TLB invalidation based on the evicted pte
&gt; &gt; +	 * value (if any).
&gt; &gt; +	 */
&gt; &gt; +	if (kvm_pte_table(old, level))
&gt; &gt; +		kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt; &gt; +	else if (kvm_pte_valid(old))
&gt; &gt; +		kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, data-&gt;mmu, addr, level);
&gt; &gt; +
&gt; &gt; +	if (stage2_pte_is_counted(old))
&gt; &gt; +		mm_ops-&gt;put_page(ptep);
&gt; &gt; +
&gt; &gt; +	return true;
&gt; &gt; +}
&gt; &gt; +
&gt; &gt; +static void stage2_make_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new,
&gt; &gt; +			    struct stage2_map_data *data)
&gt; &gt; +{
</span>
nit: old is not used

<span class=q>&gt; &gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; &gt; +
&gt; &gt; +	WARN_ON(!stage2_pte_is_locked(*ptep));
&gt; &gt; +
&gt; &gt; +	if (stage2_pte_is_counted(new))
&gt; &gt; +		mm_ops-&gt;get_page(ptep);
&gt; &gt; +
&gt; &gt; +	smp_store_release(ptep, new);
&gt; &gt; +}
&gt; &gt; +
&gt; &gt;  static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_s2_mmu *mmu, u64 addr,
&gt; &gt;  			   u32 level, struct kvm_pgtable_mm_ops *mm_ops)
&gt; &gt;  {
&gt; &gt; @@ -836,17 +912,18 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; &gt;  	if (!childp)
&gt; &gt;  		return -ENOMEM;
&gt; &gt;  
&gt; &gt; +	if (!stage2_try_break_pte(ptep, *old, addr, level, data)) {
&gt; &gt; +		mm_ops-&gt;put_page(childp);
&gt; &gt; +		return -EAGAIN;
&gt; &gt; +	}
&gt; &gt; +
&gt; &gt;  	/*
&gt; &gt;  	 * If we've run into an existing block mapping then replace it with
&gt; &gt;  	 * a table. Accesses beyond 'end' that fall within the new table
&gt; &gt;  	 * will be mapped lazily.
&gt; &gt;  	 */
&gt; &gt; -	if (stage2_pte_is_counted(pte))
&gt; &gt; -		stage2_put_pte(ptep, data-&gt;mmu, addr, level, mm_ops);
&gt; &gt; -
&gt; &gt;  	new = kvm_init_table_pte(childp, mm_ops);
&gt; &gt; -	mm_ops-&gt;get_page(ptep);
&gt; &gt; -	smp_store_release(ptep, new);
&gt; &gt; +	stage2_make_pte(ptep, *old, new, data);
&gt; &gt;  	*old = new;
&gt; &gt;  
&gt; &gt;  	return 0;
&gt; &gt; -- 
&gt; &gt; 2.37.2.672.g94769d06f0-goog
&gt; &gt; 
</span>
<a href=#m77ade85e72dafda18c138e2b7a41ffd92979d92e id=e77ade85e72dafda18c138e2b7a41ffd92979d92e>^</a> <a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/t/#u><b>nested</b></a>] <a href=#r77ade85e72dafda18c138e2b7a41ffd92979d92e>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e77ade85e72dafda18c138e2b7a41ffd92979d92e id=m77ade85e72dafda18c138e2b7a41ffd92979d92e>*</a> <b>Re: [PATCH 11/14] KVM: arm64: Make changes block-&gt;table to leaf PTEs parallel-aware</b>
<b>@ 2022-09-14  0:53       ` Ricardo Koller</b>
  <a href=#r77ade85e72dafda18c138e2b7a41ffd92979d92e>0 siblings, 0 replies; 96+ messages in thread</a>
From: Ricardo Koller @ 2022-09-14  0:53 UTC (<a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Catalin Marinas, Will Deacon, linux-arm-kernel, kvmarm, kvm,
	Quentin Perret, Reiji Watanabe, David Matlack, Ben Gardon,
	Paolo Bonzini, Gavin Shan, Peter Xu, Sean Christopherson,
	linux-kernel

On Tue, Sep 13, 2022 at 05:51:55PM -0700, Ricardo Koller wrote:
<span class=q>&gt; On Tue, Aug 30, 2022 at 07:51:01PM +0000, Oliver Upton wrote:
&gt; &gt; In order to service stage-2 faults in parallel, stage-2 table walkers
&gt; &gt; must take exclusive ownership of the PTE being worked on. An additional
&gt; &gt; requirement of the architecture is that software must perform a
&gt; &gt; 'break-before-make' operation when changing the block size used for
&gt; &gt; mapping memory.
&gt; &gt; 
&gt; &gt; Roll these two concepts together into helpers for performing a
&gt; &gt; 'break-before-make' sequence. Use a special PTE value to indicate a PTE
&gt; &gt; has been locked by a software walker. Additionally, use an atomic
&gt; &gt; compare-exchange to 'break' the PTE when the stage-2 page tables are
&gt; &gt; possibly shared with another software walker. Elide the DSB + TLBI if
&gt; &gt; the evicted PTE was invalid (and thus not subject to break-before-make).
&gt; &gt; 
&gt; &gt; All of the atomics do nothing for now, as the stage-2 walker isn't fully
&gt; &gt; ready to perform parallel walks.
&gt; &gt; 
&gt; &gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; &gt; ---
&gt; &gt;  arch/arm64/kvm/hyp/pgtable.c | 87 +++++++++++++++++++++++++++++++++---
&gt; &gt;  1 file changed, 82 insertions(+), 5 deletions(-)
&gt; &gt; 
&gt; &gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; index 61a4437c8c16..71ae96608752 100644
&gt; &gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; @@ -49,6 +49,12 @@
&gt; &gt;  #define KVM_INVALID_PTE_OWNER_MASK	GENMASK(9, 2)
&gt; &gt;  #define KVM_MAX_OWNER_ID		1
&gt; &gt;  
&gt; &gt; +/*
&gt; &gt; + * Used to indicate a pte for which a 'break-before-make' sequence is in
&gt; &gt; + * progress.
&gt; &gt; + */
&gt; &gt; +#define KVM_INVALID_PTE_LOCKED		BIT(10)
&gt; &gt; +
&gt; &gt;  struct kvm_pgtable_walk_data {
&gt; &gt;  	struct kvm_pgtable		*pgt;
&gt; &gt;  	struct kvm_pgtable_walker	*walker;
&gt; &gt; @@ -586,6 +592,8 @@ struct stage2_map_data {
&gt; &gt;  
&gt; &gt;  	/* Force mappings to page granularity */
&gt; &gt;  	bool				force_pte;
&gt; &gt; +
&gt; &gt; +	bool				shared;
&gt; &gt;  };
&gt; &gt;  
&gt; &gt;  u64 kvm_get_vtcr(u64 mmfr0, u64 mmfr1, u32 phys_shift)
&gt; &gt; @@ -691,6 +699,11 @@ static bool stage2_pte_is_counted(kvm_pte_t pte)
&gt; &gt;  	return kvm_pte_valid(pte) || kvm_invalid_pte_owner(pte);
&gt; &gt;  }
&gt; &gt;  
&gt; &gt; +static bool stage2_pte_is_locked(kvm_pte_t pte)
&gt; &gt; +{
&gt; &gt; +	return !kvm_pte_valid(pte) &amp;&amp; (pte &amp; KVM_INVALID_PTE_LOCKED);
&gt; &gt; +}
&gt; &gt; +
&gt; &gt;  static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bool shared)
&gt; &gt;  {
&gt; &gt;  	if (!shared) {
&gt; &gt; @@ -701,6 +714,69 @@ static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bo
&gt; &gt;  	return cmpxchg(ptep, old, new) == old;
&gt; &gt;  }
&gt; &gt;  
&gt; &gt; +/**
&gt; &gt; + * stage2_try_break_pte() - Invalidates a pte according to the
&gt; &gt; + *			    'break-before-make' requirements of the
&gt; &gt; + *			    architecture.
&gt; &gt; + *
&gt; &gt; + * @ptep: Pointer to the pte to break
&gt; &gt; + * @old: The previously observed value of the pte
&gt; &gt; + * @addr: IPA corresponding to the pte
&gt; &gt; + * @level: Table level of the pte
&gt; &gt; + * @shared: true if the stage-2 page tables could be shared by multiple software
&gt; &gt; + *	    walkers
&gt; &gt; + *
&gt; &gt; + * Returns: true if the pte was successfully broken.
&gt; &gt; + *
&gt; &gt; + * If the removed pte was valid, performs the necessary serialization and TLB
&gt; &gt; + * invalidation for the old value. For counted ptes, drops the reference count
&gt; &gt; + * on the containing table page.
&gt; &gt; + */
&gt; &gt; +static bool stage2_try_break_pte(kvm_pte_t *ptep, kvm_pte_t old, u64 addr, u32 level,
&gt; &gt; +				 struct stage2_map_data *data)
&gt; &gt; +{
&gt; &gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; &gt; +
&gt; &gt; +	if (stage2_pte_is_locked(old)) {
&gt; &gt; +		/*
&gt; &gt; +		 * Should never occur if this walker has exclusive access to the
&gt; &gt; +		 * page tables.
&gt; &gt; +		 */
&gt; &gt; +		WARN_ON(!data-&gt;shared);
&gt; &gt; +		return false;
&gt; &gt; +	}
&gt; 
&gt; The above check is not needed as the cmpxchg() will return false if the
&gt; old pte is equal to "new" (KVM_INVALID_PTE_LOCKED).
&gt; 
&gt; &gt; +
&gt; &gt; +	if (!stage2_try_set_pte(ptep, old, KVM_INVALID_PTE_LOCKED, data-&gt;shared))
&gt; &gt; +		return false;
&gt; &gt; +
&gt; &gt; +	/*
&gt; &gt; +	 * Perform the appropriate TLB invalidation based on the evicted pte
&gt; &gt; +	 * value (if any).
&gt; &gt; +	 */
&gt; &gt; +	if (kvm_pte_table(old, level))
&gt; &gt; +		kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt; &gt; +	else if (kvm_pte_valid(old))
&gt; &gt; +		kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, data-&gt;mmu, addr, level);
&gt; &gt; +
&gt; &gt; +	if (stage2_pte_is_counted(old))
&gt; &gt; +		mm_ops-&gt;put_page(ptep);
&gt; &gt; +
&gt; &gt; +	return true;
&gt; &gt; +}
&gt; &gt; +
&gt; &gt; +static void stage2_make_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new,
&gt; &gt; +			    struct stage2_map_data *data)
&gt; &gt; +{
</span>
nit: old is not used

<span class=q>&gt; &gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; &gt; +
&gt; &gt; +	WARN_ON(!stage2_pte_is_locked(*ptep));
&gt; &gt; +
&gt; &gt; +	if (stage2_pte_is_counted(new))
&gt; &gt; +		mm_ops-&gt;get_page(ptep);
&gt; &gt; +
&gt; &gt; +	smp_store_release(ptep, new);
&gt; &gt; +}
&gt; &gt; +
&gt; &gt;  static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_s2_mmu *mmu, u64 addr,
&gt; &gt;  			   u32 level, struct kvm_pgtable_mm_ops *mm_ops)
&gt; &gt;  {
&gt; &gt; @@ -836,17 +912,18 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; &gt;  	if (!childp)
&gt; &gt;  		return -ENOMEM;
&gt; &gt;  
&gt; &gt; +	if (!stage2_try_break_pte(ptep, *old, addr, level, data)) {
&gt; &gt; +		mm_ops-&gt;put_page(childp);
&gt; &gt; +		return -EAGAIN;
&gt; &gt; +	}
&gt; &gt; +
&gt; &gt;  	/*
&gt; &gt;  	 * If we've run into an existing block mapping then replace it with
&gt; &gt;  	 * a table. Accesses beyond 'end' that fall within the new table
&gt; &gt;  	 * will be mapped lazily.
&gt; &gt;  	 */
&gt; &gt; -	if (stage2_pte_is_counted(pte))
&gt; &gt; -		stage2_put_pte(ptep, data-&gt;mmu, addr, level, mm_ops);
&gt; &gt; -
&gt; &gt;  	new = kvm_init_table_pte(childp, mm_ops);
&gt; &gt; -	mm_ops-&gt;get_page(ptep);
&gt; &gt; -	smp_store_release(ptep, new);
&gt; &gt; +	stage2_make_pte(ptep, *old, new, data);
&gt; &gt;  	*old = new;
&gt; &gt;  
&gt; &gt;  	return 0;
&gt; &gt; -- 
&gt; &gt; 2.37.2.672.g94769d06f0-goog
&gt; &gt; 
</span>
_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m77ade85e72dafda18c138e2b7a41ffd92979d92e id=e77ade85e72dafda18c138e2b7a41ffd92979d92e>^</a> <a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/t/#u><b>nested</b></a>] <a href=#r77ade85e72dafda18c138e2b7a41ffd92979d92e>96+ messages in thread</a></pre><li><pre><a href=#e77ade85e72dafda18c138e2b7a41ffd92979d92e id=m77ade85e72dafda18c138e2b7a41ffd92979d92e>*</a> <b>Re: [PATCH 11/14] KVM: arm64: Make changes block-&gt;table to leaf PTEs parallel-aware</b>
<b>@ 2022-09-14  0:53       ` Ricardo Koller</b>
  <a href=#r77ade85e72dafda18c138e2b7a41ffd92979d92e>0 siblings, 0 replies; 96+ messages in thread</a>
From: Ricardo Koller @ 2022-09-14  0:53 UTC (<a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/raw>raw</a>)
  To: Oliver Upton
  Cc: kvm, Marc Zyngier, linux-kernel, Ben Gardon, Catalin Marinas,
	David Matlack, Paolo Bonzini, Will Deacon, kvmarm,
	linux-arm-kernel

On Tue, Sep 13, 2022 at 05:51:55PM -0700, Ricardo Koller wrote:
<span class=q>&gt; On Tue, Aug 30, 2022 at 07:51:01PM +0000, Oliver Upton wrote:
&gt; &gt; In order to service stage-2 faults in parallel, stage-2 table walkers
&gt; &gt; must take exclusive ownership of the PTE being worked on. An additional
&gt; &gt; requirement of the architecture is that software must perform a
&gt; &gt; 'break-before-make' operation when changing the block size used for
&gt; &gt; mapping memory.
&gt; &gt; 
&gt; &gt; Roll these two concepts together into helpers for performing a
&gt; &gt; 'break-before-make' sequence. Use a special PTE value to indicate a PTE
&gt; &gt; has been locked by a software walker. Additionally, use an atomic
&gt; &gt; compare-exchange to 'break' the PTE when the stage-2 page tables are
&gt; &gt; possibly shared with another software walker. Elide the DSB + TLBI if
&gt; &gt; the evicted PTE was invalid (and thus not subject to break-before-make).
&gt; &gt; 
&gt; &gt; All of the atomics do nothing for now, as the stage-2 walker isn't fully
&gt; &gt; ready to perform parallel walks.
&gt; &gt; 
&gt; &gt; Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
&gt; &gt; ---
&gt; &gt;  arch/arm64/kvm/hyp/pgtable.c | 87 +++++++++++++++++++++++++++++++++---
&gt; &gt;  1 file changed, 82 insertions(+), 5 deletions(-)
&gt; &gt; 
&gt; &gt; diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; index 61a4437c8c16..71ae96608752 100644
&gt; &gt; --- a/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; +++ b/arch/arm64/kvm/hyp/pgtable.c
&gt; &gt; @@ -49,6 +49,12 @@
&gt; &gt;  #define KVM_INVALID_PTE_OWNER_MASK	GENMASK(9, 2)
&gt; &gt;  #define KVM_MAX_OWNER_ID		1
&gt; &gt;  
&gt; &gt; +/*
&gt; &gt; + * Used to indicate a pte for which a 'break-before-make' sequence is in
&gt; &gt; + * progress.
&gt; &gt; + */
&gt; &gt; +#define KVM_INVALID_PTE_LOCKED		BIT(10)
&gt; &gt; +
&gt; &gt;  struct kvm_pgtable_walk_data {
&gt; &gt;  	struct kvm_pgtable		*pgt;
&gt; &gt;  	struct kvm_pgtable_walker	*walker;
&gt; &gt; @@ -586,6 +592,8 @@ struct stage2_map_data {
&gt; &gt;  
&gt; &gt;  	/* Force mappings to page granularity */
&gt; &gt;  	bool				force_pte;
&gt; &gt; +
&gt; &gt; +	bool				shared;
&gt; &gt;  };
&gt; &gt;  
&gt; &gt;  u64 kvm_get_vtcr(u64 mmfr0, u64 mmfr1, u32 phys_shift)
&gt; &gt; @@ -691,6 +699,11 @@ static bool stage2_pte_is_counted(kvm_pte_t pte)
&gt; &gt;  	return kvm_pte_valid(pte) || kvm_invalid_pte_owner(pte);
&gt; &gt;  }
&gt; &gt;  
&gt; &gt; +static bool stage2_pte_is_locked(kvm_pte_t pte)
&gt; &gt; +{
&gt; &gt; +	return !kvm_pte_valid(pte) &amp;&amp; (pte &amp; KVM_INVALID_PTE_LOCKED);
&gt; &gt; +}
&gt; &gt; +
&gt; &gt;  static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bool shared)
&gt; &gt;  {
&gt; &gt;  	if (!shared) {
&gt; &gt; @@ -701,6 +714,69 @@ static bool stage2_try_set_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new, bo
&gt; &gt;  	return cmpxchg(ptep, old, new) == old;
&gt; &gt;  }
&gt; &gt;  
&gt; &gt; +/**
&gt; &gt; + * stage2_try_break_pte() - Invalidates a pte according to the
&gt; &gt; + *			    'break-before-make' requirements of the
&gt; &gt; + *			    architecture.
&gt; &gt; + *
&gt; &gt; + * @ptep: Pointer to the pte to break
&gt; &gt; + * @old: The previously observed value of the pte
&gt; &gt; + * @addr: IPA corresponding to the pte
&gt; &gt; + * @level: Table level of the pte
&gt; &gt; + * @shared: true if the stage-2 page tables could be shared by multiple software
&gt; &gt; + *	    walkers
&gt; &gt; + *
&gt; &gt; + * Returns: true if the pte was successfully broken.
&gt; &gt; + *
&gt; &gt; + * If the removed pte was valid, performs the necessary serialization and TLB
&gt; &gt; + * invalidation for the old value. For counted ptes, drops the reference count
&gt; &gt; + * on the containing table page.
&gt; &gt; + */
&gt; &gt; +static bool stage2_try_break_pte(kvm_pte_t *ptep, kvm_pte_t old, u64 addr, u32 level,
&gt; &gt; +				 struct stage2_map_data *data)
&gt; &gt; +{
&gt; &gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; &gt; +
&gt; &gt; +	if (stage2_pte_is_locked(old)) {
&gt; &gt; +		/*
&gt; &gt; +		 * Should never occur if this walker has exclusive access to the
&gt; &gt; +		 * page tables.
&gt; &gt; +		 */
&gt; &gt; +		WARN_ON(!data-&gt;shared);
&gt; &gt; +		return false;
&gt; &gt; +	}
&gt; 
&gt; The above check is not needed as the cmpxchg() will return false if the
&gt; old pte is equal to "new" (KVM_INVALID_PTE_LOCKED).
&gt; 
&gt; &gt; +
&gt; &gt; +	if (!stage2_try_set_pte(ptep, old, KVM_INVALID_PTE_LOCKED, data-&gt;shared))
&gt; &gt; +		return false;
&gt; &gt; +
&gt; &gt; +	/*
&gt; &gt; +	 * Perform the appropriate TLB invalidation based on the evicted pte
&gt; &gt; +	 * value (if any).
&gt; &gt; +	 */
&gt; &gt; +	if (kvm_pte_table(old, level))
&gt; &gt; +		kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
&gt; &gt; +	else if (kvm_pte_valid(old))
&gt; &gt; +		kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, data-&gt;mmu, addr, level);
&gt; &gt; +
&gt; &gt; +	if (stage2_pte_is_counted(old))
&gt; &gt; +		mm_ops-&gt;put_page(ptep);
&gt; &gt; +
&gt; &gt; +	return true;
&gt; &gt; +}
&gt; &gt; +
&gt; &gt; +static void stage2_make_pte(kvm_pte_t *ptep, kvm_pte_t old, kvm_pte_t new,
&gt; &gt; +			    struct stage2_map_data *data)
&gt; &gt; +{
</span>
nit: old is not used

<span class=q>&gt; &gt; +	struct kvm_pgtable_mm_ops *mm_ops = data-&gt;mm_ops;
&gt; &gt; +
&gt; &gt; +	WARN_ON(!stage2_pte_is_locked(*ptep));
&gt; &gt; +
&gt; &gt; +	if (stage2_pte_is_counted(new))
&gt; &gt; +		mm_ops-&gt;get_page(ptep);
&gt; &gt; +
&gt; &gt; +	smp_store_release(ptep, new);
&gt; &gt; +}
&gt; &gt; +
&gt; &gt;  static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_s2_mmu *mmu, u64 addr,
&gt; &gt;  			   u32 level, struct kvm_pgtable_mm_ops *mm_ops)
&gt; &gt;  {
&gt; &gt; @@ -836,17 +912,18 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
&gt; &gt;  	if (!childp)
&gt; &gt;  		return -ENOMEM;
&gt; &gt;  
&gt; &gt; +	if (!stage2_try_break_pte(ptep, *old, addr, level, data)) {
&gt; &gt; +		mm_ops-&gt;put_page(childp);
&gt; &gt; +		return -EAGAIN;
&gt; &gt; +	}
&gt; &gt; +
&gt; &gt;  	/*
&gt; &gt;  	 * If we've run into an existing block mapping then replace it with
&gt; &gt;  	 * a table. Accesses beyond 'end' that fall within the new table
&gt; &gt;  	 * will be mapped lazily.
&gt; &gt;  	 */
&gt; &gt; -	if (stage2_pte_is_counted(pte))
&gt; &gt; -		stage2_put_pte(ptep, data-&gt;mmu, addr, level, mm_ops);
&gt; &gt; -
&gt; &gt;  	new = kvm_init_table_pte(childp, mm_ops);
&gt; &gt; -	mm_ops-&gt;get_page(ptep);
&gt; &gt; -	smp_store_release(ptep, new);
&gt; &gt; +	stage2_make_pte(ptep, *old, new, data);
&gt; &gt;  	*old = new;
&gt; &gt;  
&gt; &gt;  	return 0;
&gt; &gt; -- 
&gt; &gt; 2.37.2.672.g94769d06f0-goog
&gt; &gt; 
</span>_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m77ade85e72dafda18c138e2b7a41ffd92979d92e id=e77ade85e72dafda18c138e2b7a41ffd92979d92e>^</a> <a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YyEl%2FUILu+OAP5zA@google.com/t/#u><b>nested</b></a>] <a href=#r77ade85e72dafda18c138e2b7a41ffd92979d92e>96+ messages in thread</a></pre></ul></ul></ul><li><pre><a href=#e2631087360442636680e38cc623ed5c80244ee14 id=m2631087360442636680e38cc623ed5c80244ee14>*</a> <b>[PATCH 12/14] KVM: arm64: Make leaf-&gt;leaf PTE changes parallel-aware</b>
  2022-08-30 19:41 ` <a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb>Oliver Upton</a>
  (?)
<b>@ 2022-08-30 19:51   ` Oliver Upton</b>
  <a href=#r2631087360442636680e38cc623ed5c80244ee14>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:51 UTC (<a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Oliver Upton, Catalin Marinas, Will Deacon
  Cc: linux-arm-kernel, kvmarm, kvm, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, linux-kernel

Convert stage2_map_walker_try_leaf() to use the new break-before-make
helpers, thereby making the handler parallel-aware. As before, avoid the
break-before-make if recreating the existing mapping. Additionally,
retry execution if another vCPU thread is modifying the same PTE.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830195132.964800-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830195132.964800-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 26 ++++++++++++--------------
 1 file <a href=#e2631087360442636680e38cc623ed5c80244ee14>changed</a>, 12 insertions(+), 14 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830195132.964800-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830195132.964800-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 71ae96608752..de1d352657d0 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -829,18 +829,17 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
</span> 	else
 		new = kvm_init_invalid_leaf_owner(data-&gt;owner_id);
 
<span class=del>-	if (stage2_pte_is_counted(old)) {
-		/*
-		 * Skip updating the PTE if we are trying to recreate the exact
-		 * same mapping or only change the access permissions. Instead,
-		 * the vCPU will exit one more time from guest if still needed
-		 * and then go through the path of relaxing permissions.
-		 */
-		if (!stage2_pte_needs_update(old, new))
-			return -EAGAIN;
</span><span class=add>+	/*
+	 * Skip updating the PTE if we are trying to recreate the exact
+	 * same mapping or only change the access permissions. Instead,
+	 * the vCPU will exit one more time from guest if still needed
+	 * and then go through the path of relaxing permissions.
+	 */
+	if (!stage2_pte_needs_update(old, new))
+		return -EAGAIN;
</span> 
<span class=del>-		stage2_put_pte(ptep, data-&gt;mmu, addr, level, mm_ops);
-	}
</span><span class=add>+	if (!stage2_try_break_pte(ptep, old, addr, level, data))
+		return -EAGAIN;
</span> 
 	/* Perform CMOs before installation of the guest stage-2 PTE */
 	if (mm_ops-&gt;dcache_clean_inval_poc &amp;&amp; stage2_pte_cacheable(pgt, new))
<span class=hunk>@@ -850,9 +849,8 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
</span> 	if (mm_ops-&gt;icache_inval_pou &amp;&amp; stage2_pte_executable(new))
 		mm_ops-&gt;icache_inval_pou(kvm_pte_follow(new, mm_ops), granule);
 
<span class=del>-	smp_store_release(ptep, new);
-	if (stage2_pte_is_counted(new))
-		mm_ops-&gt;get_page(ptep);
</span><span class=add>+	stage2_make_pte(ptep, old, new, data);
+
</span> 	if (kvm_phys_is_valid(phys))
 		data-&gt;phys += granule;
 	return 0;
-- 
2.37.2.672.g94769d06f0-goog


<a href=#m2631087360442636680e38cc623ed5c80244ee14 id=e2631087360442636680e38cc623ed5c80244ee14>^</a> <a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r2631087360442636680e38cc623ed5c80244ee14>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e2631087360442636680e38cc623ed5c80244ee14 id=m2631087360442636680e38cc623ed5c80244ee14>*</a> <b>[PATCH 12/14] KVM: arm64: Make leaf-&gt;leaf PTE changes parallel-aware</b>
<b>@ 2022-08-30 19:51   ` Oliver Upton</b>
  <a href=#r2631087360442636680e38cc623ed5c80244ee14>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:51 UTC (<a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Oliver Upton, Catalin Marinas, Will Deacon
  Cc: linux-arm-kernel, kvmarm, kvm, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, linux-kernel

Convert stage2_map_walker_try_leaf() to use the new break-before-make
helpers, thereby making the handler parallel-aware. As before, avoid the
break-before-make if recreating the existing mapping. Additionally,
retry execution if another vCPU thread is modifying the same PTE.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830195132.964800-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830195132.964800-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 26 ++++++++++++--------------
 1 file <a href=#e2631087360442636680e38cc623ed5c80244ee14>changed</a>, 12 insertions(+), 14 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830195132.964800-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830195132.964800-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 71ae96608752..de1d352657d0 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -829,18 +829,17 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
</span> 	else
 		new = kvm_init_invalid_leaf_owner(data-&gt;owner_id);
 
<span class=del>-	if (stage2_pte_is_counted(old)) {
-		/*
-		 * Skip updating the PTE if we are trying to recreate the exact
-		 * same mapping or only change the access permissions. Instead,
-		 * the vCPU will exit one more time from guest if still needed
-		 * and then go through the path of relaxing permissions.
-		 */
-		if (!stage2_pte_needs_update(old, new))
-			return -EAGAIN;
</span><span class=add>+	/*
+	 * Skip updating the PTE if we are trying to recreate the exact
+	 * same mapping or only change the access permissions. Instead,
+	 * the vCPU will exit one more time from guest if still needed
+	 * and then go through the path of relaxing permissions.
+	 */
+	if (!stage2_pte_needs_update(old, new))
+		return -EAGAIN;
</span> 
<span class=del>-		stage2_put_pte(ptep, data-&gt;mmu, addr, level, mm_ops);
-	}
</span><span class=add>+	if (!stage2_try_break_pte(ptep, old, addr, level, data))
+		return -EAGAIN;
</span> 
 	/* Perform CMOs before installation of the guest stage-2 PTE */
 	if (mm_ops-&gt;dcache_clean_inval_poc &amp;&amp; stage2_pte_cacheable(pgt, new))
<span class=hunk>@@ -850,9 +849,8 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
</span> 	if (mm_ops-&gt;icache_inval_pou &amp;&amp; stage2_pte_executable(new))
 		mm_ops-&gt;icache_inval_pou(kvm_pte_follow(new, mm_ops), granule);
 
<span class=del>-	smp_store_release(ptep, new);
-	if (stage2_pte_is_counted(new))
-		mm_ops-&gt;get_page(ptep);
</span><span class=add>+	stage2_make_pte(ptep, old, new, data);
+
</span> 	if (kvm_phys_is_valid(phys))
 		data-&gt;phys += granule;
 	return 0;
-- 
2.37.2.672.g94769d06f0-goog


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m2631087360442636680e38cc623ed5c80244ee14 id=e2631087360442636680e38cc623ed5c80244ee14>^</a> <a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r2631087360442636680e38cc623ed5c80244ee14>96+ messages in thread</a></pre><li><pre><a href=#e2631087360442636680e38cc623ed5c80244ee14 id=m2631087360442636680e38cc623ed5c80244ee14>*</a> <b>[PATCH 12/14] KVM: arm64: Make leaf-&gt;leaf PTE changes parallel-aware</b>
<b>@ 2022-08-30 19:51   ` Oliver Upton</b>
  <a href=#r2631087360442636680e38cc623ed5c80244ee14>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:51 UTC (<a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Oliver Upton, Catalin Marinas, Will Deacon
  Cc: kvm, linux-kernel, Ben Gardon, David Matlack, Paolo Bonzini,
	kvmarm, linux-arm-kernel

Convert stage2_map_walker_try_leaf() to use the new break-before-make
helpers, thereby making the handler parallel-aware. As before, avoid the
break-before-make if recreating the existing mapping. Additionally,
retry execution if another vCPU thread is modifying the same PTE.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830195132.964800-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830195132.964800-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 26 ++++++++++++--------------
 1 file <a href=#e2631087360442636680e38cc623ed5c80244ee14>changed</a>, 12 insertions(+), 14 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830195132.964800-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830195132.964800-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 71ae96608752..de1d352657d0 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -829,18 +829,17 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
</span> 	else
 		new = kvm_init_invalid_leaf_owner(data-&gt;owner_id);
 
<span class=del>-	if (stage2_pte_is_counted(old)) {
-		/*
-		 * Skip updating the PTE if we are trying to recreate the exact
-		 * same mapping or only change the access permissions. Instead,
-		 * the vCPU will exit one more time from guest if still needed
-		 * and then go through the path of relaxing permissions.
-		 */
-		if (!stage2_pte_needs_update(old, new))
-			return -EAGAIN;
</span><span class=add>+	/*
+	 * Skip updating the PTE if we are trying to recreate the exact
+	 * same mapping or only change the access permissions. Instead,
+	 * the vCPU will exit one more time from guest if still needed
+	 * and then go through the path of relaxing permissions.
+	 */
+	if (!stage2_pte_needs_update(old, new))
+		return -EAGAIN;
</span> 
<span class=del>-		stage2_put_pte(ptep, data-&gt;mmu, addr, level, mm_ops);
-	}
</span><span class=add>+	if (!stage2_try_break_pte(ptep, old, addr, level, data))
+		return -EAGAIN;
</span> 
 	/* Perform CMOs before installation of the guest stage-2 PTE */
 	if (mm_ops-&gt;dcache_clean_inval_poc &amp;&amp; stage2_pte_cacheable(pgt, new))
<span class=hunk>@@ -850,9 +849,8 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
</span> 	if (mm_ops-&gt;icache_inval_pou &amp;&amp; stage2_pte_executable(new))
 		mm_ops-&gt;icache_inval_pou(kvm_pte_follow(new, mm_ops), granule);
 
<span class=del>-	smp_store_release(ptep, new);
-	if (stage2_pte_is_counted(new))
-		mm_ops-&gt;get_page(ptep);
</span><span class=add>+	stage2_make_pte(ptep, old, new, data);
+
</span> 	if (kvm_phys_is_valid(phys))
 		data-&gt;phys += granule;
 	return 0;
-- 
2.37.2.672.g94769d06f0-goog

_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m2631087360442636680e38cc623ed5c80244ee14 id=e2631087360442636680e38cc623ed5c80244ee14>^</a> <a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830195132.964800-1-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r2631087360442636680e38cc623ed5c80244ee14>96+ messages in thread</a></pre></ul><li><pre><a href=#e430c5439f74caa60814b5ef2834d4cf4db1c2bdf id=m430c5439f74caa60814b5ef2834d4cf4db1c2bdf>*</a> <b>[PATCH 13/14] KVM: arm64: Make table-&gt;block changes parallel-aware</b>
  2022-08-30 19:41 ` <a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb>Oliver Upton</a>
  (?)
<b>@ 2022-08-30 19:51   ` Oliver Upton</b>
  <a href=#r430c5439f74caa60814b5ef2834d4cf4db1c2bdf>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:51 UTC (<a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Oliver Upton, Catalin Marinas, Will Deacon
  Cc: linux-arm-kernel, kvmarm, kvm, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, linux-kernel

stage2_map_walk_leaf() and friends now handle stage-2 PTEs generically,
and perform the correct flush when a table PTE is removed. Additionally,
they've been made parallel-aware, using an atomic break to take
ownership of the PTE.

Stop clearing the PTE in the pre-order callback and instead let
stage2_map_walk_leaf() deal with it.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830195151.964912-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830195151.964912-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 15 +++------------
 1 file <a href=#e430c5439f74caa60814b5ef2834d4cf4db1c2bdf>changed</a>, 3 insertions(+), 12 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830195151.964912-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830195151.964912-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index de1d352657d0..92e230e7bf3a 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -871,21 +871,12 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
</span> 	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
 		return 0;
 
<span class=del>-	kvm_clear_pte(ptep);
-
-	/*
-	 * Invalidate the whole stage-2, as we may have numerous leaf
-	 * entries below us which would otherwise need invalidating
-	 * individually.
-	 */
-	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
-
</span> 	ret = stage2_map_walk_leaf(addr, end, level, ptep, old, data);
<span class=add>+	if (ret)
+		return ret;
</span> 
<span class=del>-	mm_ops-&gt;put_page(ptep);
</span> 	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
<span class=del>-
-	return ret;
</span><span class=add>+	return 0;
</span> }
 
 static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
-- 
2.37.2.672.g94769d06f0-goog


<a href=#m430c5439f74caa60814b5ef2834d4cf4db1c2bdf id=e430c5439f74caa60814b5ef2834d4cf4db1c2bdf>^</a> <a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r430c5439f74caa60814b5ef2834d4cf4db1c2bdf>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e430c5439f74caa60814b5ef2834d4cf4db1c2bdf id=m430c5439f74caa60814b5ef2834d4cf4db1c2bdf>*</a> <b>[PATCH 13/14] KVM: arm64: Make table-&gt;block changes parallel-aware</b>
<b>@ 2022-08-30 19:51   ` Oliver Upton</b>
  <a href=#r430c5439f74caa60814b5ef2834d4cf4db1c2bdf>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:51 UTC (<a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Oliver Upton, Catalin Marinas, Will Deacon
  Cc: linux-arm-kernel, kvmarm, kvm, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, linux-kernel

stage2_map_walk_leaf() and friends now handle stage-2 PTEs generically,
and perform the correct flush when a table PTE is removed. Additionally,
they've been made parallel-aware, using an atomic break to take
ownership of the PTE.

Stop clearing the PTE in the pre-order callback and instead let
stage2_map_walk_leaf() deal with it.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830195151.964912-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830195151.964912-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 15 +++------------
 1 file <a href=#e430c5439f74caa60814b5ef2834d4cf4db1c2bdf>changed</a>, 3 insertions(+), 12 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830195151.964912-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830195151.964912-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index de1d352657d0..92e230e7bf3a 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -871,21 +871,12 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
</span> 	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
 		return 0;
 
<span class=del>-	kvm_clear_pte(ptep);
-
-	/*
-	 * Invalidate the whole stage-2, as we may have numerous leaf
-	 * entries below us which would otherwise need invalidating
-	 * individually.
-	 */
-	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
-
</span> 	ret = stage2_map_walk_leaf(addr, end, level, ptep, old, data);
<span class=add>+	if (ret)
+		return ret;
</span> 
<span class=del>-	mm_ops-&gt;put_page(ptep);
</span> 	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
<span class=del>-
-	return ret;
</span><span class=add>+	return 0;
</span> }
 
 static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
-- 
2.37.2.672.g94769d06f0-goog


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m430c5439f74caa60814b5ef2834d4cf4db1c2bdf id=e430c5439f74caa60814b5ef2834d4cf4db1c2bdf>^</a> <a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r430c5439f74caa60814b5ef2834d4cf4db1c2bdf>96+ messages in thread</a></pre><li><pre><a href=#e430c5439f74caa60814b5ef2834d4cf4db1c2bdf id=m430c5439f74caa60814b5ef2834d4cf4db1c2bdf>*</a> <b>[PATCH 13/14] KVM: arm64: Make table-&gt;block changes parallel-aware</b>
<b>@ 2022-08-30 19:51   ` Oliver Upton</b>
  <a href=#r430c5439f74caa60814b5ef2834d4cf4db1c2bdf>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:51 UTC (<a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Oliver Upton, Catalin Marinas, Will Deacon
  Cc: kvm, linux-kernel, Ben Gardon, David Matlack, Paolo Bonzini,
	kvmarm, linux-arm-kernel

stage2_map_walk_leaf() and friends now handle stage-2 PTEs generically,
and perform the correct flush when a table PTE is removed. Additionally,
they've been made parallel-aware, using an atomic break to take
ownership of the PTE.

Stop clearing the PTE in the pre-order callback and instead let
stage2_map_walk_leaf() deal with it.

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830195151.964912-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830195151.964912-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a> | 15 +++------------
 1 file <a href=#e430c5439f74caa60814b5ef2834d4cf4db1c2bdf>changed</a>, 3 insertions(+), 12 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830195151.964912-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830195151.964912-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index de1d352657d0..92e230e7bf3a 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -871,21 +871,12 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
</span> 	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
 		return 0;
 
<span class=del>-	kvm_clear_pte(ptep);
-
-	/*
-	 * Invalidate the whole stage-2, as we may have numerous leaf
-	 * entries below us which would otherwise need invalidating
-	 * individually.
-	 */
-	kvm_call_hyp(__kvm_tlb_flush_vmid, data-&gt;mmu);
-
</span> 	ret = stage2_map_walk_leaf(addr, end, level, ptep, old, data);
<span class=add>+	if (ret)
+		return ret;
</span> 
<span class=del>-	mm_ops-&gt;put_page(ptep);
</span> 	mm_ops-&gt;free_removed_table(childp, level + 1, pgt);
<span class=del>-
-	return ret;
</span><span class=add>+	return 0;
</span> }
 
 static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
-- 
2.37.2.672.g94769d06f0-goog

_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m430c5439f74caa60814b5ef2834d4cf4db1c2bdf id=e430c5439f74caa60814b5ef2834d4cf4db1c2bdf>^</a> <a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830195151.964912-1-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r430c5439f74caa60814b5ef2834d4cf4db1c2bdf>96+ messages in thread</a></pre></ul><li><pre><a href=#e529155fbc90b4c51cb58a327e8a79fc4e46d349b id=m529155fbc90b4c51cb58a327e8a79fc4e46d349b>*</a> <b>[PATCH 14/14] KVM: arm64: Handle stage-2 faults in parallel</b>
  2022-08-30 19:41 ` <a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb>Oliver Upton</a>
  (?)
<b>@ 2022-08-30 19:52   ` Oliver Upton</b>
  <a href=#r529155fbc90b4c51cb58a327e8a79fc4e46d349b>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:52 UTC (<a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Oliver Upton, Catalin Marinas, Will Deacon
  Cc: linux-arm-kernel, kvmarm, kvm, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, linux-kernel

The stage-2 map walker has been made parallel-aware, and as such can be
called while only holding the read side of the MMU lock. Rip out the
conditional locking in user_mem_abort() and instead grab the read lock.
Continue to take the write lock from other callsites to
kvm_pgtable_stage2_map().

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h href=#Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>arch/arm64/include/asm/kvm_pgtable.h</a>  |  4 +++-
 <a id=iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c href=#Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c>arch/arm64/kvm/hyp/nvhe/mem_protect.c</a> |  2 +-
 <a id=iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a>          |  3 ++-
 <a id=iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c href=#Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c>arch/arm64/kvm/mmu.c</a>                  | 31 ++++++---------------------
 4 files <a href=#e529155fbc90b4c51cb58a327e8a79fc4e46d349b>changed</a>, 13 insertions(+), 27 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h id=Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>diff</a> --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index 7d2de0a98ccb..dc839db86a1a 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -355,6 +355,8 @@ void kvm_pgtable_stage2_free_removed(void *pgtable, u32 level, void *arg);
</span>  * @prot:	Permissions and attributes for the mapping.
  * @mc:		Cache of pre-allocated and zeroed memory from which to allocate
  *		page-table pages.
<span class=add>+ * @shared:	true if multiple software walkers could be traversing the tables
+ *		in parallel
</span>  *
  * The offset of @addr within a page is ignored, @size is rounded-up to
  * the next page boundary and @phys is rounded-down to the previous page
<span class=hunk>@@ -376,7 +378,7 @@ void kvm_pgtable_stage2_free_removed(void *pgtable, u32 level, void *arg);
</span>  */
 int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
 			   u64 phys, enum kvm_pgtable_prot prot,
<span class=del>-			   void *mc);
</span><span class=add>+			   void *mc, bool shared);
</span> 
 /**
  * kvm_pgtable_stage2_set_owner() - Unmap and annotate pages in the IPA space to
<span class=head><a href=#iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c id=Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c>diff</a> --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
index 61cf223e0796..924d028af447 100644
--- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c
+++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
</span><span class=hunk>@@ -252,7 +252,7 @@ static inline int __host_stage2_idmap(u64 start, u64 end,
</span> 				      enum kvm_pgtable_prot prot)
 {
 	return kvm_pgtable_stage2_map(&amp;host_kvm.pgt, start, end - start, start,
<span class=del>-				      prot, &amp;host_s2_pool);
</span><span class=add>+				      prot, &amp;host_s2_pool, false);
</span> }
 
 /*
<span class=head><a href=#iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 92e230e7bf3a..52ecaaa84b22 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -944,7 +944,7 @@ static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_
</span> 
 int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
 			   u64 phys, enum kvm_pgtable_prot prot,
<span class=del>-			   void *mc)
</span><span class=add>+			   void *mc, bool shared)
</span> {
 	int ret;
 	struct stage2_map_data map_data = {
<span class=hunk>@@ -953,6 +953,7 @@ int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
</span> 		.memcache	= mc,
 		.mm_ops		= pgt-&gt;mm_ops,
 		.force_pte	= pgt-&gt;force_pte_cb &amp;&amp; pgt-&gt;force_pte_cb(addr, addr + size, prot),
<span class=add>+		.shared		= shared,
</span> 	};
 	struct kvm_pgtable_walker walker = {
 		.cb		= stage2_map_walker,
<span class=head><a href=#iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c id=Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c>diff</a> --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index 265951c05879..a73adc35cf41 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
</span><span class=hunk>@@ -840,7 +840,7 @@ int kvm_phys_addr_ioremap(struct kvm *kvm, phys_addr_t guest_ipa,
</span> 
 		write_lock(&amp;kvm-&gt;mmu_lock);
 		ret = kvm_pgtable_stage2_map(pgt, addr, PAGE_SIZE, pa, prot,
<span class=del>-					     &amp;cache);
</span><span class=add>+					     &amp;cache, false);
</span> 		write_unlock(&amp;kvm-&gt;mmu_lock);
 		if (ret)
 			break;
<span class=hunk>@@ -1135,7 +1135,6 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
</span> 	gfn_t gfn;
 	kvm_pfn_t pfn;
 	bool logging_active = memslot_is_logging(memslot);
<span class=del>-	bool use_read_lock = false;
</span> 	unsigned long fault_level = kvm_vcpu_trap_get_fault_level(vcpu);
 	unsigned long vma_pagesize, fault_granule;
 	enum kvm_pgtable_prot prot = KVM_PGTABLE_PROT_R;
<span class=hunk>@@ -1170,8 +1169,6 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
</span> 	if (logging_active) {
 		force_pte = true;
 		vma_shift = PAGE_SHIFT;
<span class=del>-		use_read_lock = (fault_status == FSC_PERM &amp;&amp; write_fault &amp;&amp;
-				 fault_granule == PAGE_SIZE);
</span> 	} else {
 		vma_shift = get_vma_page_shift(vma, hva);
 	}
<span class=hunk>@@ -1270,15 +1267,7 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
</span> 	if (exec_fault &amp;&amp; device)
 		return -ENOEXEC;
 
<span class=del>-	/*
-	 * To reduce MMU contentions and enhance concurrency during dirty
-	 * logging dirty logging, only acquire read lock for permission
-	 * relaxation.
-	 */
-	if (use_read_lock)
-		read_lock(&amp;kvm-&gt;mmu_lock);
-	else
-		write_lock(&amp;kvm-&gt;mmu_lock);
</span><span class=add>+	read_lock(&amp;kvm-&gt;mmu_lock);
</span> 	pgt = vcpu-&gt;arch.hw_mmu-&gt;pgt;
 	if (mmu_invalidate_retry(kvm, mmu_seq))
 		goto out_unlock;
<span class=hunk>@@ -1322,15 +1311,12 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
</span> 	 * permissions only if vma_pagesize equals fault_granule. Otherwise,
 	 * kvm_pgtable_stage2_map() should be called to change block size.
 	 */
<span class=del>-	if (fault_status == FSC_PERM &amp;&amp; vma_pagesize == fault_granule) {
</span><span class=add>+	if (fault_status == FSC_PERM &amp;&amp; vma_pagesize == fault_granule)
</span> 		ret = kvm_pgtable_stage2_relax_perms(pgt, fault_ipa, prot);
<span class=del>-	} else {
-		WARN_ONCE(use_read_lock, "Attempted stage-2 map outside of write lock\n");
-
</span><span class=add>+	else
</span> 		ret = kvm_pgtable_stage2_map(pgt, fault_ipa, vma_pagesize,
 					     __pfn_to_phys(pfn), prot,
<span class=del>-					     memcache);
-	}
</span><span class=add>+					     memcache, true);
</span> 
 	/* Mark the page dirty only if the fault is handled successfully */
 	if (writable &amp;&amp; !ret) {
<span class=hunk>@@ -1339,10 +1325,7 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
</span> 	}
 
 out_unlock:
<span class=del>-	if (use_read_lock)
-		read_unlock(&amp;kvm-&gt;mmu_lock);
-	else
-		write_unlock(&amp;kvm-&gt;mmu_lock);
</span><span class=add>+	read_unlock(&amp;kvm-&gt;mmu_lock);
</span> 	kvm_set_pfn_accessed(pfn);
 	kvm_release_pfn_clean(pfn);
 	return ret != -EAGAIN ? ret : 0;
<span class=hunk>@@ -1548,7 +1531,7 @@ bool kvm_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
</span> 	 */
 	kvm_pgtable_stage2_map(kvm-&gt;arch.mmu.pgt, range-&gt;start &lt;&lt; PAGE_SHIFT,
 			       PAGE_SIZE, __pfn_to_phys(pfn),
<span class=del>-			       KVM_PGTABLE_PROT_R, NULL);
</span><span class=add>+			       KVM_PGTABLE_PROT_R, NULL, false);
</span> 
 	return false;
 }
-- 
2.37.2.672.g94769d06f0-goog


<a href=#m529155fbc90b4c51cb58a327e8a79fc4e46d349b id=e529155fbc90b4c51cb58a327e8a79fc4e46d349b>^</a> <a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r529155fbc90b4c51cb58a327e8a79fc4e46d349b>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e529155fbc90b4c51cb58a327e8a79fc4e46d349b id=m529155fbc90b4c51cb58a327e8a79fc4e46d349b>*</a> <b>[PATCH 14/14] KVM: arm64: Handle stage-2 faults in parallel</b>
<b>@ 2022-08-30 19:52   ` Oliver Upton</b>
  <a href=#r529155fbc90b4c51cb58a327e8a79fc4e46d349b>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:52 UTC (<a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Oliver Upton, Catalin Marinas, Will Deacon
  Cc: linux-arm-kernel, kvmarm, kvm, Quentin Perret, Ricardo Koller,
	Reiji Watanabe, David Matlack, Ben Gardon, Paolo Bonzini,
	Gavin Shan, Peter Xu, Sean Christopherson, linux-kernel

The stage-2 map walker has been made parallel-aware, and as such can be
called while only holding the read side of the MMU lock. Rip out the
conditional locking in user_mem_abort() and instead grab the read lock.
Continue to take the write lock from other callsites to
kvm_pgtable_stage2_map().

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h href=#Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>arch/arm64/include/asm/kvm_pgtable.h</a>  |  4 +++-
 <a id=iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c href=#Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c>arch/arm64/kvm/hyp/nvhe/mem_protect.c</a> |  2 +-
 <a id=iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a>          |  3 ++-
 <a id=iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c href=#Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c>arch/arm64/kvm/mmu.c</a>                  | 31 ++++++---------------------
 4 files <a href=#e529155fbc90b4c51cb58a327e8a79fc4e46d349b>changed</a>, 13 insertions(+), 27 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h id=Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>diff</a> --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index 7d2de0a98ccb..dc839db86a1a 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -355,6 +355,8 @@ void kvm_pgtable_stage2_free_removed(void *pgtable, u32 level, void *arg);
</span>  * @prot:	Permissions and attributes for the mapping.
  * @mc:		Cache of pre-allocated and zeroed memory from which to allocate
  *		page-table pages.
<span class=add>+ * @shared:	true if multiple software walkers could be traversing the tables
+ *		in parallel
</span>  *
  * The offset of @addr within a page is ignored, @size is rounded-up to
  * the next page boundary and @phys is rounded-down to the previous page
<span class=hunk>@@ -376,7 +378,7 @@ void kvm_pgtable_stage2_free_removed(void *pgtable, u32 level, void *arg);
</span>  */
 int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
 			   u64 phys, enum kvm_pgtable_prot prot,
<span class=del>-			   void *mc);
</span><span class=add>+			   void *mc, bool shared);
</span> 
 /**
  * kvm_pgtable_stage2_set_owner() - Unmap and annotate pages in the IPA space to
<span class=head><a href=#iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c id=Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c>diff</a> --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
index 61cf223e0796..924d028af447 100644
--- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c
+++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
</span><span class=hunk>@@ -252,7 +252,7 @@ static inline int __host_stage2_idmap(u64 start, u64 end,
</span> 				      enum kvm_pgtable_prot prot)
 {
 	return kvm_pgtable_stage2_map(&amp;host_kvm.pgt, start, end - start, start,
<span class=del>-				      prot, &amp;host_s2_pool);
</span><span class=add>+				      prot, &amp;host_s2_pool, false);
</span> }
 
 /*
<span class=head><a href=#iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 92e230e7bf3a..52ecaaa84b22 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -944,7 +944,7 @@ static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_
</span> 
 int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
 			   u64 phys, enum kvm_pgtable_prot prot,
<span class=del>-			   void *mc)
</span><span class=add>+			   void *mc, bool shared)
</span> {
 	int ret;
 	struct stage2_map_data map_data = {
<span class=hunk>@@ -953,6 +953,7 @@ int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
</span> 		.memcache	= mc,
 		.mm_ops		= pgt-&gt;mm_ops,
 		.force_pte	= pgt-&gt;force_pte_cb &amp;&amp; pgt-&gt;force_pte_cb(addr, addr + size, prot),
<span class=add>+		.shared		= shared,
</span> 	};
 	struct kvm_pgtable_walker walker = {
 		.cb		= stage2_map_walker,
<span class=head><a href=#iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c id=Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c>diff</a> --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index 265951c05879..a73adc35cf41 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
</span><span class=hunk>@@ -840,7 +840,7 @@ int kvm_phys_addr_ioremap(struct kvm *kvm, phys_addr_t guest_ipa,
</span> 
 		write_lock(&amp;kvm-&gt;mmu_lock);
 		ret = kvm_pgtable_stage2_map(pgt, addr, PAGE_SIZE, pa, prot,
<span class=del>-					     &amp;cache);
</span><span class=add>+					     &amp;cache, false);
</span> 		write_unlock(&amp;kvm-&gt;mmu_lock);
 		if (ret)
 			break;
<span class=hunk>@@ -1135,7 +1135,6 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
</span> 	gfn_t gfn;
 	kvm_pfn_t pfn;
 	bool logging_active = memslot_is_logging(memslot);
<span class=del>-	bool use_read_lock = false;
</span> 	unsigned long fault_level = kvm_vcpu_trap_get_fault_level(vcpu);
 	unsigned long vma_pagesize, fault_granule;
 	enum kvm_pgtable_prot prot = KVM_PGTABLE_PROT_R;
<span class=hunk>@@ -1170,8 +1169,6 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
</span> 	if (logging_active) {
 		force_pte = true;
 		vma_shift = PAGE_SHIFT;
<span class=del>-		use_read_lock = (fault_status == FSC_PERM &amp;&amp; write_fault &amp;&amp;
-				 fault_granule == PAGE_SIZE);
</span> 	} else {
 		vma_shift = get_vma_page_shift(vma, hva);
 	}
<span class=hunk>@@ -1270,15 +1267,7 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
</span> 	if (exec_fault &amp;&amp; device)
 		return -ENOEXEC;
 
<span class=del>-	/*
-	 * To reduce MMU contentions and enhance concurrency during dirty
-	 * logging dirty logging, only acquire read lock for permission
-	 * relaxation.
-	 */
-	if (use_read_lock)
-		read_lock(&amp;kvm-&gt;mmu_lock);
-	else
-		write_lock(&amp;kvm-&gt;mmu_lock);
</span><span class=add>+	read_lock(&amp;kvm-&gt;mmu_lock);
</span> 	pgt = vcpu-&gt;arch.hw_mmu-&gt;pgt;
 	if (mmu_invalidate_retry(kvm, mmu_seq))
 		goto out_unlock;
<span class=hunk>@@ -1322,15 +1311,12 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
</span> 	 * permissions only if vma_pagesize equals fault_granule. Otherwise,
 	 * kvm_pgtable_stage2_map() should be called to change block size.
 	 */
<span class=del>-	if (fault_status == FSC_PERM &amp;&amp; vma_pagesize == fault_granule) {
</span><span class=add>+	if (fault_status == FSC_PERM &amp;&amp; vma_pagesize == fault_granule)
</span> 		ret = kvm_pgtable_stage2_relax_perms(pgt, fault_ipa, prot);
<span class=del>-	} else {
-		WARN_ONCE(use_read_lock, "Attempted stage-2 map outside of write lock\n");
-
</span><span class=add>+	else
</span> 		ret = kvm_pgtable_stage2_map(pgt, fault_ipa, vma_pagesize,
 					     __pfn_to_phys(pfn), prot,
<span class=del>-					     memcache);
-	}
</span><span class=add>+					     memcache, true);
</span> 
 	/* Mark the page dirty only if the fault is handled successfully */
 	if (writable &amp;&amp; !ret) {
<span class=hunk>@@ -1339,10 +1325,7 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
</span> 	}
 
 out_unlock:
<span class=del>-	if (use_read_lock)
-		read_unlock(&amp;kvm-&gt;mmu_lock);
-	else
-		write_unlock(&amp;kvm-&gt;mmu_lock);
</span><span class=add>+	read_unlock(&amp;kvm-&gt;mmu_lock);
</span> 	kvm_set_pfn_accessed(pfn);
 	kvm_release_pfn_clean(pfn);
 	return ret != -EAGAIN ? ret : 0;
<span class=hunk>@@ -1548,7 +1531,7 @@ bool kvm_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
</span> 	 */
 	kvm_pgtable_stage2_map(kvm-&gt;arch.mmu.pgt, range-&gt;start &lt;&lt; PAGE_SHIFT,
 			       PAGE_SIZE, __pfn_to_phys(pfn),
<span class=del>-			       KVM_PGTABLE_PROT_R, NULL);
</span><span class=add>+			       KVM_PGTABLE_PROT_R, NULL, false);
</span> 
 	return false;
 }
-- 
2.37.2.672.g94769d06f0-goog


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m529155fbc90b4c51cb58a327e8a79fc4e46d349b id=e529155fbc90b4c51cb58a327e8a79fc4e46d349b>^</a> <a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r529155fbc90b4c51cb58a327e8a79fc4e46d349b>96+ messages in thread</a></pre><li><pre><a href=#e529155fbc90b4c51cb58a327e8a79fc4e46d349b id=m529155fbc90b4c51cb58a327e8a79fc4e46d349b>*</a> <b>[PATCH 14/14] KVM: arm64: Handle stage-2 faults in parallel</b>
<b>@ 2022-08-30 19:52   ` Oliver Upton</b>
  <a href=#r529155fbc90b4c51cb58a327e8a79fc4e46d349b>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-08-30 19:52 UTC (<a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/>permalink</a> / <a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/raw>raw</a>)
  To: Marc Zyngier, James Morse, Alexandru Elisei, Suzuki K Poulose,
	Oliver Upton, Catalin Marinas, Will Deacon
  Cc: kvm, linux-kernel, Ben Gardon, David Matlack, Paolo Bonzini,
	kvmarm, linux-arm-kernel

The stage-2 map walker has been made parallel-aware, and as such can be
called while only holding the read side of the MMU lock. Rip out the
conditional locking in user_mem_abort() and instead grab the read lock.
Continue to take the write lock from other callsites to
kvm_pgtable_stage2_map().

Signed-off-by: Oliver Upton &lt;oliver.upton@linux.dev&gt;
---
 <a id=iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h href=#Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>arch/arm64/include/asm/kvm_pgtable.h</a>  |  4 +++-
 <a id=iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c href=#Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c>arch/arm64/kvm/hyp/nvhe/mem_protect.c</a> |  2 +-
 <a id=iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c href=#Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>arch/arm64/kvm/hyp/pgtable.c</a>          |  3 ++-
 <a id=iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c href=#Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c>arch/arm64/kvm/mmu.c</a>                  | 31 ++++++---------------------
 4 files <a href=#e529155fbc90b4c51cb58a327e8a79fc4e46d349b>changed</a>, 13 insertions(+), 27 deletions(-)

<span class=head><a href=#iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h id=Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:include:asm:kvm_pgtable.h>diff</a> --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index 7d2de0a98ccb..dc839db86a1a 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -355,6 +355,8 @@ void kvm_pgtable_stage2_free_removed(void *pgtable, u32 level, void *arg);
</span>  * @prot:	Permissions and attributes for the mapping.
  * @mc:		Cache of pre-allocated and zeroed memory from which to allocate
  *		page-table pages.
<span class=add>+ * @shared:	true if multiple software walkers could be traversing the tables
+ *		in parallel
</span>  *
  * The offset of @addr within a page is ignored, @size is rounded-up to
  * the next page boundary and @phys is rounded-down to the previous page
<span class=hunk>@@ -376,7 +378,7 @@ void kvm_pgtable_stage2_free_removed(void *pgtable, u32 level, void *arg);
</span>  */
 int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
 			   u64 phys, enum kvm_pgtable_prot prot,
<span class=del>-			   void *mc);
</span><span class=add>+			   void *mc, bool shared);
</span> 
 /**
  * kvm_pgtable_stage2_set_owner() - Unmap and annotate pages in the IPA space to
<span class=head><a href=#iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c id=Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:nvhe:mem_protect.c>diff</a> --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
index 61cf223e0796..924d028af447 100644
--- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c
+++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
</span><span class=hunk>@@ -252,7 +252,7 @@ static inline int __host_stage2_idmap(u64 start, u64 end,
</span> 				      enum kvm_pgtable_prot prot)
 {
 	return kvm_pgtable_stage2_map(&amp;host_kvm.pgt, start, end - start, start,
<span class=del>-				      prot, &amp;host_s2_pool);
</span><span class=add>+				      prot, &amp;host_s2_pool, false);
</span> }
 
 /*
<span class=head><a href=#iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c id=Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:hyp:pgtable.c>diff</a> --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 92e230e7bf3a..52ecaaa84b22 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
</span><span class=hunk>@@ -944,7 +944,7 @@ static int stage2_map_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep, kvm_
</span> 
 int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
 			   u64 phys, enum kvm_pgtable_prot prot,
<span class=del>-			   void *mc)
</span><span class=add>+			   void *mc, bool shared)
</span> {
 	int ret;
 	struct stage2_map_data map_data = {
<span class=hunk>@@ -953,6 +953,7 @@ int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
</span> 		.memcache	= mc,
 		.mm_ops		= pgt-&gt;mm_ops,
 		.force_pte	= pgt-&gt;force_pte_cb &amp;&amp; pgt-&gt;force_pte_cb(addr, addr + size, prot),
<span class=add>+		.shared		= shared,
</span> 	};
 	struct kvm_pgtable_walker walker = {
 		.cb		= stage2_map_walker,
<span class=head><a href=#iZ2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c id=Z2e.:..:20220830195216.964988-1-oliver.upton::40linux.dev:1arch:arm64:kvm:mmu.c>diff</a> --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index 265951c05879..a73adc35cf41 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
</span><span class=hunk>@@ -840,7 +840,7 @@ int kvm_phys_addr_ioremap(struct kvm *kvm, phys_addr_t guest_ipa,
</span> 
 		write_lock(&amp;kvm-&gt;mmu_lock);
 		ret = kvm_pgtable_stage2_map(pgt, addr, PAGE_SIZE, pa, prot,
<span class=del>-					     &amp;cache);
</span><span class=add>+					     &amp;cache, false);
</span> 		write_unlock(&amp;kvm-&gt;mmu_lock);
 		if (ret)
 			break;
<span class=hunk>@@ -1135,7 +1135,6 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
</span> 	gfn_t gfn;
 	kvm_pfn_t pfn;
 	bool logging_active = memslot_is_logging(memslot);
<span class=del>-	bool use_read_lock = false;
</span> 	unsigned long fault_level = kvm_vcpu_trap_get_fault_level(vcpu);
 	unsigned long vma_pagesize, fault_granule;
 	enum kvm_pgtable_prot prot = KVM_PGTABLE_PROT_R;
<span class=hunk>@@ -1170,8 +1169,6 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
</span> 	if (logging_active) {
 		force_pte = true;
 		vma_shift = PAGE_SHIFT;
<span class=del>-		use_read_lock = (fault_status == FSC_PERM &amp;&amp; write_fault &amp;&amp;
-				 fault_granule == PAGE_SIZE);
</span> 	} else {
 		vma_shift = get_vma_page_shift(vma, hva);
 	}
<span class=hunk>@@ -1270,15 +1267,7 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
</span> 	if (exec_fault &amp;&amp; device)
 		return -ENOEXEC;
 
<span class=del>-	/*
-	 * To reduce MMU contentions and enhance concurrency during dirty
-	 * logging dirty logging, only acquire read lock for permission
-	 * relaxation.
-	 */
-	if (use_read_lock)
-		read_lock(&amp;kvm-&gt;mmu_lock);
-	else
-		write_lock(&amp;kvm-&gt;mmu_lock);
</span><span class=add>+	read_lock(&amp;kvm-&gt;mmu_lock);
</span> 	pgt = vcpu-&gt;arch.hw_mmu-&gt;pgt;
 	if (mmu_invalidate_retry(kvm, mmu_seq))
 		goto out_unlock;
<span class=hunk>@@ -1322,15 +1311,12 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
</span> 	 * permissions only if vma_pagesize equals fault_granule. Otherwise,
 	 * kvm_pgtable_stage2_map() should be called to change block size.
 	 */
<span class=del>-	if (fault_status == FSC_PERM &amp;&amp; vma_pagesize == fault_granule) {
</span><span class=add>+	if (fault_status == FSC_PERM &amp;&amp; vma_pagesize == fault_granule)
</span> 		ret = kvm_pgtable_stage2_relax_perms(pgt, fault_ipa, prot);
<span class=del>-	} else {
-		WARN_ONCE(use_read_lock, "Attempted stage-2 map outside of write lock\n");
-
</span><span class=add>+	else
</span> 		ret = kvm_pgtable_stage2_map(pgt, fault_ipa, vma_pagesize,
 					     __pfn_to_phys(pfn), prot,
<span class=del>-					     memcache);
-	}
</span><span class=add>+					     memcache, true);
</span> 
 	/* Mark the page dirty only if the fault is handled successfully */
 	if (writable &amp;&amp; !ret) {
<span class=hunk>@@ -1339,10 +1325,7 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
</span> 	}
 
 out_unlock:
<span class=del>-	if (use_read_lock)
-		read_unlock(&amp;kvm-&gt;mmu_lock);
-	else
-		write_unlock(&amp;kvm-&gt;mmu_lock);
</span><span class=add>+	read_unlock(&amp;kvm-&gt;mmu_lock);
</span> 	kvm_set_pfn_accessed(pfn);
 	kvm_release_pfn_clean(pfn);
 	return ret != -EAGAIN ? ret : 0;
<span class=hunk>@@ -1548,7 +1531,7 @@ bool kvm_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
</span> 	 */
 	kvm_pgtable_stage2_map(kvm-&gt;arch.mmu.pgt, range-&gt;start &lt;&lt; PAGE_SHIFT,
 			       PAGE_SIZE, __pfn_to_phys(pfn),
<span class=del>-			       KVM_PGTABLE_PROT_R, NULL);
</span><span class=add>+			       KVM_PGTABLE_PROT_R, NULL, false);
</span> 
 	return false;
 }
-- 
2.37.2.672.g94769d06f0-goog

_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m529155fbc90b4c51cb58a327e8a79fc4e46d349b id=e529155fbc90b4c51cb58a327e8a79fc4e46d349b>^</a> <a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/>permalink</a> <a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/raw>raw</a> <a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/#R>reply</a> <a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/#related>related</a>	[<a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/T/#u>flat</a>|<a href=https://lore.kernel.org/all/20220830195216.964988-1-oliver.upton@linux.dev/t/#u><b>nested</b></a>] <a href=#r529155fbc90b4c51cb58a327e8a79fc4e46d349b>96+ messages in thread</a></pre></ul><li><pre><a href=#e22f22300af9fa61252f02fd205e1a7e7c6d99627 id=m22f22300af9fa61252f02fd205e1a7e7c6d99627>*</a> <b>Re: [PATCH 00/14] KVM: arm64: Parallel stage-2 fault handling</b>
  2022-08-30 19:41 ` <a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb>Oliver Upton</a>
  (?)
<b>@ 2022-09-06 10:00   ` Marc Zyngier</b>
  <a href=#r22f22300af9fa61252f02fd205e1a7e7c6d99627>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Marc Zyngier @ 2022-09-06 10:00 UTC (<a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/>permalink</a> / <a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/raw>raw</a>)
  To: Oliver Upton
  Cc: James Morse, Alexandru Elisei, Suzuki K Poulose, Catalin Marinas,
	Will Deacon, Quentin Perret, Ricardo Koller, Reiji Watanabe,
	David Matlack, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm

On Tue, 30 Aug 2022 20:41:18 +0100,
Oliver Upton &lt;oliver.upton@linux.dev&gt; wrote:
<span class=q>&gt; 
&gt; Presently KVM only takes a read lock for stage 2 faults if it believes
&gt; the fault can be fixed by relaxing permissions on a PTE (write unprotect
&gt; for dirty logging). Otherwise, stage 2 faults grab the write lock, which
&gt; predictably can pile up all the vCPUs in a sufficiently large VM.
&gt; 
&gt; Like the TDP MMU for x86, this series loosens the locking around
&gt; manipulations of the stage 2 page tables to allow parallel faults. RCU
&gt; and atomics are exploited to safely build/destroy the stage 2 page
&gt; tables in light of multiple software observers.
&gt; 
&gt; Patches 1-2 are a cleanup to the way we collapse page tables, with the
&gt; added benefit of narrowing the window of time a range of memory is
&gt; unmapped.
&gt; 
&gt; Patches 3-7 are minor cleanups and refactorings to the way KVM reads
&gt; PTEs and traverses the stage 2 page tables to make it amenable to
&gt; concurrent modification.
&gt; 
&gt; Patches 8-9 use RCU to punt page table cleanup out of the vCPU fault
&gt; path, which should also improve fault latency a bit.
&gt; 
&gt; Patches 10-14 implement the meat of this series, extending the
&gt; 'break-before-make' sequence with atomics to realize locking on PTEs.
&gt; Effectively a cmpxchg() is used to 'break' a PTE, thereby serializing
&gt; changes to a given PTE.
&gt; 
&gt; Finally, patch 15 flips the switch on all the new code and starts
&gt; grabbing the read side of the MMU lock for stage 2 faults.
&gt; 
&gt; Applies to 6.0-rc3. Tested with KVM selftests and benchmarked with
&gt; dirty_log_perf_test, scaling from 1 to 48 vCPUs with 4GB of memory per
&gt; vCPU backed by THP.
&gt; 
&gt;   ./dirty_log_perf_test -s anonymous_thp -m 2 -b 4G -v ${NR_VCPUS}
&gt; 
&gt; Time to dirty memory:
&gt; 
&gt;         +-------+---------+------------------+
&gt;         | vCPUs | 6.0-rc3 | 6.0-rc3 + series |
&gt;         +-------+---------+------------------+
&gt;         |     1 | 0.89s   | 0.92s            |
&gt;         |     2 | 1.13s   | 1.18s            |
&gt;         |     4 | 2.42s   | 1.25s            |
&gt;         |     8 | 5.03s   | 1.36s            |
&gt;         |    16 | 8.84s   | 2.09s            |
&gt;         |    32 | 19.60s  | 4.47s            |
&gt;         |    48 | 31.39s  | 6.22s            |
&gt;         +-------+---------+------------------+
&gt; 
&gt; It is also worth mentioning that the time to populate memory has
&gt; improved:
&gt; 
&gt;         +-------+---------+------------------+
&gt;         | vCPUs | 6.0-rc3 | 6.0-rc3 + series |
&gt;         +-------+---------+------------------+
&gt;         |     1 | 0.19s   | 0.18s            |
&gt;         |     2 | 0.25s   | 0.21s            |
&gt;         |     4 | 0.38s   | 0.32s            |
&gt;         |     8 | 0.64s   | 0.40s            |
&gt;         |    16 | 1.22s   | 0.54s            |
&gt;         |    32 | 2.50s   | 1.03s            |
&gt;         |    48 | 3.88s   | 1.52s            |
&gt;         +-------+---------+------------------+
&gt; 
&gt; RFC: <a href=https://lore.kernel.org/kvmarm/20220415215901.1737897-1-oupton@google.com/>https://lore.kernel.org/kvmarm/20220415215901.1737897-1-oupton@google.com/</a>
&gt; 
&gt; RFC -&gt; v1:
&gt;  - Factored out page table teardown from kvm_pgtable_stage2_map()
&gt;  - Use the RCU callback to tear down a subtree, instead of scheduling a
&gt;    callback for every individual table page.
&gt;  - Reorganized series to (hopefully) avoid intermediate breakage.
&gt;  - Dropped the use of page headers, instead stuffing KVM metadata into
&gt;    page::private directly
&gt; 
&gt; Oliver Upton (14):
&gt;   KVM: arm64: Add a helper to tear down unlinked stage-2 subtrees
&gt;   KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make
&gt;   KVM: arm64: Directly read owner id field in stage2_pte_is_counted()
&gt;   KVM: arm64: Read the PTE once per visit
&gt;   KVM: arm64: Split init and set for table PTE
&gt;   KVM: arm64: Return next table from map callbacks
&gt;   KVM: arm64: Document behavior of pgtable visitor callback
&gt;   KVM: arm64: Protect page table traversal with RCU
&gt;   KVM: arm64: Free removed stage-2 tables in RCU callback
&gt;   KVM: arm64: Atomically update stage 2 leaf attributes in parallel
&gt;     walks
&gt;   KVM: arm64: Make changes block-&gt;table to leaf PTEs parallel-aware
&gt;   KVM: arm64: Make leaf-&gt;leaf PTE changes parallel-aware
&gt;   KVM: arm64: Make table-&gt;block changes parallel-aware
&gt;   KVM: arm64: Handle stage-2 faults in parallel
&gt; 
&gt;  arch/arm64/include/asm/kvm_pgtable.h  |  59 ++++-
&gt;  arch/arm64/kvm/hyp/nvhe/mem_protect.c |   7 +-
&gt;  arch/arm64/kvm/hyp/nvhe/setup.c       |   4 +-
&gt;  arch/arm64/kvm/hyp/pgtable.c          | 360 ++++++++++++++++----------
&gt;  arch/arm64/kvm/mmu.c                  |  65 +++--
&gt;  5 files changed, 325 insertions(+), 170 deletions(-)
</span>
This fails to build on -rc4:

  MODPOST vmlinux.symvers
  MODINFO modules.builtin.modinfo
  GEN     modules.builtin
  CC      .vmlinux.export.o
  LD      .tmp_vmlinux.kallsyms1
ld: Unexpected GOT/PLT entries detected!
ld: Unexpected run-time procedure linkages detected!
ld: ID map text too big or misaligned
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_walk':
(.hyp.text+0xdc0c): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xdc1c): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_get_leaf':
(.hyp.text+0xdc80): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xdc90): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_hyp_map':
(.hyp.text+0xddb0): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xddc0): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_hyp_unmap':
(.hyp.text+0xde44): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xde50): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_hyp_destroy':
(.hyp.text+0xdf40): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xdf50): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_map':
(.hyp.text+0xe16c): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xe17c): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_set_owner':
(.hyp.text+0xe264): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xe274): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_unmap':
(.hyp.text+0xe2d4): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xe2e4): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_flush':
(.hyp.text+0xe5b4): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xe5c4): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_destroy':
(.hyp.text+0xe6f0): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xe700): undefined reference to `__kvm_nvhe___rcu_read_unlock'
make[3]: *** [Makefile:1169: vmlinux] Error 1
make[2]: *** [debian/rules:7: build-arch] Error 2

as this drags the RCU read-lock into EL2, and that's not going to
work... The following fixes it, but I wonder how you tested it.

Thanks,

	M.

<span class=head>diff --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index dc839db86a1a..adf170122daf 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -580,7 +580,7 @@ enum kvm_pgtable_prot kvm_pgtable_stage2_pte_prot(kvm_pte_t pte);
</span>  */
 enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte);
 
<span class=del>-#if defined(__KVM_NVHE_HYPERVISOR___)
</span><span class=add>+#if defined(__KVM_NVHE_HYPERVISOR__)
</span> 
 static inline void kvm_pgtable_walk_begin(void) {}
 static inline void kvm_pgtable_walk_end(void) {}

-- 
Without deviation from the norm, progress is not possible.

<a href=#m22f22300af9fa61252f02fd205e1a7e7c6d99627 id=e22f22300af9fa61252f02fd205e1a7e7c6d99627>^</a> <a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/>permalink</a> <a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/raw>raw</a> <a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/#R>reply</a> <a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/#related>related</a>	[<a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/T/#u>flat</a>|<a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/t/#u><b>nested</b></a>] <a href=#r22f22300af9fa61252f02fd205e1a7e7c6d99627>96+ messages in thread</a></pre><li><ul><li><pre><a href=#e22f22300af9fa61252f02fd205e1a7e7c6d99627 id=m22f22300af9fa61252f02fd205e1a7e7c6d99627>*</a> <b>Re: [PATCH 00/14] KVM: arm64: Parallel stage-2 fault handling</b>
<b>@ 2022-09-06 10:00   ` Marc Zyngier</b>
  <a href=#r22f22300af9fa61252f02fd205e1a7e7c6d99627>0 siblings, 0 replies; 96+ messages in thread</a>
From: Marc Zyngier @ 2022-09-06 10:00 UTC (<a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/>permalink</a> / <a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/raw>raw</a>)
  To: Oliver Upton
  Cc: James Morse, Alexandru Elisei, Suzuki K Poulose, Catalin Marinas,
	Will Deacon, Quentin Perret, Ricardo Koller, Reiji Watanabe,
	David Matlack, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm

On Tue, 30 Aug 2022 20:41:18 +0100,
Oliver Upton &lt;oliver.upton@linux.dev&gt; wrote:
<span class=q>&gt; 
&gt; Presently KVM only takes a read lock for stage 2 faults if it believes
&gt; the fault can be fixed by relaxing permissions on a PTE (write unprotect
&gt; for dirty logging). Otherwise, stage 2 faults grab the write lock, which
&gt; predictably can pile up all the vCPUs in a sufficiently large VM.
&gt; 
&gt; Like the TDP MMU for x86, this series loosens the locking around
&gt; manipulations of the stage 2 page tables to allow parallel faults. RCU
&gt; and atomics are exploited to safely build/destroy the stage 2 page
&gt; tables in light of multiple software observers.
&gt; 
&gt; Patches 1-2 are a cleanup to the way we collapse page tables, with the
&gt; added benefit of narrowing the window of time a range of memory is
&gt; unmapped.
&gt; 
&gt; Patches 3-7 are minor cleanups and refactorings to the way KVM reads
&gt; PTEs and traverses the stage 2 page tables to make it amenable to
&gt; concurrent modification.
&gt; 
&gt; Patches 8-9 use RCU to punt page table cleanup out of the vCPU fault
&gt; path, which should also improve fault latency a bit.
&gt; 
&gt; Patches 10-14 implement the meat of this series, extending the
&gt; 'break-before-make' sequence with atomics to realize locking on PTEs.
&gt; Effectively a cmpxchg() is used to 'break' a PTE, thereby serializing
&gt; changes to a given PTE.
&gt; 
&gt; Finally, patch 15 flips the switch on all the new code and starts
&gt; grabbing the read side of the MMU lock for stage 2 faults.
&gt; 
&gt; Applies to 6.0-rc3. Tested with KVM selftests and benchmarked with
&gt; dirty_log_perf_test, scaling from 1 to 48 vCPUs with 4GB of memory per
&gt; vCPU backed by THP.
&gt; 
&gt;   ./dirty_log_perf_test -s anonymous_thp -m 2 -b 4G -v ${NR_VCPUS}
&gt; 
&gt; Time to dirty memory:
&gt; 
&gt;         +-------+---------+------------------+
&gt;         | vCPUs | 6.0-rc3 | 6.0-rc3 + series |
&gt;         +-------+---------+------------------+
&gt;         |     1 | 0.89s   | 0.92s            |
&gt;         |     2 | 1.13s   | 1.18s            |
&gt;         |     4 | 2.42s   | 1.25s            |
&gt;         |     8 | 5.03s   | 1.36s            |
&gt;         |    16 | 8.84s   | 2.09s            |
&gt;         |    32 | 19.60s  | 4.47s            |
&gt;         |    48 | 31.39s  | 6.22s            |
&gt;         +-------+---------+------------------+
&gt; 
&gt; It is also worth mentioning that the time to populate memory has
&gt; improved:
&gt; 
&gt;         +-------+---------+------------------+
&gt;         | vCPUs | 6.0-rc3 | 6.0-rc3 + series |
&gt;         +-------+---------+------------------+
&gt;         |     1 | 0.19s   | 0.18s            |
&gt;         |     2 | 0.25s   | 0.21s            |
&gt;         |     4 | 0.38s   | 0.32s            |
&gt;         |     8 | 0.64s   | 0.40s            |
&gt;         |    16 | 1.22s   | 0.54s            |
&gt;         |    32 | 2.50s   | 1.03s            |
&gt;         |    48 | 3.88s   | 1.52s            |
&gt;         +-------+---------+------------------+
&gt; 
&gt; RFC: <a href=https://lore.kernel.org/kvmarm/20220415215901.1737897-1-oupton@google.com/>https://lore.kernel.org/kvmarm/20220415215901.1737897-1-oupton@google.com/</a>
&gt; 
&gt; RFC -&gt; v1:
&gt;  - Factored out page table teardown from kvm_pgtable_stage2_map()
&gt;  - Use the RCU callback to tear down a subtree, instead of scheduling a
&gt;    callback for every individual table page.
&gt;  - Reorganized series to (hopefully) avoid intermediate breakage.
&gt;  - Dropped the use of page headers, instead stuffing KVM metadata into
&gt;    page::private directly
&gt; 
&gt; Oliver Upton (14):
&gt;   KVM: arm64: Add a helper to tear down unlinked stage-2 subtrees
&gt;   KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make
&gt;   KVM: arm64: Directly read owner id field in stage2_pte_is_counted()
&gt;   KVM: arm64: Read the PTE once per visit
&gt;   KVM: arm64: Split init and set for table PTE
&gt;   KVM: arm64: Return next table from map callbacks
&gt;   KVM: arm64: Document behavior of pgtable visitor callback
&gt;   KVM: arm64: Protect page table traversal with RCU
&gt;   KVM: arm64: Free removed stage-2 tables in RCU callback
&gt;   KVM: arm64: Atomically update stage 2 leaf attributes in parallel
&gt;     walks
&gt;   KVM: arm64: Make changes block-&gt;table to leaf PTEs parallel-aware
&gt;   KVM: arm64: Make leaf-&gt;leaf PTE changes parallel-aware
&gt;   KVM: arm64: Make table-&gt;block changes parallel-aware
&gt;   KVM: arm64: Handle stage-2 faults in parallel
&gt; 
&gt;  arch/arm64/include/asm/kvm_pgtable.h  |  59 ++++-
&gt;  arch/arm64/kvm/hyp/nvhe/mem_protect.c |   7 +-
&gt;  arch/arm64/kvm/hyp/nvhe/setup.c       |   4 +-
&gt;  arch/arm64/kvm/hyp/pgtable.c          | 360 ++++++++++++++++----------
&gt;  arch/arm64/kvm/mmu.c                  |  65 +++--
&gt;  5 files changed, 325 insertions(+), 170 deletions(-)
</span>
This fails to build on -rc4:

  MODPOST vmlinux.symvers
  MODINFO modules.builtin.modinfo
  GEN     modules.builtin
  CC      .vmlinux.export.o
  LD      .tmp_vmlinux.kallsyms1
ld: Unexpected GOT/PLT entries detected!
ld: Unexpected run-time procedure linkages detected!
ld: ID map text too big or misaligned
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_walk':
(.hyp.text+0xdc0c): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xdc1c): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_get_leaf':
(.hyp.text+0xdc80): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xdc90): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_hyp_map':
(.hyp.text+0xddb0): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xddc0): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_hyp_unmap':
(.hyp.text+0xde44): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xde50): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_hyp_destroy':
(.hyp.text+0xdf40): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xdf50): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_map':
(.hyp.text+0xe16c): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xe17c): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_set_owner':
(.hyp.text+0xe264): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xe274): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_unmap':
(.hyp.text+0xe2d4): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xe2e4): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_flush':
(.hyp.text+0xe5b4): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xe5c4): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_destroy':
(.hyp.text+0xe6f0): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xe700): undefined reference to `__kvm_nvhe___rcu_read_unlock'
make[3]: *** [Makefile:1169: vmlinux] Error 1
make[2]: *** [debian/rules:7: build-arch] Error 2

as this drags the RCU read-lock into EL2, and that's not going to
work... The following fixes it, but I wonder how you tested it.

Thanks,

	M.

<span class=head>diff --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index dc839db86a1a..adf170122daf 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -580,7 +580,7 @@ enum kvm_pgtable_prot kvm_pgtable_stage2_pte_prot(kvm_pte_t pte);
</span>  */
 enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte);
 
<span class=del>-#if defined(__KVM_NVHE_HYPERVISOR___)
</span><span class=add>+#if defined(__KVM_NVHE_HYPERVISOR__)
</span> 
 static inline void kvm_pgtable_walk_begin(void) {}
 static inline void kvm_pgtable_walk_end(void) {}

-- 
Without deviation from the norm, progress is not possible.

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#m22f22300af9fa61252f02fd205e1a7e7c6d99627 id=e22f22300af9fa61252f02fd205e1a7e7c6d99627>^</a> <a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/>permalink</a> <a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/raw>raw</a> <a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/#R>reply</a> <a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/#related>related</a>	[<a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/T/#u>flat</a>|<a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/t/#u><b>nested</b></a>] <a href=#r22f22300af9fa61252f02fd205e1a7e7c6d99627>96+ messages in thread</a></pre><li><pre><a href=#e22f22300af9fa61252f02fd205e1a7e7c6d99627 id=m22f22300af9fa61252f02fd205e1a7e7c6d99627>*</a> <b>Re: [PATCH 00/14] KVM: arm64: Parallel stage-2 fault handling</b>
<b>@ 2022-09-06 10:00   ` Marc Zyngier</b>
  <a href=#r22f22300af9fa61252f02fd205e1a7e7c6d99627>0 siblings, 0 replies; 96+ messages in thread</a>
From: Marc Zyngier @ 2022-09-06 10:00 UTC (<a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/>permalink</a> / <a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/raw>raw</a>)
  To: Oliver Upton
  Cc: kvm, Will Deacon, Catalin Marinas, Ben Gardon, David Matlack,
	Paolo Bonzini, kvmarm, linux-arm-kernel

On Tue, 30 Aug 2022 20:41:18 +0100,
Oliver Upton &lt;oliver.upton@linux.dev&gt; wrote:
<span class=q>&gt; 
&gt; Presently KVM only takes a read lock for stage 2 faults if it believes
&gt; the fault can be fixed by relaxing permissions on a PTE (write unprotect
&gt; for dirty logging). Otherwise, stage 2 faults grab the write lock, which
&gt; predictably can pile up all the vCPUs in a sufficiently large VM.
&gt; 
&gt; Like the TDP MMU for x86, this series loosens the locking around
&gt; manipulations of the stage 2 page tables to allow parallel faults. RCU
&gt; and atomics are exploited to safely build/destroy the stage 2 page
&gt; tables in light of multiple software observers.
&gt; 
&gt; Patches 1-2 are a cleanup to the way we collapse page tables, with the
&gt; added benefit of narrowing the window of time a range of memory is
&gt; unmapped.
&gt; 
&gt; Patches 3-7 are minor cleanups and refactorings to the way KVM reads
&gt; PTEs and traverses the stage 2 page tables to make it amenable to
&gt; concurrent modification.
&gt; 
&gt; Patches 8-9 use RCU to punt page table cleanup out of the vCPU fault
&gt; path, which should also improve fault latency a bit.
&gt; 
&gt; Patches 10-14 implement the meat of this series, extending the
&gt; 'break-before-make' sequence with atomics to realize locking on PTEs.
&gt; Effectively a cmpxchg() is used to 'break' a PTE, thereby serializing
&gt; changes to a given PTE.
&gt; 
&gt; Finally, patch 15 flips the switch on all the new code and starts
&gt; grabbing the read side of the MMU lock for stage 2 faults.
&gt; 
&gt; Applies to 6.0-rc3. Tested with KVM selftests and benchmarked with
&gt; dirty_log_perf_test, scaling from 1 to 48 vCPUs with 4GB of memory per
&gt; vCPU backed by THP.
&gt; 
&gt;   ./dirty_log_perf_test -s anonymous_thp -m 2 -b 4G -v ${NR_VCPUS}
&gt; 
&gt; Time to dirty memory:
&gt; 
&gt;         +-------+---------+------------------+
&gt;         | vCPUs | 6.0-rc3 | 6.0-rc3 + series |
&gt;         +-------+---------+------------------+
&gt;         |     1 | 0.89s   | 0.92s            |
&gt;         |     2 | 1.13s   | 1.18s            |
&gt;         |     4 | 2.42s   | 1.25s            |
&gt;         |     8 | 5.03s   | 1.36s            |
&gt;         |    16 | 8.84s   | 2.09s            |
&gt;         |    32 | 19.60s  | 4.47s            |
&gt;         |    48 | 31.39s  | 6.22s            |
&gt;         +-------+---------+------------------+
&gt; 
&gt; It is also worth mentioning that the time to populate memory has
&gt; improved:
&gt; 
&gt;         +-------+---------+------------------+
&gt;         | vCPUs | 6.0-rc3 | 6.0-rc3 + series |
&gt;         +-------+---------+------------------+
&gt;         |     1 | 0.19s   | 0.18s            |
&gt;         |     2 | 0.25s   | 0.21s            |
&gt;         |     4 | 0.38s   | 0.32s            |
&gt;         |     8 | 0.64s   | 0.40s            |
&gt;         |    16 | 1.22s   | 0.54s            |
&gt;         |    32 | 2.50s   | 1.03s            |
&gt;         |    48 | 3.88s   | 1.52s            |
&gt;         +-------+---------+------------------+
&gt; 
&gt; RFC: <a href=https://lore.kernel.org/kvmarm/20220415215901.1737897-1-oupton@google.com/>https://lore.kernel.org/kvmarm/20220415215901.1737897-1-oupton@google.com/</a>
&gt; 
&gt; RFC -&gt; v1:
&gt;  - Factored out page table teardown from kvm_pgtable_stage2_map()
&gt;  - Use the RCU callback to tear down a subtree, instead of scheduling a
&gt;    callback for every individual table page.
&gt;  - Reorganized series to (hopefully) avoid intermediate breakage.
&gt;  - Dropped the use of page headers, instead stuffing KVM metadata into
&gt;    page::private directly
&gt; 
&gt; Oliver Upton (14):
&gt;   KVM: arm64: Add a helper to tear down unlinked stage-2 subtrees
&gt;   KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make
&gt;   KVM: arm64: Directly read owner id field in stage2_pte_is_counted()
&gt;   KVM: arm64: Read the PTE once per visit
&gt;   KVM: arm64: Split init and set for table PTE
&gt;   KVM: arm64: Return next table from map callbacks
&gt;   KVM: arm64: Document behavior of pgtable visitor callback
&gt;   KVM: arm64: Protect page table traversal with RCU
&gt;   KVM: arm64: Free removed stage-2 tables in RCU callback
&gt;   KVM: arm64: Atomically update stage 2 leaf attributes in parallel
&gt;     walks
&gt;   KVM: arm64: Make changes block-&gt;table to leaf PTEs parallel-aware
&gt;   KVM: arm64: Make leaf-&gt;leaf PTE changes parallel-aware
&gt;   KVM: arm64: Make table-&gt;block changes parallel-aware
&gt;   KVM: arm64: Handle stage-2 faults in parallel
&gt; 
&gt;  arch/arm64/include/asm/kvm_pgtable.h  |  59 ++++-
&gt;  arch/arm64/kvm/hyp/nvhe/mem_protect.c |   7 +-
&gt;  arch/arm64/kvm/hyp/nvhe/setup.c       |   4 +-
&gt;  arch/arm64/kvm/hyp/pgtable.c          | 360 ++++++++++++++++----------
&gt;  arch/arm64/kvm/mmu.c                  |  65 +++--
&gt;  5 files changed, 325 insertions(+), 170 deletions(-)
</span>
This fails to build on -rc4:

  MODPOST vmlinux.symvers
  MODINFO modules.builtin.modinfo
  GEN     modules.builtin
  CC      .vmlinux.export.o
  LD      .tmp_vmlinux.kallsyms1
ld: Unexpected GOT/PLT entries detected!
ld: Unexpected run-time procedure linkages detected!
ld: ID map text too big or misaligned
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_walk':
(.hyp.text+0xdc0c): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xdc1c): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_get_leaf':
(.hyp.text+0xdc80): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xdc90): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_hyp_map':
(.hyp.text+0xddb0): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xddc0): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_hyp_unmap':
(.hyp.text+0xde44): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xde50): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_hyp_destroy':
(.hyp.text+0xdf40): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xdf50): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_map':
(.hyp.text+0xe16c): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xe17c): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_set_owner':
(.hyp.text+0xe264): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xe274): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_unmap':
(.hyp.text+0xe2d4): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xe2e4): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_flush':
(.hyp.text+0xe5b4): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xe5c4): undefined reference to `__kvm_nvhe___rcu_read_unlock'
ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_destroy':
(.hyp.text+0xe6f0): undefined reference to `__kvm_nvhe___rcu_read_lock'
ld: (.hyp.text+0xe700): undefined reference to `__kvm_nvhe___rcu_read_unlock'
make[3]: *** [Makefile:1169: vmlinux] Error 1
make[2]: *** [debian/rules:7: build-arch] Error 2

as this drags the RCU read-lock into EL2, and that's not going to
work... The following fixes it, but I wonder how you tested it.

Thanks,

	M.

<span class=head>diff --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index dc839db86a1a..adf170122daf 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
</span><span class=hunk>@@ -580,7 +580,7 @@ enum kvm_pgtable_prot kvm_pgtable_stage2_pte_prot(kvm_pte_t pte);
</span>  */
 enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte);
 
<span class=del>-#if defined(__KVM_NVHE_HYPERVISOR___)
</span><span class=add>+#if defined(__KVM_NVHE_HYPERVISOR__)
</span> 
 static inline void kvm_pgtable_walk_begin(void) {}
 static inline void kvm_pgtable_walk_end(void) {}

-- 
Without deviation from the norm, progress is not possible.
_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#m22f22300af9fa61252f02fd205e1a7e7c6d99627 id=e22f22300af9fa61252f02fd205e1a7e7c6d99627>^</a> <a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/>permalink</a> <a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/raw>raw</a> <a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/#R>reply</a> <a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/#related>related</a>	[<a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/T/#u>flat</a>|<a href=https://lore.kernel.org/all/87o7vsvn4m.wl-maz@kernel.org/t/#u><b>nested</b></a>] <a href=#r22f22300af9fa61252f02fd205e1a7e7c6d99627>96+ messages in thread</a></pre><li><pre><a href=#edda8f0fc4b67e247e2625ddf414446f6e7ab9ab0 id=mdda8f0fc4b67e247e2625ddf414446f6e7ab9ab0>*</a> <b>Re: [PATCH 00/14] KVM: arm64: Parallel stage-2 fault handling</b>
  2022-09-06 10:00   ` <a href=#m22f22300af9fa61252f02fd205e1a7e7c6d99627>Marc Zyngier</a>
  (?)
<b>@ 2022-09-09 10:01     ` Oliver Upton</b>
  <a href=#rdda8f0fc4b67e247e2625ddf414446f6e7ab9ab0>-1 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-09-09 10:01 UTC (<a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/raw>raw</a>)
  To: Marc Zyngier
  Cc: James Morse, Alexandru Elisei, Suzuki K Poulose, Catalin Marinas,
	Will Deacon, Quentin Perret, Ricardo Koller, Reiji Watanabe,
	David Matlack, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm

Hey Marc,

On Tue, Sep 06, 2022 at 11:00:09AM +0100, Marc Zyngier wrote:

[...]

<span class=q>&gt; This fails to build on -rc4:
&gt; 
&gt;   MODPOST vmlinux.symvers
&gt;   MODINFO modules.builtin.modinfo
&gt;   GEN     modules.builtin
&gt;   CC      .vmlinux.export.o
&gt;   LD      .tmp_vmlinux.kallsyms1
&gt; ld: Unexpected GOT/PLT entries detected!
&gt; ld: Unexpected run-time procedure linkages detected!
&gt; ld: ID map text too big or misaligned
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_walk':
&gt; (.hyp.text+0xdc0c): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xdc1c): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_get_leaf':
&gt; (.hyp.text+0xdc80): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xdc90): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_hyp_map':
&gt; (.hyp.text+0xddb0): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xddc0): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_hyp_unmap':
&gt; (.hyp.text+0xde44): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xde50): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_hyp_destroy':
&gt; (.hyp.text+0xdf40): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xdf50): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_map':
&gt; (.hyp.text+0xe16c): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xe17c): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_set_owner':
&gt; (.hyp.text+0xe264): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xe274): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_unmap':
&gt; (.hyp.text+0xe2d4): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xe2e4): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_flush':
&gt; (.hyp.text+0xe5b4): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xe5c4): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_destroy':
&gt; (.hyp.text+0xe6f0): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xe700): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; make[3]: *** [Makefile:1169: vmlinux] Error 1
&gt; make[2]: *** [debian/rules:7: build-arch] Error 2
&gt; 
&gt; as this drags the RCU read-lock into EL2, and that's not going to
&gt; work... The following fixes it, but I wonder how you tested it.
</span>
Ugh. I was carrying a patch on top of my series to handle compilation
issues with rseq_test, I managed to squash the equivalent of below in
that patch.

Nonetheless, I *did* actually test it to get the numbers above :)

--
Thanks,
Oliver

<span class=q>&gt; diff --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
&gt; index dc839db86a1a..adf170122daf 100644
&gt; --- a/arch/arm64/include/asm/kvm_pgtable.h
&gt; +++ b/arch/arm64/include/asm/kvm_pgtable.h
&gt; @@ -580,7 +580,7 @@ enum kvm_pgtable_prot kvm_pgtable_stage2_pte_prot(kvm_pte_t pte);
&gt;   */
&gt;  enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte);
&gt;  
&gt; -#if defined(__KVM_NVHE_HYPERVISOR___)
&gt; +#if defined(__KVM_NVHE_HYPERVISOR__)
&gt;  
&gt;  static inline void kvm_pgtable_walk_begin(void) {}
&gt;  static inline void kvm_pgtable_walk_end(void) {}
&gt; 
&gt; -- 
&gt; Without deviation from the norm, progress is not possible.
</span>
<a href=#mdda8f0fc4b67e247e2625ddf414446f6e7ab9ab0 id=edda8f0fc4b67e247e2625ddf414446f6e7ab9ab0>^</a> <a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/t/#u><b>nested</b></a>] <a href=#rdda8f0fc4b67e247e2625ddf414446f6e7ab9ab0>96+ messages in thread</a></pre><li><ul><li><pre><a href=#edda8f0fc4b67e247e2625ddf414446f6e7ab9ab0 id=mdda8f0fc4b67e247e2625ddf414446f6e7ab9ab0>*</a> <b>Re: [PATCH 00/14] KVM: arm64: Parallel stage-2 fault handling</b>
<b>@ 2022-09-09 10:01     ` Oliver Upton</b>
  <a href=#rdda8f0fc4b67e247e2625ddf414446f6e7ab9ab0>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-09-09 10:01 UTC (<a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/raw>raw</a>)
  To: Marc Zyngier
  Cc: James Morse, Alexandru Elisei, Suzuki K Poulose, Catalin Marinas,
	Will Deacon, Quentin Perret, Ricardo Koller, Reiji Watanabe,
	David Matlack, Ben Gardon, Paolo Bonzini, Gavin Shan, Peter Xu,
	Sean Christopherson, linux-arm-kernel, kvmarm, kvm

Hey Marc,

On Tue, Sep 06, 2022 at 11:00:09AM +0100, Marc Zyngier wrote:

[...]

<span class=q>&gt; This fails to build on -rc4:
&gt; 
&gt;   MODPOST vmlinux.symvers
&gt;   MODINFO modules.builtin.modinfo
&gt;   GEN     modules.builtin
&gt;   CC      .vmlinux.export.o
&gt;   LD      .tmp_vmlinux.kallsyms1
&gt; ld: Unexpected GOT/PLT entries detected!
&gt; ld: Unexpected run-time procedure linkages detected!
&gt; ld: ID map text too big or misaligned
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_walk':
&gt; (.hyp.text+0xdc0c): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xdc1c): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_get_leaf':
&gt; (.hyp.text+0xdc80): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xdc90): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_hyp_map':
&gt; (.hyp.text+0xddb0): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xddc0): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_hyp_unmap':
&gt; (.hyp.text+0xde44): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xde50): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_hyp_destroy':
&gt; (.hyp.text+0xdf40): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xdf50): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_map':
&gt; (.hyp.text+0xe16c): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xe17c): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_set_owner':
&gt; (.hyp.text+0xe264): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xe274): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_unmap':
&gt; (.hyp.text+0xe2d4): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xe2e4): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_flush':
&gt; (.hyp.text+0xe5b4): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xe5c4): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_destroy':
&gt; (.hyp.text+0xe6f0): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xe700): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; make[3]: *** [Makefile:1169: vmlinux] Error 1
&gt; make[2]: *** [debian/rules:7: build-arch] Error 2
&gt; 
&gt; as this drags the RCU read-lock into EL2, and that's not going to
&gt; work... The following fixes it, but I wonder how you tested it.
</span>
Ugh. I was carrying a patch on top of my series to handle compilation
issues with rseq_test, I managed to squash the equivalent of below in
that patch.

Nonetheless, I *did* actually test it to get the numbers above :)

--
Thanks,
Oliver

<span class=q>&gt; diff --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
&gt; index dc839db86a1a..adf170122daf 100644
&gt; --- a/arch/arm64/include/asm/kvm_pgtable.h
&gt; +++ b/arch/arm64/include/asm/kvm_pgtable.h
&gt; @@ -580,7 +580,7 @@ enum kvm_pgtable_prot kvm_pgtable_stage2_pte_prot(kvm_pte_t pte);
&gt;   */
&gt;  enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte);
&gt;  
&gt; -#if defined(__KVM_NVHE_HYPERVISOR___)
&gt; +#if defined(__KVM_NVHE_HYPERVISOR__)
&gt;  
&gt;  static inline void kvm_pgtable_walk_begin(void) {}
&gt;  static inline void kvm_pgtable_walk_end(void) {}
&gt; 
&gt; -- 
&gt; Without deviation from the norm, progress is not possible.
</span>
_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
<a href=http://lists.infradead.org/mailman/listinfo/linux-arm-kernel>http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</a>

<a href=#mdda8f0fc4b67e247e2625ddf414446f6e7ab9ab0 id=edda8f0fc4b67e247e2625ddf414446f6e7ab9ab0>^</a> <a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/t/#u><b>nested</b></a>] <a href=#rdda8f0fc4b67e247e2625ddf414446f6e7ab9ab0>96+ messages in thread</a></pre><li><pre><a href=#edda8f0fc4b67e247e2625ddf414446f6e7ab9ab0 id=mdda8f0fc4b67e247e2625ddf414446f6e7ab9ab0>*</a> <b>Re: [PATCH 00/14] KVM: arm64: Parallel stage-2 fault handling</b>
<b>@ 2022-09-09 10:01     ` Oliver Upton</b>
  <a href=#rdda8f0fc4b67e247e2625ddf414446f6e7ab9ab0>0 siblings, 0 replies; 96+ messages in thread</a>
From: Oliver Upton @ 2022-09-09 10:01 UTC (<a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/>permalink</a> / <a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/raw>raw</a>)
  To: Marc Zyngier
  Cc: kvm, Will Deacon, Catalin Marinas, Ben Gardon, David Matlack,
	Paolo Bonzini, kvmarm, linux-arm-kernel

Hey Marc,

On Tue, Sep 06, 2022 at 11:00:09AM +0100, Marc Zyngier wrote:

[...]

<span class=q>&gt; This fails to build on -rc4:
&gt; 
&gt;   MODPOST vmlinux.symvers
&gt;   MODINFO modules.builtin.modinfo
&gt;   GEN     modules.builtin
&gt;   CC      .vmlinux.export.o
&gt;   LD      .tmp_vmlinux.kallsyms1
&gt; ld: Unexpected GOT/PLT entries detected!
&gt; ld: Unexpected run-time procedure linkages detected!
&gt; ld: ID map text too big or misaligned
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_walk':
&gt; (.hyp.text+0xdc0c): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xdc1c): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_get_leaf':
&gt; (.hyp.text+0xdc80): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xdc90): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_hyp_map':
&gt; (.hyp.text+0xddb0): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xddc0): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_hyp_unmap':
&gt; (.hyp.text+0xde44): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xde50): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_hyp_destroy':
&gt; (.hyp.text+0xdf40): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xdf50): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_map':
&gt; (.hyp.text+0xe16c): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xe17c): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_set_owner':
&gt; (.hyp.text+0xe264): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xe274): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_unmap':
&gt; (.hyp.text+0xe2d4): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xe2e4): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_flush':
&gt; (.hyp.text+0xe5b4): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xe5c4): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; ld: arch/arm64/kvm/hyp/nvhe/kvm_nvhe.o: in function `__kvm_nvhe_kvm_pgtable_stage2_destroy':
&gt; (.hyp.text+0xe6f0): undefined reference to `__kvm_nvhe___rcu_read_lock'
&gt; ld: (.hyp.text+0xe700): undefined reference to `__kvm_nvhe___rcu_read_unlock'
&gt; make[3]: *** [Makefile:1169: vmlinux] Error 1
&gt; make[2]: *** [debian/rules:7: build-arch] Error 2
&gt; 
&gt; as this drags the RCU read-lock into EL2, and that's not going to
&gt; work... The following fixes it, but I wonder how you tested it.
</span>
Ugh. I was carrying a patch on top of my series to handle compilation
issues with rseq_test, I managed to squash the equivalent of below in
that patch.

Nonetheless, I *did* actually test it to get the numbers above :)

--
Thanks,
Oliver

<span class=q>&gt; diff --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
&gt; index dc839db86a1a..adf170122daf 100644
&gt; --- a/arch/arm64/include/asm/kvm_pgtable.h
&gt; +++ b/arch/arm64/include/asm/kvm_pgtable.h
&gt; @@ -580,7 +580,7 @@ enum kvm_pgtable_prot kvm_pgtable_stage2_pte_prot(kvm_pte_t pte);
&gt;   */
&gt;  enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte);
&gt;  
&gt; -#if defined(__KVM_NVHE_HYPERVISOR___)
&gt; +#if defined(__KVM_NVHE_HYPERVISOR__)
&gt;  
&gt;  static inline void kvm_pgtable_walk_begin(void) {}
&gt;  static inline void kvm_pgtable_walk_end(void) {}
&gt; 
&gt; -- 
&gt; Without deviation from the norm, progress is not possible.
</span>_______________________________________________
kvmarm mailing list
kvmarm@lists.cs.columbia.edu
<a href=https://lists.cs.columbia.edu/mailman/listinfo/kvmarm>https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</a>

<a href=#mdda8f0fc4b67e247e2625ddf414446f6e7ab9ab0 id=edda8f0fc4b67e247e2625ddf414446f6e7ab9ab0>^</a> <a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/>permalink</a> <a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/raw>raw</a> <a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/#R>reply</a>	[<a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/T/#u>flat</a>|<a href=https://lore.kernel.org/all/YxsPFltAMvls%2FA9n@google.com/t/#u><b>nested</b></a>] <a href=#rdda8f0fc4b67e247e2625ddf414446f6e7ab9ab0>96+ messages in thread</a></pre></ul></ul></ul><hr><pre>end of thread, other threads:[<a href="https://lore.kernel.org/all/?t=20221010040009">~2022-10-10  4:00 UTC</a> | <a href=https://lore.kernel.org/all/>newest</a>]

<b id=t>Thread overview:</b> 96+ messages (download: <a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/t.mbox.gz>mbox.gz</a> / follow: <a href=https://lore.kernel.org/all/20220830195036.964607-1-oliver.upton@linux.dev/t.atom>Atom feed</a>)
-- links below jump to the message on this page --
2022-08-30 19:41 <a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb id=rf4079d7dc9325e98db218fc0ba0c80e4866a66fb>[PATCH 00/14] KVM: arm64: Parallel stage-2 fault handling</a> Oliver Upton
2022-08-30 19:41 ` <a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb id=rf4079d7dc9325e98db218fc0ba0c80e4866a66fb>Oliver Upton</a>
2022-08-30 19:41 ` <a href=#mf4079d7dc9325e98db218fc0ba0c80e4866a66fb id=rf4079d7dc9325e98db218fc0ba0c80e4866a66fb>Oliver Upton</a>
2022-08-30 19:41 ` <a href=#m1892e0cea5df00fa40615e71920dd8d9ef48a4ad id=r1892e0cea5df00fa40615e71920dd8d9ef48a4ad>[PATCH 01/14] KVM: arm64: Add a helper to tear down unlinked stage-2 subtrees</a> Oliver Upton
2022-08-30 19:41   ` <a href=#m1892e0cea5df00fa40615e71920dd8d9ef48a4ad id=r1892e0cea5df00fa40615e71920dd8d9ef48a4ad>Oliver Upton</a>
2022-08-30 19:41   ` <a href=#m1892e0cea5df00fa40615e71920dd8d9ef48a4ad id=r1892e0cea5df00fa40615e71920dd8d9ef48a4ad>Oliver Upton</a>
2022-08-30 19:41 ` <a href=#mbc09837aeeb8bda33d35c82fe34163b1b3d4fc52 id=rbc09837aeeb8bda33d35c82fe34163b1b3d4fc52>[PATCH 02/14] KVM: arm64: Tear down unlinked stage-2 subtree after break-before-make</a> Oliver Upton
2022-08-30 19:41   ` <a href=#mbc09837aeeb8bda33d35c82fe34163b1b3d4fc52 id=rbc09837aeeb8bda33d35c82fe34163b1b3d4fc52>Oliver Upton</a>
2022-08-30 19:41   ` <a href=#mbc09837aeeb8bda33d35c82fe34163b1b3d4fc52 id=rbc09837aeeb8bda33d35c82fe34163b1b3d4fc52>Oliver Upton</a>
2022-09-06 14:35   ` <a href=#m842f491058e4efc7828eeedfd560576e55b54761 id=r842f491058e4efc7828eeedfd560576e55b54761>Quentin Perret</a>
2022-09-06 14:35     ` <a href=#m842f491058e4efc7828eeedfd560576e55b54761 id=r842f491058e4efc7828eeedfd560576e55b54761>Quentin Perret</a>
2022-09-06 14:35     ` <a href=#m842f491058e4efc7828eeedfd560576e55b54761 id=r842f491058e4efc7828eeedfd560576e55b54761>Quentin Perret</a>
2022-09-09 10:04     ` <a href=#m7c9be6b16683874a19186f163e3ebda03660861d id=r7c9be6b16683874a19186f163e3ebda03660861d>Oliver Upton</a>
2022-09-09 10:04       ` <a href=#m7c9be6b16683874a19186f163e3ebda03660861d id=r7c9be6b16683874a19186f163e3ebda03660861d>Oliver Upton</a>
2022-09-09 10:04       ` <a href=#m7c9be6b16683874a19186f163e3ebda03660861d id=r7c9be6b16683874a19186f163e3ebda03660861d>Oliver Upton</a>
2022-09-07 20:57   ` <a href=#mdeb735cd82e69f17c67ed4ddc2564466844b1de1 id=rdeb735cd82e69f17c67ed4ddc2564466844b1de1>David Matlack</a>
2022-09-07 20:57     ` <a href=#mdeb735cd82e69f17c67ed4ddc2564466844b1de1 id=rdeb735cd82e69f17c67ed4ddc2564466844b1de1>David Matlack</a>
2022-09-07 20:57     ` <a href=#mdeb735cd82e69f17c67ed4ddc2564466844b1de1 id=rdeb735cd82e69f17c67ed4ddc2564466844b1de1>David Matlack</a>
2022-09-09 10:07     ` <a href=#m7541819af8643527fcd1ce40f43b8e768a1d7f56 id=r7541819af8643527fcd1ce40f43b8e768a1d7f56>Oliver Upton</a>
2022-09-09 10:07       ` <a href=#m7541819af8643527fcd1ce40f43b8e768a1d7f56 id=r7541819af8643527fcd1ce40f43b8e768a1d7f56>Oliver Upton</a>
2022-09-09 10:07       ` <a href=#m7541819af8643527fcd1ce40f43b8e768a1d7f56 id=r7541819af8643527fcd1ce40f43b8e768a1d7f56>Oliver Upton</a>
2022-09-14  0:20   ` <a href=#m9afe0d55f1b5ee88addd9c7a3785153e6b774154 id=r9afe0d55f1b5ee88addd9c7a3785153e6b774154>Ricardo Koller</a>
2022-09-14  0:20     ` <a href=#m9afe0d55f1b5ee88addd9c7a3785153e6b774154 id=r9afe0d55f1b5ee88addd9c7a3785153e6b774154>Ricardo Koller</a>
2022-09-14  0:20     ` <a href=#m9afe0d55f1b5ee88addd9c7a3785153e6b774154 id=r9afe0d55f1b5ee88addd9c7a3785153e6b774154>Ricardo Koller</a>
2022-10-10  3:58     ` <a href=#mf2dd4c8a8568222f26d28c48d755f3b2d6a101d5 id=rf2dd4c8a8568222f26d28c48d755f3b2d6a101d5>Oliver Upton</a>
2022-10-10  3:58       ` <a href=#mf2dd4c8a8568222f26d28c48d755f3b2d6a101d5 id=rf2dd4c8a8568222f26d28c48d755f3b2d6a101d5>Oliver Upton</a>
2022-10-10  3:58       ` <a href=#mf2dd4c8a8568222f26d28c48d755f3b2d6a101d5 id=rf2dd4c8a8568222f26d28c48d755f3b2d6a101d5>Oliver Upton</a>
2022-08-30 19:41 ` <a href=#m5c21368d47ad2cf5f94a28750f6278efd3a8e11a id=r5c21368d47ad2cf5f94a28750f6278efd3a8e11a>[PATCH 03/14] KVM: arm64: Directly read owner id field in stage2_pte_is_counted()</a> Oliver Upton
2022-08-30 19:41   ` <a href=#m5c21368d47ad2cf5f94a28750f6278efd3a8e11a id=r5c21368d47ad2cf5f94a28750f6278efd3a8e11a>Oliver Upton</a>
2022-08-30 19:41   ` <a href=#m5c21368d47ad2cf5f94a28750f6278efd3a8e11a id=r5c21368d47ad2cf5f94a28750f6278efd3a8e11a>Oliver Upton</a>
2022-08-30 19:41 ` <a href=#m87296201935175ac920837e7c079530409989146 id=r87296201935175ac920837e7c079530409989146>[PATCH 04/14] KVM: arm64: Read the PTE once per visit</a> Oliver Upton
2022-08-30 19:41   ` <a href=#m87296201935175ac920837e7c079530409989146 id=r87296201935175ac920837e7c079530409989146>Oliver Upton</a>
2022-08-30 19:41   ` <a href=#m87296201935175ac920837e7c079530409989146 id=r87296201935175ac920837e7c079530409989146>Oliver Upton</a>
2022-08-30 19:41 ` <a href=#m0c83aa5eb3183e4da038ba8b166c16ec0318c6af id=r0c83aa5eb3183e4da038ba8b166c16ec0318c6af>[PATCH 05/14] KVM: arm64: Split init and set for table PTE</a> Oliver Upton
2022-08-30 19:41   ` <a href=#m0c83aa5eb3183e4da038ba8b166c16ec0318c6af id=r0c83aa5eb3183e4da038ba8b166c16ec0318c6af>Oliver Upton</a>
2022-08-30 19:41   ` <a href=#m0c83aa5eb3183e4da038ba8b166c16ec0318c6af id=r0c83aa5eb3183e4da038ba8b166c16ec0318c6af>Oliver Upton</a>
2022-08-30 19:41 ` <a href=#md064c93aacab4c28bae96834e85250c5d1b20665 id=rd064c93aacab4c28bae96834e85250c5d1b20665>[PATCH 06/14] KVM: arm64: Return next table from map callbacks</a> Oliver Upton
2022-08-30 19:41   ` <a href=#md064c93aacab4c28bae96834e85250c5d1b20665 id=rd064c93aacab4c28bae96834e85250c5d1b20665>Oliver Upton</a>
2022-08-30 19:41   ` <a href=#md064c93aacab4c28bae96834e85250c5d1b20665 id=rd064c93aacab4c28bae96834e85250c5d1b20665>Oliver Upton</a>
2022-09-07 21:32   ` <a href=#m6f8c8d7673199558c639367a6d59cc5a82236961 id=r6f8c8d7673199558c639367a6d59cc5a82236961>David Matlack</a>
2022-09-07 21:32     ` <a href=#m6f8c8d7673199558c639367a6d59cc5a82236961 id=r6f8c8d7673199558c639367a6d59cc5a82236961>David Matlack</a>
2022-09-07 21:32     ` <a href=#m6f8c8d7673199558c639367a6d59cc5a82236961 id=r6f8c8d7673199558c639367a6d59cc5a82236961>David Matlack</a>
2022-09-09  9:38     ` <a href=#mf81c90edc45528cf7dd4450f40e52c4c65567a9b id=rf81c90edc45528cf7dd4450f40e52c4c65567a9b>Oliver Upton</a>
2022-09-09  9:38       ` <a href=#mf81c90edc45528cf7dd4450f40e52c4c65567a9b id=rf81c90edc45528cf7dd4450f40e52c4c65567a9b>Oliver Upton</a>
2022-09-09  9:38       ` <a href=#mf81c90edc45528cf7dd4450f40e52c4c65567a9b id=rf81c90edc45528cf7dd4450f40e52c4c65567a9b>Oliver Upton</a>
2022-08-30 19:41 ` <a href=#m51d5975bbb33a8f80342da992689bae320bdb065 id=r51d5975bbb33a8f80342da992689bae320bdb065>[PATCH 07/14] KVM: arm64: Document behavior of pgtable visitor callback</a> Oliver Upton
2022-08-30 19:41   ` <a href=#m51d5975bbb33a8f80342da992689bae320bdb065 id=r51d5975bbb33a8f80342da992689bae320bdb065>Oliver Upton</a>
2022-08-30 19:41   ` <a href=#m51d5975bbb33a8f80342da992689bae320bdb065 id=r51d5975bbb33a8f80342da992689bae320bdb065>Oliver Upton</a>
2022-08-30 19:41 ` <a href=#m8317ace1fc52c8970cdc9375251ca31c97cef19e id=r8317ace1fc52c8970cdc9375251ca31c97cef19e>[PATCH 08/14] KVM: arm64: Protect page table traversal with RCU</a> Oliver Upton
2022-08-30 19:41   ` <a href=#m8317ace1fc52c8970cdc9375251ca31c97cef19e id=r8317ace1fc52c8970cdc9375251ca31c97cef19e>Oliver Upton</a>
2022-08-30 19:41   ` <a href=#m8317ace1fc52c8970cdc9375251ca31c97cef19e id=r8317ace1fc52c8970cdc9375251ca31c97cef19e>Oliver Upton</a>
2022-09-07 21:47   ` <a href=#m937ef34a7f44d4724cc32b00672074194178449e id=r937ef34a7f44d4724cc32b00672074194178449e>David Matlack</a>
2022-09-07 21:47     ` <a href=#m937ef34a7f44d4724cc32b00672074194178449e id=r937ef34a7f44d4724cc32b00672074194178449e>David Matlack</a>
2022-09-07 21:47     ` <a href=#m937ef34a7f44d4724cc32b00672074194178449e id=r937ef34a7f44d4724cc32b00672074194178449e>David Matlack</a>
2022-09-09  9:55     ` <a href=#ma09f4a08fe867eda2557c47d2ab6c40f959980d8 id=ra09f4a08fe867eda2557c47d2ab6c40f959980d8>Oliver Upton</a>
2022-09-09  9:55       ` <a href=#ma09f4a08fe867eda2557c47d2ab6c40f959980d8 id=ra09f4a08fe867eda2557c47d2ab6c40f959980d8>Oliver Upton</a>
2022-09-09  9:55       ` <a href=#ma09f4a08fe867eda2557c47d2ab6c40f959980d8 id=ra09f4a08fe867eda2557c47d2ab6c40f959980d8>Oliver Upton</a>
2022-08-30 19:41 ` <a href=#m10409b482b1166e85de5a4cc2b432e6de2e770c8 id=r10409b482b1166e85de5a4cc2b432e6de2e770c8>[PATCH 09/14] KVM: arm64: Free removed stage-2 tables in RCU callback</a> Oliver Upton
2022-08-30 19:41   ` <a href=#m10409b482b1166e85de5a4cc2b432e6de2e770c8 id=r10409b482b1166e85de5a4cc2b432e6de2e770c8>Oliver Upton</a>
2022-08-30 19:41   ` <a href=#m10409b482b1166e85de5a4cc2b432e6de2e770c8 id=r10409b482b1166e85de5a4cc2b432e6de2e770c8>Oliver Upton</a>
2022-09-07 22:00   ` <a href=#m6886d4365210102a4cbe3b8bf47c108818ab632a id=r6886d4365210102a4cbe3b8bf47c108818ab632a>David Matlack</a>
2022-09-07 22:00     ` <a href=#m6886d4365210102a4cbe3b8bf47c108818ab632a id=r6886d4365210102a4cbe3b8bf47c108818ab632a>David Matlack</a>
2022-09-07 22:00     ` <a href=#m6886d4365210102a4cbe3b8bf47c108818ab632a id=r6886d4365210102a4cbe3b8bf47c108818ab632a>David Matlack</a>
2022-09-08 16:40     ` <a href=#mdde2e85c111e7bf38027f66ddceab79cb9619dc0 id=rdde2e85c111e7bf38027f66ddceab79cb9619dc0>David Matlack</a>
2022-09-08 16:40       ` <a href=#mdde2e85c111e7bf38027f66ddceab79cb9619dc0 id=rdde2e85c111e7bf38027f66ddceab79cb9619dc0>David Matlack</a>
2022-09-08 16:40       ` <a href=#mdde2e85c111e7bf38027f66ddceab79cb9619dc0 id=rdde2e85c111e7bf38027f66ddceab79cb9619dc0>David Matlack</a>
2022-09-14  0:49   ` <a href=#m0151ebce75fd307e534ce3cf4350fe1455739219 id=r0151ebce75fd307e534ce3cf4350fe1455739219>Ricardo Koller</a>
2022-09-14  0:49     ` <a href=#m0151ebce75fd307e534ce3cf4350fe1455739219 id=r0151ebce75fd307e534ce3cf4350fe1455739219>Ricardo Koller</a>
2022-09-14  0:49     ` <a href=#m0151ebce75fd307e534ce3cf4350fe1455739219 id=r0151ebce75fd307e534ce3cf4350fe1455739219>Ricardo Koller</a>
2022-08-30 19:50 ` <a href=#mdeed2e1f6386a9a9fb93754ec5655a1618c425e1 id=rdeed2e1f6386a9a9fb93754ec5655a1618c425e1>[PATCH 10/14] KVM: arm64: Atomically update stage 2 leaf attributes in parallel walks</a> Oliver Upton
2022-08-30 19:50   ` <a href=#mdeed2e1f6386a9a9fb93754ec5655a1618c425e1 id=rdeed2e1f6386a9a9fb93754ec5655a1618c425e1>Oliver Upton</a>
2022-08-30 19:50   ` <a href=#mdeed2e1f6386a9a9fb93754ec5655a1618c425e1 id=rdeed2e1f6386a9a9fb93754ec5655a1618c425e1>Oliver Upton</a>
2022-08-30 19:51 ` <a href=#m12c4d59838e62fa01806ebbbd99b6882ebdbbcc2 id=r12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>[PATCH 11/14] KVM: arm64: Make changes block-&gt;table to leaf PTEs parallel-aware</a> Oliver Upton
2022-08-30 19:51   ` <a href=#m12c4d59838e62fa01806ebbbd99b6882ebdbbcc2 id=r12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>Oliver Upton</a>
2022-08-30 19:51   ` <a href=#m12c4d59838e62fa01806ebbbd99b6882ebdbbcc2 id=r12c4d59838e62fa01806ebbbd99b6882ebdbbcc2>Oliver Upton</a>
2022-09-14  0:51   ` <a href=#mee4bebe450c8627f2ff70393c2bd5400869ff940 id=ree4bebe450c8627f2ff70393c2bd5400869ff940>Ricardo Koller</a>
2022-09-14  0:51     ` <a href=#mee4bebe450c8627f2ff70393c2bd5400869ff940 id=ree4bebe450c8627f2ff70393c2bd5400869ff940>Ricardo Koller</a>
2022-09-14  0:51     ` <a href=#mee4bebe450c8627f2ff70393c2bd5400869ff940 id=ree4bebe450c8627f2ff70393c2bd5400869ff940>Ricardo Koller</a>
2022-09-14  0:53     ` <a href=#m77ade85e72dafda18c138e2b7a41ffd92979d92e id=r77ade85e72dafda18c138e2b7a41ffd92979d92e>Ricardo Koller</a>
2022-09-14  0:53       ` <a href=#m77ade85e72dafda18c138e2b7a41ffd92979d92e id=r77ade85e72dafda18c138e2b7a41ffd92979d92e>Ricardo Koller</a>
2022-09-14  0:53       ` <a href=#m77ade85e72dafda18c138e2b7a41ffd92979d92e id=r77ade85e72dafda18c138e2b7a41ffd92979d92e>Ricardo Koller</a>
2022-08-30 19:51 ` <a href=#m2631087360442636680e38cc623ed5c80244ee14 id=r2631087360442636680e38cc623ed5c80244ee14>[PATCH 12/14] KVM: arm64: Make leaf-&gt;leaf PTE changes parallel-aware</a> Oliver Upton
2022-08-30 19:51   ` <a href=#m2631087360442636680e38cc623ed5c80244ee14 id=r2631087360442636680e38cc623ed5c80244ee14>Oliver Upton</a>
2022-08-30 19:51   ` <a href=#m2631087360442636680e38cc623ed5c80244ee14 id=r2631087360442636680e38cc623ed5c80244ee14>Oliver Upton</a>
2022-08-30 19:51 ` <a href=#m430c5439f74caa60814b5ef2834d4cf4db1c2bdf id=r430c5439f74caa60814b5ef2834d4cf4db1c2bdf>[PATCH 13/14] KVM: arm64: Make table-&gt;block</a> " Oliver Upton
2022-08-30 19:51   ` <a href=#m430c5439f74caa60814b5ef2834d4cf4db1c2bdf id=r430c5439f74caa60814b5ef2834d4cf4db1c2bdf>Oliver Upton</a>
2022-08-30 19:51   ` <a href=#m430c5439f74caa60814b5ef2834d4cf4db1c2bdf id=r430c5439f74caa60814b5ef2834d4cf4db1c2bdf>Oliver Upton</a>
2022-08-30 19:52 ` <a href=#m529155fbc90b4c51cb58a327e8a79fc4e46d349b id=r529155fbc90b4c51cb58a327e8a79fc4e46d349b>[PATCH 14/14] KVM: arm64: Handle stage-2 faults in parallel</a> Oliver Upton
2022-08-30 19:52   ` <a href=#m529155fbc90b4c51cb58a327e8a79fc4e46d349b id=r529155fbc90b4c51cb58a327e8a79fc4e46d349b>Oliver Upton</a>
2022-08-30 19:52   ` <a href=#m529155fbc90b4c51cb58a327e8a79fc4e46d349b id=r529155fbc90b4c51cb58a327e8a79fc4e46d349b>Oliver Upton</a>
2022-09-06 10:00 ` <a href=#m22f22300af9fa61252f02fd205e1a7e7c6d99627 id=r22f22300af9fa61252f02fd205e1a7e7c6d99627>[PATCH 00/14] KVM: arm64: Parallel stage-2 fault handling</a> Marc Zyngier
2022-09-06 10:00   ` <a href=#m22f22300af9fa61252f02fd205e1a7e7c6d99627 id=r22f22300af9fa61252f02fd205e1a7e7c6d99627>Marc Zyngier</a>
2022-09-06 10:00   ` <a href=#m22f22300af9fa61252f02fd205e1a7e7c6d99627 id=r22f22300af9fa61252f02fd205e1a7e7c6d99627>Marc Zyngier</a>
2022-09-09 10:01   ` <a href=#mdda8f0fc4b67e247e2625ddf414446f6e7ab9ab0 id=rdda8f0fc4b67e247e2625ddf414446f6e7ab9ab0>Oliver Upton</a>
2022-09-09 10:01     ` <a href=#mdda8f0fc4b67e247e2625ddf414446f6e7ab9ab0 id=rdda8f0fc4b67e247e2625ddf414446f6e7ab9ab0>Oliver Upton</a>
2022-09-09 10:01     ` <a href=#mdda8f0fc4b67e247e2625ddf414446f6e7ab9ab0 id=rdda8f0fc4b67e247e2625ddf414446f6e7ab9ab0>Oliver Upton</a>
</pre><hr><pre>This is an external index of several public inboxes,
see <a href=https://lore.kernel.org/all/_/text/mirror/>mirroring instructions</a> on how to clone and mirror
all data and code used by this external index.</pre><div style=all:initial><div style=all:initial id=__hcfy__><template shadowrootmode=open><style class=sf-hidden>#root{-webkit-text-size-adjust:100%;box-sizing:border-box;font-size:14px;font-weight:400;letter-spacing:0;line-height:1.28581;text-transform:none;color:#182026;font-family:-apple-system,"BlinkMacSystemFont","Segoe UI","Roboto","Oxygen","Ubuntu","Cantarell","Open Sans","Helvetica Neue","Icons16",sans-serif;touch-action:manipulation}#root>.bp5-portal{z-index:9999999999}</style><style class=sf-hidden>#translate-panel{background-color:#f6f7f9;display:flex;flex-direction:column;padding-bottom:8px}.bp5-dark #translate-panel{background-color:#252a31}#translate-panel .fixed{flex-shrink:0;margin-bottom:10px}#translate-panel .body{flex-grow:1;overflow:auto;overscroll-behavior:contain}#translate-panel .body::-webkit-scrollbar{width:8px;background-color:rgba(0,0,0,0);-webkit-border-radius:100px}#translate-panel .body::-webkit-scrollbar:hover{background-color:rgba(0,0,0,.09)}#translate-panel .body::-webkit-scrollbar-thumb:vertical{background:rgba(0,0,0,.5);-webkit-border-radius:100px}#translate-panel .body::-webkit-scrollbar-thumb:vertical:active{background:rgba(0,0,0,.61);-webkit-border-radius:100px}#translate-panel.size-small,#translate-panel.size-small h6.bp5-heading,#translate-panel.size-small .bp5-control.bp5-large,#translate-panel.size-small textarea.bp5-input.bp5-small{font-size:14px}#translate-panel.size-small .phonetic-item,#translate-panel.size-small .quick-settings a{font-size:12px}#translate-panel.size-middle,#translate-panel.size-middle h6.bp5-heading,#translate-panel.size-middle .bp5-control.bp5-large,#translate-panel.size-middle textarea.bp5-input{font-size:18px}#translate-panel.size-middle .phonetic-item,#translate-panel.size-middle .quick-settings a{font-size:14px}#translate-panel.size-large,#translate-panel.size-large h6.bp5-heading,#translate-panel.size-large .bp5-control.bp5-large,#translate-panel.size-large textarea.bp5-input.bp5-large{font-size:22px}#translate-panel.size-large .source,#translate-panel.size-large .phonetic-item,#translate-panel.size-large .quick-settings a{font-size:18px}#translate-panel .bp5-button.bp5-small,#translate-panel .bp5-small .bp5-button{min-height:20px;min-width:20px}#translate-panel .header{display:flex;align-items:center;padding:4px 6px 4px 10px;border-bottom:1px solid #d1d1d1}.bp5-dark #translate-panel .header{border-bottom-color:rgba(17,20,24,.4)}#translate-panel .header .drag-block{min-width:5px;flex-shrink:0;flex-grow:1;align-self:stretch}#translate-panel .header .left{flex-shrink:0;display:flex}#translate-panel .header .right{flex-shrink:0;display:flex;align-items:center}#translate-panel .header .right .bp5-icon-arrow-right{flex-shrink:0;margin:0 5px}#translate-panel .header .right>.bp5-button{flex-shrink:0;margin:0 1px}#translate-panel .header .right>.bp5-button:last-child{margin-right:0}#translate-panel .quick-settings{padding:4px 9px;margin:0 1px}#translate-panel .quick-settings>div{margin-bottom:5px}#translate-panel .quick-settings .bp5-control{margin-bottom:0}#translate-panel .query-text{position:relative;padding:10px 10px 2px 10px}#translate-panel .query-text textarea.bp5-input{min-height:44px;font-family:system-ui,-apple-system,"Segoe UI","Roboto","Ubuntu","Cantarell","Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";overscroll-behavior:contain}#translate-panel .query-text .translate-btn{position:absolute;opacity:.6}#translate-panel .query-text .translate-btn:hover{opacity:1}#translate-panel .body{padding:0 10px}#translate-panel .body .bp5-card:first-child{margin-top:1px}#translate-panel .body .bp5-card:last-child{margin-bottom:1px}#translate-panel .body .no-api{margin:20px 0}.result-block{margin:8px 0;padding:2px 5px}.result-block .bp5-button{visibility:hidden}.result-block .error .bp5-button,.result-block:hover .bp5-button{visibility:visible}.result-block .legend{display:flex;align-items:center;justify-content:space-between}.result-block .legend .legend-left{display:flex;align-items:center}.result-block .legend .api-ico,.result-block .legend .bp5-heading{flex-shrink:0;white-space:nowrap}.result-block .legend .api-ico{display:inline-block;width:14px;height:14px;background-size:contain;margin-right:3px}.result-block .legend .bp5-heading{margin-bottom:0;margin-right:10px}.result-block .legend .source{cursor:pointer;font-size:12px;display:inline-flex;align-items:center}.result-block .legend .source .source-text{overflow:hidden}.result-block .legend .source .bp5-icon{position:relative;top:-1px;margin-left:1px}.result-block .phonetic{display:flex;flex-wrap:wrap}.result-block .phonetic .phonetic-item{display:flex;align-items:center;font-size:12px}.result-block .phonetic .phonetic-item:not(:last-child){margin-right:12px}.result-block .common-result p{line-height:1.3;margin:2px 0;overflow-wrap:break-word}.result-block .common-result .dict a{text-decoration:underline}.result-block .error{font-size:12px;padding:5px 10px}.result-block .dict-pos{margin-right:5px}.external-translators{margin-bottom:3px;padding:0;display:flex;flex-wrap:wrap}.external-translators>div{margin:0 5px 5px 0}.quick-links a{margin:0 5px 5px 0}#popper-container{width:250px;max-width:100%;position:absolute;left:0;top:0;z-index:9999999998;touch-action:none;transition:opacity .2s;background-color:#f6f7f9}.bp5-dark #popper-container{background-color:#252a31}#popper-container.show{opacity:1;pointer-events:auto;-moz-user-select:auto;user-select:auto}#popper-container,#popper-container[data-popper-reference-hidden=true]{opacity:0;pointer-events:none;-moz-user-select:none;user-select:none}#popper-container .drag-block{cursor:-webkit-grab;cursor:grab}#popper-container.pin{position:fixed}#popper-container.pin .arrow{display:none}#popper-container .arrow,#popper-container .arrow::before{position:absolute;width:8px;height:8px;z-index:-1}#popper-container .arrow::before{content:"";transform:rotate(45deg);background:#f6f7f9}.bp5-dark #popper-container .arrow::before{background-color:#252a31}#popper-container .arrow{display:none}#popper-container.show[data-popper-placement]:not([data-popper-reference-hidden=true]) .arrow{display:block}#popper-container[data-popper-placement^=top] .arrow{bottom:-5px}#popper-container[data-popper-placement^=top] .arrow::before{border-right:1px solid #d1d1d1;border-bottom:1px solid #d1d1d1}#popper-container[data-popper-placement^=bottom] .arrow{top:-5px}#popper-container[data-popper-placement^=bottom] .arrow::before{border-left:1px solid #d1d1d1;border-top:1px solid #d1d1d1}#popper-container[data-popper-placement^=left] .arrow{right:-5px}#popper-container[data-popper-placement^=left] .arrow::before{border-right:1px solid #d1d1d1;border-top:1px solid #d1d1d1}#popper-container[data-popper-placement^=right] .arrow{left:-5px}#popper-container[data-popper-placement^=right] .arrow::before{border-left:1px solid #d1d1d1;border-bottom:1px solid #d1d1d1}#translate-btn{display:none;position:absolute;z-index:9999999999;left:0;top:0}#translate-btn .bp5-button{padding:2px;min-width:0;min-height:0}#translate-btn .btn-icon{width:18px;height:18px;background-image:url(moz-extension://72d6ca68-5609-1440-b1ba-15daf8cbdb2d/logo.png);background-size:contain}.bp5-dark #translate-btn .btn-icon{background-image:url(moz-extension://72d6ca68-5609-1440-b1ba-15daf8cbdb2d/logowhite36.png)}#translate-btn.show{display:block}.translate-type-selector .bp5-label{display:inline}.translate-type-selector .bp5-radio{margin-bottom:0}#ocr-container{position:fixed;z-index:99999999999999;left:0;top:0;right:0;bottom:0}#ocr-container .toolbar{display:none;position:absolute;z-index:1}#ocr-container img{max-width:100%}#app{cursor:default}.switch-pin{flex-shrink:0;cursor:pointer}.switch-pin .bp5-icon-pin{transition:transform .2s,color .2s;transform:rotate(-45deg)}.pin .switch-pin .bp5-icon-pin{transform:rotate(-70deg)}.cut-btn{margin-left:2px}.app-toaster-container{position:fixed!important;z-index:9999999999!important}.app-toaster-container .bp5-toast{min-width:auto}#web-trs-panel .app-toaster-container{padding-right:0;padding-left:0}#web-trs-panel .page-trs-form-group{margin:0 0 0 0;align-items:center}#web-trs-panel .page-trs-form-group>label{width:70px}</style><div id=root dir=ltr class=bp5-dark><div id=app class=bp5-dark><div id=translate-btn class=sf-hidden></div><div id=popper-container style=width:250px;transform:translate(0px) class=bp5-elevation-4><div id=translate-panel class=size-small><div class=fixed><div class=header><div class=left><div class=switch-pin><button type=button class="bp5-button bp5-minimal bp5-small"><span aria-hidden=true class="bp5-icon bp5-icon-pin"><svg data-icon=pin height=14 role=img viewBox="0 0 16 16" width=14><path d="M9.41.92c-.51.51-.41 1.5.15 2.56L4.34 7.54C2.8 6.48 1.45 6.05.92 6.58l3.54 3.54-3.54 4.95 4.95-3.54 3.54 3.54c.53-.53.1-1.88-.96-3.42l4.06-5.22c1.06.56 2.04.66 2.55.15L9.41.92z" fill-rule=evenodd></path></svg></span></button></div><button type=button class="bp5-button bp5-minimal bp5-small cut-btn" title=截图翻译><span aria-hidden=true class="bp5-icon bp5-icon-cut"><svg data-icon=cut height=14 role=img viewBox="0 0 16 16" width=14><path d="M13 2s.71-1.29 0-2L8.66 5.07l1.05 1.32L13 2zm.07 8c-.42 0-.82.09-1.18.26L3.31 0c-.69.71 0 2 0 2l3.68 5.02-2.77 3.24A2.996 2.996 0 000 13c0 1.66 1.34 3 3 3s3-1.34 3-3c0-.46-.11-.89-.29-1.27L8.1 8.54l2.33 3.19c-.18.39-.29.82-.29 1.27 0 1.66 1.31 3 2.93 3S16 14.66 16 13s-1.31-3-2.93-3zM3 14c-.55 0-1-.45-1-1s.45-1 1-1 1 .45 1 1-.45 1-1 1zm10.07 0c-.54 0-.98-.45-.98-1s.44-1 .98-1 .98.45.98 1-.44 1-.98 1z" fill-rule=evenodd></path></svg></span></button><button type=button class="bp5-button bp5-minimal bp5-small cut-btn" title=网页全文翻译><span aria-hidden=true class="bp5-icon bp5-icon-translate"><svg data-icon=translate height=14 role=img viewBox="0 0 16 16" width=14><path d="M15.89 14.56l-3.99-8h-.01c-.17-.33-.5-.56-.89-.56s-.72.23-.89.56h-.01L9 8.76 7.17 7.38l.23-.18C8.37 6.47 9 5.31 9 4V3h1c.55 0 1-.45 1-1s-.45-1-1-1H7c0-.55-.45-1-1-1H5c-.55 0-1 .45-1 1H1c-.55 0-1 .45-1 1s.45 1 1 1h6v1c0 .66-.32 1.25-.82 1.61l-.68.51-.68-.5C4.32 5.25 4 4.66 4 4H2c0 1.31.63 2.47 1.6 3.2l.23.17L1.4 9.2l.01.01C1.17 9.4 1 9.67 1 10c0 .55.45 1 1 1 .23 0 .42-.09.59-.21l.01.01 2.9-2.17 2.6 1.95-1.99 3.98h.01c-.07.13-.12.28-.12.44 0 .55.45 1 1 1 .39 0 .72-.23.89-.56h.01L8.62 14h4.76l.72 1.45h.01c.17.32.5.55.89.55.55 0 1-.45 1-1 0-.16-.05-.31-.11-.44zM9.62 12L11 9.24 12.38 12H9.62z" fill-rule=evenodd></path></svg></span></button><button type=button class="bp5-button bp5-minimal bp5-small cut-btn" title=音视频翻译 style=margin-right:2px><span aria-hidden=true class="bp5-icon bp5-icon-video"><svg data-icon=video height=14 role=img viewBox="0 0 16 16" width=14><path d="M15 2H1c-.55 0-1 .45-1 1v10c0 .55.45 1 1 1h14c.55 0 1-.45 1-1V3c0-.55-.45-1-1-1zM5 11V5l6 3-6 3z" fill-rule=evenodd></path></svg></span></button><button type=button title=图片翻译 class="bp5-button bp5-minimal bp5-small"><span aria-hidden=true class="bp5-icon bp5-icon-media"><svg data-icon=media height=14 role=img viewBox="0 0 16 16" width=14><path d="M11.99 6.99c.55 0 1-.45 1-1s-.45-1-1-1-1 .45-1 1 .45 1 1 1zm3-5h-14c-.55 0-1 .45-1 1v10c0 .55.45 1 1 1h14c.55 0 1-.45 1-1v-10c0-.55-.45-1-1-1zm-1 9l-5-3-1 2-3-4-3 5v-7h12v7z" fill-rule=evenodd></path></svg></span></button><button type=button title=语音翻译 class="bp5-button bp5-minimal bp5-small"><span class=bp5-icon><svg version=1.1 id=Capa_1 width=14 height=14 xmlns=http://www.w3.org/2000/svg xmlns:xlink=http://www.w3.org/1999/xlink x=0px y=0px viewBox="0 0 490.9 490.9" xml:space=preserve><g><g><path d="M245.5,322.9c53,0,96.2-43.2,96.2-96.2V96.2c0-53-43.2-96.2-96.2-96.2s-96.2,43.2-96.2,96.2v130.5 C149.3,279.8,192.5,322.9,245.5,322.9z M173.8,96.2c0-39.5,32.2-71.7,71.7-71.7s71.7,32.2,71.7,71.7v130.5 c0,39.5-32.2,71.7-71.7,71.7s-71.7-32.2-71.7-71.7V96.2z"></path><path d="M94.4,214.5c-6.8,0-12.3,5.5-12.3,12.3c0,85.9,66.7,156.6,151.1,162.8v76.7h-63.9c-6.8,0-12.3,5.5-12.3,12.3 s5.5,12.3,12.3,12.3h152.3c6.8,0,12.3-5.5,12.3-12.3s-5.5-12.3-12.3-12.3h-63.9v-76.7c84.4-6.3,151.1-76.9,151.1-162.8 c0-6.8-5.5-12.3-12.3-12.3s-12.3,5.5-12.3,12.3c0,76.6-62.3,138.9-138.9,138.9s-138.9-62.3-138.9-138.9 C106.6,220,101.2,214.5,94.4,214.5z"></path></g></g></svg></span></button></div><div class=drag-block title=按住不放可以拖动></div><div class=right><button type=button disabled title=添加到收藏夹 class="bp5-button bp5-disabled bp5-minimal bp5-small" tabindex=-1><span aria-hidden=true class="bp5-icon bp5-icon-star-empty"><svg data-icon=star-empty height=14 role=img viewBox="0 0 16 16" width=14><path d="M16 6.11l-5.53-.84L8 0 5.53 5.27 0 6.11l4 4.1L3.06 16 8 13.27 12.94 16 12 10.21l4-4.1zM4.91 13.2l.59-3.62L3 7.02l3.45-.53L8 3.2l1.55 3.29 3.45.53-2.5 2.56.59 3.62L8 11.49 4.91 13.2z" fill-rule=evenodd></path></svg></span></button><button type=button class="bp5-button bp5-minimal bp5-small settings" title=快捷设置><span aria-hidden=true class="bp5-icon bp5-icon-cog"><svg data-icon=cog height=14 role=img viewBox="0 0 16 16" width=14><path d="M15.19 6.39h-1.85c-.11-.37-.27-.71-.45-1.04l1.36-1.36c.31-.31.31-.82 0-1.13l-1.13-1.13a.803.803 0 00-1.13 0l-1.36 1.36c-.33-.17-.67-.33-1.04-.44V.79c0-.44-.36-.8-.8-.8h-1.6c-.44 0-.8.36-.8.8v1.86c-.39.12-.75.28-1.1.47l-1.3-1.3c-.3-.3-.79-.3-1.09 0L1.82 2.91c-.3.3-.3.79 0 1.09l1.3 1.3c-.2.34-.36.7-.48 1.09H.79c-.44 0-.8.36-.8.8v1.6c0 .44.36.8.8.8h1.85c.11.37.27.71.45 1.04l-1.36 1.36c-.31.31-.31.82 0 1.13l1.13 1.13c.31.31.82.31 1.13 0l1.36-1.36c.33.18.67.33 1.04.44v1.86c0 .44.36.8.8.8h1.6c.44 0 .8-.36.8-.8v-1.86c.39-.12.75-.28 1.1-.47l1.3 1.3c.3.3.79.3 1.09 0l1.09-1.09c.3-.3.3-.79 0-1.09l-1.3-1.3c.19-.35.36-.71.48-1.1h1.85c.44 0 .8-.36.8-.8v-1.6a.816.816 0 00-.81-.79zm-7.2 4.6c-1.66 0-3-1.34-3-3s1.34-3 3-3 3 1.34 3 3-1.34 3-3 3z" fill-rule=evenodd></path></svg></span></button><button type=button title=关闭(Esc) class="bp5-button bp5-minimal bp5-small"><span aria-hidden=true class="bp5-icon bp5-icon-cross"><svg data-icon=cross height=18 role=img viewBox="0 0 16 16" width=18><path d="M9.41 8l3.29-3.29c.19-.18.3-.43.3-.71a1.003 1.003 0 00-1.71-.71L8 6.59l-3.29-3.3a1.003 1.003 0 00-1.42 1.42L6.59 8 3.3 11.29c-.19.18-.3.43-.3.71a1.003 1.003 0 001.71.71L8 9.41l3.29 3.29c.18.19.43.3.71.3a1.003 1.003 0 00.71-1.71L9.41 8z" fill-rule=evenodd></path></svg></span></button></div></div><div class=bp5-collapse><div class="bp5-collapse-body sf-hidden" aria-hidden=true></div></div></div><div class=body><p>请输入需要翻译的文本。</p></div></div><div class="arrow sf-hidden"></div></div><div id=web-trs-panel></div></div></div></template></div></div><script data-template-shadow-root>(()=>{document.currentScript.remove();processNode(document);function processNode(node){node.querySelectorAll("template[shadowrootmode]").forEach(element=>{let shadowRoot = element.parentElement.shadowRoot;if (!shadowRoot) {try {shadowRoot=element.parentElement.attachShadow({mode:element.getAttribute("shadowrootmode")});shadowRoot.innerHTML=element.innerHTML;element.remove()} catch (error) {} if (shadowRoot) {processNode(shadowRoot)}}})}})()</script>