<!DOCTYPE html> <html xmlns=http://www.w3.org/1999/xhtml lang=en style><!--
 Page saved with SingleFile 
 url: https://patchwork.kernel.org/project/linux-riscv/cover/20231003044403.1974628-1-apatel@ventanamicro.com/#25566109 
 saved date: Sun Jan 19 2025 10:11:47 GMT+0800 (中国标准时间)
--><meta charset=utf-8>
<title>[v10,00/15] Linux RISC-V AIA Support - Patchwork</title>
<style>/*!
 * Bootstrap v3.2.0 (http://getbootstrap.com)
 * Copyright 2011-2014 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 *//*! normalize.css v3.0.1 | MIT License | git.io/normalize */html{font-family:sans-serif;-webkit-text-size-adjust:100%;-ms-text-size-adjust:100%}body{margin:0}nav{display:block}template{display:none}a{background:0 0}a:active,a:hover{outline:0}h1{margin:.67em 0}svg:not(:root){overflow:hidden}pre{overflow:auto}button{margin:0;font:inherit;color:inherit}button{overflow:visible}button{text-transform:none}button{-webkit-appearance:button;cursor:pointer}button[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{padding:0;border:0}table{border-spacing:0}@font-face{font-family:"Glyphicons Halflings";src:url(data:font/woff;base64,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)format("woff")}.glyphicon{position:relative;top:1px;display:inline-block;font-family:"Glyphicons Halflings";font-style:normal;font-weight:400;line-height:1;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}.glyphicon-file:before{content:""}.glyphicon-info-sign:before{content:""}.glyphicon-gift:before{content:""}*{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box}:before,:after{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box}html{font-size:10px;-webkit-tap-highlight-color:rgba(0,0,0,0)}body{font-family:"Helvetica Neue",Helvetica,Arial,sans-serif;font-size:14px;line-height:1.42857143;color:#333;background-color:#fff}button{font-family:inherit;font-size:inherit;line-height:inherit}a{color:#428bca;text-decoration:none}a:hover,a:focus{color:#2a6496;text-decoration:underline}a:focus{outline:thin dotted;outline:5px auto -webkit-focus-ring-color;outline-offset:-2px}h1,h2{font-family:inherit;font-weight:500;line-height:1.1;color:inherit}h1,h2{margin-top:20px;margin-bottom:10px}h1{font-size:36px}p{margin:0 0 10px}@media (min-width:768px){}ul{margin-top:0}@media (min-width:768px){}pre{font-family:Menlo,Monaco,Consolas,"Courier New",monospace}pre{display:block;padding:9.5px;margin:0 0 10px;font-size:13px;color:#333;word-break:break-all;word-wrap:break-word}@media (min-width:768px){}@media (min-width:992px){}@media (min-width:1200px){}.container-fluid{padding-right:15px;padding-left:15px;margin-right:auto;margin-left:auto}@media (min-width:768px){}@media (min-width:992px){}@media (min-width:1200px){}table{background-color:transparent}@media screen and (max-width:767px){}@media (min-width:768px){}@media (min-width:768px){}@media (min-width:768px){}@media (min-width:768px){}.btn{display:inline-block;padding:6px 12px;margin-bottom:0;font-size:14px;font-weight:400;line-height:1.42857143;text-align:center;white-space:nowrap;vertical-align:middle;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-image:none;border:1px solid transparent;border-radius:4px}.btn:focus,.btn:active:focus,.btn.active:focus{outline:thin dotted;outline:5px auto -webkit-focus-ring-color;outline-offset:-2px}.btn:hover,.btn:focus{color:#333;text-decoration:none}.btn:active,.btn.active{background-image:none;outline:0;-webkit-box-shadow:inset 0 3px 5px rgba(0,0,0,.125);box-shadow:inset 0 3px 5px rgba(0,0,0,.125)}.btn-default{color:#333;background-color:#fff;border-color:#ccc}.btn-default:hover,.btn-default:focus,.btn-default:active,.btn-default.active,.open>.dropdown-toggle.btn-default{color:#333;background-color:#e6e6e6;border-color:#adadad}.btn-default:active,.btn-default.active,.open>.dropdown-toggle.btn-default{background-image:none}.collapse{display:none}@media (min-width:768px){}.btn-group{position:relative;display:inline-block;vertical-align:middle}.btn-group>.btn{position:relative;float:left}.btn-group>.btn:hover,.btn-group-vertical>.btn:hover,.btn-group>.btn:focus,.btn-group-vertical>.btn:focus,.btn-group>.btn:active,.btn-group-vertical>.btn:active,.btn-group>.btn.active,.btn-group-vertical>.btn.active{z-index:2}.btn-group>.btn:focus,.btn-group-vertical>.btn:focus{outline:0}.btn-group .btn+.btn{margin-left:-1px}.btn-group>.btn:not(:first-child):not(:last-child):not(.dropdown-toggle){border-radius:0}.btn-group>.btn:first-child{margin-left:0}.btn-group>.btn:first-child:not(:last-child):not(.dropdown-toggle){border-top-right-radius:0;border-bottom-right-radius:0}.btn-group>.btn:last-child:not(:first-child),.btn-group>.dropdown-toggle:not(:first-child){border-top-left-radius:0;border-bottom-left-radius:0}.nav{padding-left:0;margin-bottom:0;list-style:none}.nav>li{position:relative;display:block}.nav>li>a{position:relative;display:block;padding:10px 15px}.nav>li>a:hover,.nav>li>a:focus{text-decoration:none;background-color:#eee}@media (min-width:768px){}@media (min-width:768px){}@media (min-width:768px){}@media (min-width:768px){}.navbar{position:relative;min-height:50px;margin-bottom:20px;border:1px solid transparent}@media (min-width:768px){.navbar{border-radius:4px}}@media (min-width:768px){.navbar-header{float:left}}.navbar-collapse{padding-right:15px;padding-left:15px;overflow-x:visible;-webkit-overflow-scrolling:touch;border-top:1px solid transparent;-webkit-box-shadow:inset 0 1px 0 rgba(255,255,255,.1);box-shadow:inset 0 1px 0 rgba(255,255,255,.1)}@media (min-width:768px){.navbar-collapse{width:auto;border-top:0;-webkit-box-shadow:none;box-shadow:none}.navbar-collapse.collapse{display:block!important;height:auto!important;padding-bottom:0;overflow:visible!important}.navbar-static-top .navbar-collapse{padding-right:0;padding-left:0}}@media (max-width:480px) and (orientation:landscape){}.container-fluid>.navbar-header,.container-fluid>.navbar-collapse{margin-right:-15px;margin-left:-15px}@media (min-width:768px){.container-fluid>.navbar-header,.container-fluid>.navbar-collapse{margin-right:0;margin-left:0}}.navbar-static-top{z-index:1000;border-width:0 0 1px}@media (min-width:768px){.navbar-static-top{border-radius:0}}@media (min-width:768px){}.navbar-brand{float:left;height:50px;padding:15px 15px;font-size:18px;line-height:20px}.navbar-brand:hover,.navbar-brand:focus{text-decoration:none}@media (min-width:768px){.navbar>.container-fluid .navbar-brand{margin-left:-15px}}@media (min-width:768px){}.navbar-nav{margin:7.5px -15px}.navbar-nav>li>a{padding-top:10px;padding-bottom:10px;line-height:20px}@media (max-width:767px){}@media (min-width:768px){.navbar-nav{float:left;margin:0}.navbar-nav>li{float:left}.navbar-nav>li>a{padding-top:15px;padding-bottom:15px}.navbar-nav.navbar-right:last-child{margin-right:-15px}}@media (min-width:768px){.navbar-right{float:right!important}}@media (min-width:768px){}@media (max-width:767px){}@media (min-width:768px){}@media (min-width:768px){}@media (max-width:767px){}.navbar-inverse{background-color:#222;border-color:#080808}.navbar-inverse .navbar-brand{color:#777}.navbar-inverse .navbar-brand:hover,.navbar-inverse .navbar-brand:focus{color:#fff;background-color:transparent}.navbar-inverse .navbar-nav>li>a:hover,.navbar-inverse .navbar-nav>li>a:focus{color:#fff;background-color:transparent}.navbar-inverse .navbar-collapse{border-color:#101010}@media (max-width:767px){}@media screen and (min-width:768px){}@-webkit-keyframes progress-bar-stripes{from{background-position:40px 0}to{background-position:0 0}}@-o-keyframes progress-bar-stripes{from{background-position:40px 0}to{background-position:0 0}}@keyframes progress-bar-stripes{from{background-position:40px 0}to{background-position:0 0}}@media (min-width:768px){}@media (min-width:992px){}@media screen and (min-width:768px){}.clearfix:before,.clearfix:after,.dl-horizontal dd:before,.dl-horizontal dd:after,.container:before,.container:after,.container-fluid:before,.container-fluid:after,.row:before,.row:after,.form-horizontal .form-group:before,.form-horizontal .form-group:after,.btn-toolbar:before,.btn-toolbar:after,.btn-group-vertical>.btn-group:before,.btn-group-vertical>.btn-group:after,.nav:before,.nav:after,.navbar:before,.navbar:after,.navbar-header:before,.navbar-header:after,.navbar-collapse:before,.navbar-collapse:after,.pager:before,.pager:after,.panel-body:before,.panel-body:after,.modal-footer:before,.modal-footer:after{display:table;content:" "}.clearfix:after,.dl-horizontal dd:after,.container:after,.container-fluid:after,.row:after,.form-horizontal .form-group:after,.btn-toolbar:after,.btn-group-vertical>.btn-group:after,.nav:after,.navbar:after,.navbar-header:after,.navbar-collapse:after,.pager:after,.panel-body:after,.modal-footer:after{clear:both}.pull-right{float:right!important}@-ms-viewport{width:device-width}@media (max-width:767px){}@media (max-width:767px){}@media (max-width:767px){}@media (max-width:767px){}@media (min-width:768px) and (max-width:991px){}@media (min-width:768px) and (max-width:991px){}@media (min-width:768px) and (max-width:991px){}@media (min-width:768px) and (max-width:991px){}@media (min-width:992px) and (max-width:1199px){}@media (min-width:992px) and (max-width:1199px){}@media (min-width:992px) and (max-width:1199px){}@media (min-width:992px) and (max-width:1199px){}@media (min-width:1200px){}@media (min-width:1200px){}@media (min-width:1200px){}@media (min-width:1200px){}@media (max-width:767px){}@media (min-width:768px) and (max-width:991px){}@media (min-width:992px) and (max-width:1199px){}@media (min-width:1200px){}</style>
<style>h2{font-size:25px;margin:18px 0 18px 0}pre{line-height:110%;background-color:white;border-radius:0}.navbar-inverse .navbar-nav>li>a{color:#999}.navbar-subbrand{float:left;height:50px;padding:15px 15px 15px 0;font-size:18px;line-height:20px}.navbar-subbrand{color:#999}table{border-collapse:collapse}#footer{padding:1em;font-size:small;text-align:center;color:#909090}#footer a{color:#909090}div.patchforms{margin-top:1em}table.patchmeta tr th,table.patchmeta tr td{text-align:left;padding:1px 10px;vertical-align:top}.comment .meta{background:#f0f0f0;padding:0.3em 0.5em}.comment .content{border:0}.quote{color:#007f00}</style>
<!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js">
    </script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/es5-shim/2.0.8/es5-shim.min.js"></script>
  <![endif]-->
<meta name=referrer content=no-referrer><style>.sf-hidden{display:none!important}</style><link rel=canonical href=https://patchwork.kernel.org/project/linux-riscv/cover/20231003044403.1974628-1-apatel@ventanamicro.com/#25566109><meta http-equiv=content-security-policy content="default-src 'none'; font-src 'self' data:; img-src 'self' data:; style-src 'unsafe-inline'; media-src 'self' data:; script-src 'unsafe-inline' data:; object-src 'self' data:; frame-src 'self' data:;"></head>
 <body>
 <nav class="navbar navbar-inverse navbar-static-top">
 <div class=container-fluid>
 <div class=navbar-header>
 <button type=button class="navbar-toggle collapsed sf-hidden" data-toggle=collapse data-target=#navbar-collapse>
 
 
 
 
 </button>
 <a class=navbar-brand href=https://patchwork.kernel.org/>Patchwork</a>
 <span class=navbar-subbrand>
 Linux RISC-V
 </span>
 </div>
 <div class="collapse navbar-collapse" id=navbar-collapse>
 <ul class="nav navbar-nav">
 <li>
 <a href=https://patchwork.kernel.org/project/linux-riscv/list/>
 <span class="glyphicon glyphicon-file"></span>
 Patches
 </a>
 </li>
 <li>
 <a href=https://patchwork.kernel.org/project/linux-riscv/bundles/>
 <span class="glyphicon glyphicon-gift"></span>
 Bundles
 </a>
 </li>
 <li>
 <a href=https://patchwork.kernel.org/project/linux-riscv/>
 <span class="glyphicon glyphicon-info-sign"></span>
 About this project
 </a>
 </li>
 </ul>
 <ul class="nav navbar-nav navbar-right">
 <li><a href=https://patchwork.kernel.org/user/login/>Login</a></li>
 <li><a href=https://patchwork.kernel.org/register/>Register</a></li>
 <li><a href=https://patchwork.kernel.org/mail/>Mail settings</a></li>
 </ul>
 </div>
 </div>
 </nav>
 <div class=container-fluid>
<div>
 <div class="btn-group pull-right">
 <button type=button class="btn btn-default btn-copy" data-clipboard-text=13406749 title="Copy to Clipboard">
 13406749
 </button>
 
 <a href=https://patchwork.kernel.org/project/linux-riscv/cover/20231003044403.1974628-1-apatel@ventanamicro.com/mbox/ class="btn btn-default" role=button title="Download cover mbox">mbox</a>
 
 
 <a href=https://patchwork.kernel.org/series/789461/mbox/ class="btn btn-default" role=button title="Download patch mbox with dependencies">series</a>
 
</div>
 <h1>[v10,00/15] Linux RISC-V AIA Support</h1>
</div>
<table class=patchmeta>
 <tbody><tr>
 <th>Message ID</th>
 
 <td>20231003044403.1974628-1-apatel@ventanamicro.com (<a href=https://lore.kernel.org/r/20231003044403.1974628-1-apatel@ventanamicro.com>mailing list archive</a>)</td>
 
 </tr>
 <tr>
 <th>Headers</th>
 <td><a id=togglepatchheaders href=javascript:void(0)>show</a>
 <div id=patchheaders class=patchheaders style=display:none>
 
 </div>
 </td>
 </tr>
 <tr>
 <th>Series</th>
 <td>
 <a href="https://patchwork.kernel.org/project/linux-riscv/list/?series=789461">
 Linux RISC-V AIA Support
 </a> |
 <a id=togglepatchseries href=javascript:void(0)>expand</a>
 <div id=patchseries class=submissionlist style=display:none>
 
 </div>
 </td>
 </tr>
</table>
<div class=patchforms>
 <div style=clear:both>
 </div>
</div>
<h2>Message</h2>
<div class=comment>
<div class=meta>
 <span><a href="https://patchwork.kernel.org/project/linux-riscv/list/?submitter=204452">Anup Patel</a></span>
 <span class=pull-right>Oct. 3, 2023, 4:43 a.m. UTC</span>
</div>
<pre class=content>The RISC-V AIA specification is ratified as-per the RISC-V international
process. The latest ratified AIA specifcation can be found at:
https://github.com/riscv/riscv-aia/releases/download/1.0/riscv-interrupts-1.0.pdf

At a high-level, the AIA specification adds three things:
1) AIA CSRs
   - Improved local interrupt support
2) Incoming Message Signaled Interrupt Controller (IMSIC)
   - Per-HART MSI controller
   - Support MSI virtualization
   - Support IPI along with virtualization
3) Advanced Platform-Level Interrupt Controller (APLIC)
   - Wired interrupt controller
   - In MSI-mode, converts wired interrupt into MSIs (i.e. MSI generator)
   - In Direct-mode, injects external interrupts directly into HARTs

For an overview of the AIA specification, refer the AIA virtualization
talk at KVM Forum 2022:
https://static.sched.com/hosted_files/kvmforum2022/a1/AIA_Virtualization_in_KVM_RISCV_final.pdf
https://www.youtube.com/watch?v=r071dL8Z0yo

To test this series, use QEMU v7.2 (or higher) and OpenSBI v1.2 (or higher).

These patches can also be found in the riscv_aia_v10 branch at:
https://github.com/avpatel/linux.git

Changes since v9:
 - Rebased on Linux-6.6-rc4
 - Use builtin_platform_driver() in PATCH5, PATCH9, and PATCH12

Changes since v8:
 - Rebased on Linux-6.6-rc3
 - Dropped PATCH2 of v8 series since we won't be requiring
   riscv_get_intc_hartid() based on Marc Z's comments on ACPI AIA support.
 - Addressed Saravana's comments in PATCH3 of v8 series
 - Update PATCH9 and PATCH13 of v8 series based on comments from Sunil

Changes since v7:
 - Rebased on Linux-6.6-rc1
 - Addressed comments on PATCH1 of v7 series and split it into two PATCHes
 - Use DEFINE_SIMPLE_PROP() in PATCH2 of v7 series

Changes since v6:
 - Rebased on Linux-6.5-rc4
 - Updated PATCH2 to use IS_ENABLED(CONFIG_SPARC) instead of
   !IS_ENABLED(CONFIG_OF_IRQ)
 - Added new PATCH4 to fix syscore registration in PLIC driver
 - Update PATCH5 to convert PLIC driver into full-blown platform driver
   with a re-written probe function.

Changes since v5:
 - Rebased on Linux-6.5-rc2
 - Updated the overall series to ensure that only IPI, timer, and
   INTC drivers are probed very early whereas rest of the interrupt
   controllers (such as PLIC, APLIC, and IMISC) are probed as
   regular platform drivers.
 - Renamed riscv_fw_parent_hartid() to riscv_get_intc_hartid()
 - New PATCH1 to add fw_devlink support for msi-parent DT property
 - New PATCH2 to ensure all INTC suppliers are initialized which in-turn
   fixes the probing issue for PLIC, APLIC and IMSIC as platform driver
 - New PATCH3 to use platform driver probing for PLIC
 - Re-structured the IMSIC driver into two separate drivers: early and
   platform. The IMSIC early driver (PATCH7) only initialized IMSIC state
   and provides IPIs whereas the IMSIC platform driver (PATCH8) is probed
   provides MSI domain for platform devices.
 - Re-structure the APLIC platform driver into three separe sources: main,
   direct mode, and MSI mode.

Changes since v4:
 - Rebased on Linux-6.5-rc1
 - Added "Dependencies" in the APLIC bindings (PATCH6 in v4)
 - Dropped the PATCH6 which was changing the IOMMU DMA domain APIs
 - Dropped use of IOMMU DMA APIs in the IMSIC driver (PATCH4)

Changes since v3:
 - Rebased on Linux-6.4-rc6
 - Droped PATCH2 of v3 series instead we now set FWNODE_FLAG_BEST_EFFORT via
   IRQCHIP_DECLARE()
 - Extend riscv_fw_parent_hartid() to support both DT and ACPI in PATCH1
 - Extend iommu_dma_compose_msi_msg() instead of adding iommu_dma_select_msi()
   in PATCH6
 - Addressed Conor's comments in PATCH3
 - Addressed Conor's and Rob's comments in PATCH7

Changes since v2:
 - Rebased on Linux-6.4-rc1
 - Addressed Rob's comments on DT bindings patches 4 and 8.
 - Addessed Marc's comments on IMSIC driver PATCH5
 - Replaced use of OF apis in APLIC and IMSIC drivers with FWNODE apis
   this makes both drivers easily portable for ACPI support. This also
   removes unnecessary indirection from the APLIC and IMSIC drivers.
 - PATCH1 is a new patch for portability with ACPI support
 - PATCH2 is a new patch to fix probing in APLIC drivers for APLIC-only systems.
 - PATCH7 is a new patch which addresses the IOMMU DMA domain issues pointed
   out by SiFive

Changes since v1:
 - Rebased on Linux-6.2-rc2
 - Addressed comments on IMSIC DT bindings for PATCH4
 - Use raw_spin_lock_irqsave() on ids_lock for PATCH5
 - Improved MMIO alignment checks in PATCH5 to allow MMIO regions
   with holes.
 - Addressed comments on APLIC DT bindings for PATCH6
 - Fixed warning splat in aplic_msi_write_msg() caused by
   zeroed MSI message in PATCH7
 - Dropped DT property riscv,slow-ipi instead will have module
   parameter in future.

Anup Patel (15):
  RISC-V: Don't fail in riscv_of_parent_hartid() for disabled HARTs
  of: property: Add fw_devlink support for msi-parent
  drivers: irqchip/riscv-intc: Mark all INTC nodes as initialized
  irqchip/sifive-plic: Fix syscore registration for multi-socket systems
  irqchip/sifive-plic: Convert PLIC driver into a platform driver
  irqchip/riscv-intc: Add support for RISC-V AIA
  dt-bindings: interrupt-controller: Add RISC-V incoming MSI controller
  irqchip: Add RISC-V incoming MSI controller early driver
  irqchip/riscv-imsic: Add support for platform MSI irqdomain
  irqchip/riscv-imsic: Add support for PCI MSI irqdomain
  dt-bindings: interrupt-controller: Add RISC-V advanced PLIC
  irqchip: Add RISC-V advanced PLIC driver for direct-mode
  irqchip/riscv-aplic: Add support for MSI-mode
  RISC-V: Select APLIC and IMSIC drivers
  MAINTAINERS: Add entry for RISC-V AIA drivers

 .../interrupt-controller/riscv,aplic.yaml     | 172 ++++++
 .../interrupt-controller/riscv,imsics.yaml    | 172 ++++++
 MAINTAINERS                                   |  14 +
 arch/riscv/Kconfig                            |   2 +
 arch/riscv/kernel/cpu.c                       |  11 +-
 drivers/irqchip/Kconfig                       |  24 +
 drivers/irqchip/Makefile                      |   3 +
 drivers/irqchip/irq-riscv-aplic-direct.c      | 343 +++++++++++
 drivers/irqchip/irq-riscv-aplic-main.c        | 232 +++++++
 drivers/irqchip/irq-riscv-aplic-main.h        |  53 ++
 drivers/irqchip/irq-riscv-aplic-msi.c         | 285 +++++++++
 drivers/irqchip/irq-riscv-imsic-early.c       | 259 ++++++++
 drivers/irqchip/irq-riscv-imsic-platform.c    | 319 ++++++++++
 drivers/irqchip/irq-riscv-imsic-state.c       | 570 ++++++++++++++++++
 drivers/irqchip/irq-riscv-imsic-state.h       |  67 ++
 drivers/irqchip/irq-riscv-intc.c              |  44 +-
 drivers/irqchip/irq-sifive-plic.c             | 242 +++++---
 drivers/of/property.c                         |   2 +
 include/linux/irqchip/riscv-aplic.h           | 119 ++++
 include/linux/irqchip/riscv-imsic.h           |  86 +++
 20 files changed, 2915 insertions(+), 104 deletions(-)
 create mode 100644 Documentation/devicetree/bindings/interrupt-controller/riscv,aplic.yaml
 create mode 100644 Documentation/devicetree/bindings/interrupt-controller/riscv,imsics.yaml
 create mode 100644 drivers/irqchip/irq-riscv-aplic-direct.c
 create mode 100644 drivers/irqchip/irq-riscv-aplic-main.c
 create mode 100644 drivers/irqchip/irq-riscv-aplic-main.h
 create mode 100644 drivers/irqchip/irq-riscv-aplic-msi.c
 create mode 100644 drivers/irqchip/irq-riscv-imsic-early.c
 create mode 100644 drivers/irqchip/irq-riscv-imsic-platform.c
 create mode 100644 drivers/irqchip/irq-riscv-imsic-state.c
 create mode 100644 drivers/irqchip/irq-riscv-imsic-state.h
 create mode 100644 include/linux/irqchip/riscv-aplic.h
 create mode 100644 include/linux/irqchip/riscv-imsic.h
</pre>
</div>
<h2>Comments</h2>
<a name=25561636></a>
<div class=comment>
<div class=meta>
 <span><a href="https://patchwork.kernel.org/project/linux-riscv/list/?submitter=198739">Björn Töpel</a></span>
 <span class=pull-right>Oct. 19, 2023, 1:43 p.m. UTC | <a href=https://patchwork.kernel.org/comment/25561636/>#1</a></span>
</div>
<pre class=content>Hi Anup,

Anup Patel &lt;apatel@ventanamicro.com&gt; writes:
<span class=quote>
&gt; The RISC-V AIA specification is ratified as-per the RISC-V international</span>
<span class=quote>&gt; process. The latest ratified AIA specifcation can be found at:</span>
<span class=quote>&gt; https://github.com/riscv/riscv-aia/releases/download/1.0/riscv-interrupts-1.0.pdf</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; At a high-level, the AIA specification adds three things:</span>
<span class=quote>&gt; 1) AIA CSRs</span>
<span class=quote>&gt;    - Improved local interrupt support</span>
<span class=quote>&gt; 2) Incoming Message Signaled Interrupt Controller (IMSIC)</span>
<span class=quote>&gt;    - Per-HART MSI controller</span>
<span class=quote>&gt;    - Support MSI virtualization</span>
<span class=quote>&gt;    - Support IPI along with virtualization</span>
<span class=quote>&gt; 3) Advanced Platform-Level Interrupt Controller (APLIC)</span>
<span class=quote>&gt;    - Wired interrupt controller</span>
<span class=quote>&gt;    - In MSI-mode, converts wired interrupt into MSIs (i.e. MSI generator)</span>
<span class=quote>&gt;    - In Direct-mode, injects external interrupts directly into HARTs</span>

Thanks for working on the AIA support! I had a look at the series, and
have some concerns about interrupt ID abstraction.

A bit of background, for readers not familiar with the AIA details.

IMSIC allows for 2047 unique MSI ("msi-irq") sources per hart, and
each MSI is dedicated to a certain hart. The series takes the approach
to say that there are, e.g., 2047 interrupts ("lnx-irq") globally.
Each lnx-irq consists of #harts * msi-irq -- a slice -- and in the
slice only *one* msi-irq is acutally used.

This scheme makes affinity changes more robust, because the interrupt
sources on "other" harts are pre-allocated. On the other hand it
requires to propagate irq masking to other harts via IPIs (this is
mostly done up setup/tear down). It's also wasteful, because msi-irqs
are hogged, and cannot be used.

Contemporary storage/networking drivers usually uses queues per core
(or a sub-set of cores). The current scheme wastes a lot of msi-irqs.
If we instead used a scheme where "msi-irq == lnx-irq", instead of
"lnq-irq = {hart 0;msi-irq x , ... hart N;msi-irq x}", there would be
a lot MSIs for other users. 1-1 vs 1-N. E.g., if a storage device
would like to use 5 queues (5 cores) on a 128 core system, the current
scheme would consume 5 * 128 MSIs, instead of just 5.

On the plus side:
* Changing interrupts affinity will never fail, because the interrupts
  on each hart is pre-allocated.

On the negative side:
* Wasteful interrupt usage, and a system can potientially "run out" of
  interrupts. Especially for many core systems.
* Interrupt masking need to proagate to harts via IPIs (there's no
  broadcast csr in IMSIC), and a more complex locking scheme IMSIC

Summary:
The current series caps the number of global interrupts to maximum
2047 MSIs for all cores (whole system). A better scheme, IMO, would be
to expose 2047 * #harts unique MSIs.

I think this could simplify/remove(?) the locking as well.

I realize that the series in v10, and coming with a change like this
now might be a bit of a pain...

Finally, another question related to APLIC/IMSIC. AFAIU the memory map
of the IMSIC regions are constrained by the APLIC, which requires a
certain layout for MSI forwarding (group/hart/guest bits). Say that a
system doesn't have an APLIC, couldn't the layout requirement be
simplified?


Again, thanks for the hard work!
Björn
</pre>
</div>
<a name=25561979></a>
<div class=comment>
<div class=meta>
 <span><a href="https://patchwork.kernel.org/project/linux-riscv/list/?submitter=204452">Anup Patel</a></span>
 <span class=pull-right>Oct. 19, 2023, 4:11 p.m. UTC | <a href=https://patchwork.kernel.org/comment/25561979/>#2</a></span>
</div>
<pre class=content>On Thu, Oct 19, 2023 at 7:13 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:
<span class=quote>&gt;</span>
<span class=quote>&gt; Hi Anup,</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; &gt; The RISC-V AIA specification is ratified as-per the RISC-V international</span>
<span class=quote>&gt; &gt; process. The latest ratified AIA specifcation can be found at:</span>
<span class=quote>&gt; &gt; https://github.com/riscv/riscv-aia/releases/download/1.0/riscv-interrupts-1.0.pdf</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; At a high-level, the AIA specification adds three things:</span>
<span class=quote>&gt; &gt; 1) AIA CSRs</span>
<span class=quote>&gt; &gt;    - Improved local interrupt support</span>
<span class=quote>&gt; &gt; 2) Incoming Message Signaled Interrupt Controller (IMSIC)</span>
<span class=quote>&gt; &gt;    - Per-HART MSI controller</span>
<span class=quote>&gt; &gt;    - Support MSI virtualization</span>
<span class=quote>&gt; &gt;    - Support IPI along with virtualization</span>
<span class=quote>&gt; &gt; 3) Advanced Platform-Level Interrupt Controller (APLIC)</span>
<span class=quote>&gt; &gt;    - Wired interrupt controller</span>
<span class=quote>&gt; &gt;    - In MSI-mode, converts wired interrupt into MSIs (i.e. MSI generator)</span>
<span class=quote>&gt; &gt;    - In Direct-mode, injects external interrupts directly into HARTs</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Thanks for working on the AIA support! I had a look at the series, and</span>
<span class=quote>&gt; have some concerns about interrupt ID abstraction.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; A bit of background, for readers not familiar with the AIA details.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; IMSIC allows for 2047 unique MSI ("msi-irq") sources per hart, and</span>
<span class=quote>&gt; each MSI is dedicated to a certain hart. The series takes the approach</span>
<span class=quote>&gt; to say that there are, e.g., 2047 interrupts ("lnx-irq") globally.</span>
<span class=quote>&gt; Each lnx-irq consists of #harts * msi-irq -- a slice -- and in the</span>
<span class=quote>&gt; slice only *one* msi-irq is acutally used.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; This scheme makes affinity changes more robust, because the interrupt</span>
<span class=quote>&gt; sources on "other" harts are pre-allocated. On the other hand it</span>
<span class=quote>&gt; requires to propagate irq masking to other harts via IPIs (this is</span>
<span class=quote>&gt; mostly done up setup/tear down). It's also wasteful, because msi-irqs</span>
<span class=quote>&gt; are hogged, and cannot be used.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Contemporary storage/networking drivers usually uses queues per core</span>
<span class=quote>&gt; (or a sub-set of cores). The current scheme wastes a lot of msi-irqs.</span>
<span class=quote>&gt; If we instead used a scheme where "msi-irq == lnx-irq", instead of</span>
<span class=quote>&gt; "lnq-irq = {hart 0;msi-irq x , ... hart N;msi-irq x}", there would be</span>
<span class=quote>&gt; a lot MSIs for other users. 1-1 vs 1-N. E.g., if a storage device</span>
<span class=quote>&gt; would like to use 5 queues (5 cores) on a 128 core system, the current</span>
<span class=quote>&gt; scheme would consume 5 * 128 MSIs, instead of just 5.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; On the plus side:</span>
<span class=quote>&gt; * Changing interrupts affinity will never fail, because the interrupts</span>
<span class=quote>&gt;   on each hart is pre-allocated.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; On the negative side:</span>
<span class=quote>&gt; * Wasteful interrupt usage, and a system can potientially "run out" of</span>
<span class=quote>&gt;   interrupts. Especially for many core systems.</span>
<span class=quote>&gt; * Interrupt masking need to proagate to harts via IPIs (there's no</span>
<span class=quote>&gt;   broadcast csr in IMSIC), and a more complex locking scheme IMSIC</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Summary:</span>
<span class=quote>&gt; The current series caps the number of global interrupts to maximum</span>
<span class=quote>&gt; 2047 MSIs for all cores (whole system). A better scheme, IMO, would be</span>
<span class=quote>&gt; to expose 2047 * #harts unique MSIs.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; I think this could simplify/remove(?) the locking as well.</span>

Exposing 2047 * #harts unique MSIs has multiple issues:
1) The irq_set_affinity() does not work for MSIs because each
     IRQ is not tied to a particular HART. This means we can't
     balance the IRQ processing load among HARTs.
2) All wired IRQs for APLIC MSI-mode will also target a
    fixed HART hence irq_set_affinity() won't work for wired
    IRQs as well.
3) Contemporary storage/networking drivers which use per-core
     queues use irq_set_affinity() on queue IRQs to balance
     across cores but this will fail.
4) HART hotplug breaks because kernel irq-subsystem can't
    migrate the IRQs (both MSIs and Wired) targeting HART X
    to another HART Y when the HART X goes down.

The idea of treating per-HART MSIs as separate IRQs has
been discussed in the past. The current approach works nicely
with all kernel use-cases at the cost of additional work on the
driver side.

Also, the current approach is very similar to the ARM GICv3
driver where ITS LPIs across CPUs are treated as single IRQ.
<span class=quote>
&gt;</span>
<span class=quote>&gt; I realize that the series in v10, and coming with a change like this</span>
<span class=quote>&gt; now might be a bit of a pain...</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Finally, another question related to APLIC/IMSIC. AFAIU the memory map</span>
<span class=quote>&gt; of the IMSIC regions are constrained by the APLIC, which requires a</span>
<span class=quote>&gt; certain layout for MSI forwarding (group/hart/guest bits). Say that a</span>
<span class=quote>&gt; system doesn't have an APLIC, couldn't the layout requirement be</span>
<span class=quote>&gt; simplified?</span>

Yes, this is already taken care of in the current IMSIC driver based
on feedback from Atish. We can certainly improve flexibility on the
IMSIC driver side if some case is missed-out.

The APLIC driver is certainly very strict about the arrangement of
IMSIC files so we do additional sanity checks on the APLIC driver
side at the time of probing.
<span class=quote>
&gt;</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Again, thanks for the hard work!</span>
<span class=quote>&gt; Björn</span>

Regards,
Anup
</pre>
</div>
<a name=25562940></a>
<div class=comment>
<div class=meta>
 <span><a href="https://patchwork.kernel.org/project/linux-riscv/list/?submitter=198739">Björn Töpel</a></span>
 <span class=pull-right>Oct. 20, 2023, 8:47 a.m. UTC | <a href=https://patchwork.kernel.org/comment/25562940/>#3</a></span>
</div>
<pre class=content>Thanks for the quick reply!

Anup Patel &lt;apatel@ventanamicro.com&gt; writes:
<span class=quote>
&gt; On Thu, Oct 19, 2023 at 7:13 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Hi Anup,</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt; The RISC-V AIA specification is ratified as-per the RISC-V international</span>
<span class=quote>&gt;&gt; &gt; process. The latest ratified AIA specifcation can be found at:</span>
<span class=quote>&gt;&gt; &gt; https://github.com/riscv/riscv-aia/releases/download/1.0/riscv-interrupts-1.0.pdf</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; At a high-level, the AIA specification adds three things:</span>
<span class=quote>&gt;&gt; &gt; 1) AIA CSRs</span>
<span class=quote>&gt;&gt; &gt;    - Improved local interrupt support</span>
<span class=quote>&gt;&gt; &gt; 2) Incoming Message Signaled Interrupt Controller (IMSIC)</span>
<span class=quote>&gt;&gt; &gt;    - Per-HART MSI controller</span>
<span class=quote>&gt;&gt; &gt;    - Support MSI virtualization</span>
<span class=quote>&gt;&gt; &gt;    - Support IPI along with virtualization</span>
<span class=quote>&gt;&gt; &gt; 3) Advanced Platform-Level Interrupt Controller (APLIC)</span>
<span class=quote>&gt;&gt; &gt;    - Wired interrupt controller</span>
<span class=quote>&gt;&gt; &gt;    - In MSI-mode, converts wired interrupt into MSIs (i.e. MSI generator)</span>
<span class=quote>&gt;&gt; &gt;    - In Direct-mode, injects external interrupts directly into HARTs</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Thanks for working on the AIA support! I had a look at the series, and</span>
<span class=quote>&gt;&gt; have some concerns about interrupt ID abstraction.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; A bit of background, for readers not familiar with the AIA details.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; IMSIC allows for 2047 unique MSI ("msi-irq") sources per hart, and</span>
<span class=quote>&gt;&gt; each MSI is dedicated to a certain hart. The series takes the approach</span>
<span class=quote>&gt;&gt; to say that there are, e.g., 2047 interrupts ("lnx-irq") globally.</span>
<span class=quote>&gt;&gt; Each lnx-irq consists of #harts * msi-irq -- a slice -- and in the</span>
<span class=quote>&gt;&gt; slice only *one* msi-irq is acutally used.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; This scheme makes affinity changes more robust, because the interrupt</span>
<span class=quote>&gt;&gt; sources on "other" harts are pre-allocated. On the other hand it</span>
<span class=quote>&gt;&gt; requires to propagate irq masking to other harts via IPIs (this is</span>
<span class=quote>&gt;&gt; mostly done up setup/tear down). It's also wasteful, because msi-irqs</span>
<span class=quote>&gt;&gt; are hogged, and cannot be used.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Contemporary storage/networking drivers usually uses queues per core</span>
<span class=quote>&gt;&gt; (or a sub-set of cores). The current scheme wastes a lot of msi-irqs.</span>
<span class=quote>&gt;&gt; If we instead used a scheme where "msi-irq == lnx-irq", instead of</span>
<span class=quote>&gt;&gt; "lnq-irq = {hart 0;msi-irq x , ... hart N;msi-irq x}", there would be</span>
<span class=quote>&gt;&gt; a lot MSIs for other users. 1-1 vs 1-N. E.g., if a storage device</span>
<span class=quote>&gt;&gt; would like to use 5 queues (5 cores) on a 128 core system, the current</span>
<span class=quote>&gt;&gt; scheme would consume 5 * 128 MSIs, instead of just 5.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; On the plus side:</span>
<span class=quote>&gt;&gt; * Changing interrupts affinity will never fail, because the interrupts</span>
<span class=quote>&gt;&gt;   on each hart is pre-allocated.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; On the negative side:</span>
<span class=quote>&gt;&gt; * Wasteful interrupt usage, and a system can potientially "run out" of</span>
<span class=quote>&gt;&gt;   interrupts. Especially for many core systems.</span>
<span class=quote>&gt;&gt; * Interrupt masking need to proagate to harts via IPIs (there's no</span>
<span class=quote>&gt;&gt;   broadcast csr in IMSIC), and a more complex locking scheme IMSIC</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Summary:</span>
<span class=quote>&gt;&gt; The current series caps the number of global interrupts to maximum</span>
<span class=quote>&gt;&gt; 2047 MSIs for all cores (whole system). A better scheme, IMO, would be</span>
<span class=quote>&gt;&gt; to expose 2047 * #harts unique MSIs.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; I think this could simplify/remove(?) the locking as well.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Exposing 2047 * #harts unique MSIs has multiple issues:</span>
<span class=quote>&gt; 1) The irq_set_affinity() does not work for MSIs because each</span>
<span class=quote>&gt;      IRQ is not tied to a particular HART. This means we can't</span>
<span class=quote>&gt;      balance the IRQ processing load among HARTs.</span>

Yes, you can balance. In your code, each *active* MSI is still
bound/active to a specific hard together with the affinity mask. In an
1-1 model you would still need to track the affinity mask, but the
irq_set_affinity() would be different. It would try to allocate a new
MSI from the target CPU, and then switch to having that MSI active.

That's what x86 does AFAIU, which is also constrained by the # of
available MSIs.

The downside, as I pointed out, is that the set affinity action can
fail for a certain target CPU.
<span class=quote>
&gt; 2) All wired IRQs for APLIC MSI-mode will also target a</span>
<span class=quote>&gt;     fixed HART hence irq_set_affinity() won't work for wired</span>
<span class=quote>&gt;     IRQs as well.</span>

I'm not following here. Why would APLIC put a constraint here? I had a
look at the specs, and I didn't see anything supporting the current
scheme explicitly.
<span class=quote>
&gt; 3) Contemporary storage/networking drivers which use per-core</span>
<span class=quote>&gt;      queues use irq_set_affinity() on queue IRQs to balance</span>
<span class=quote>&gt;      across cores but this will fail.</span>

Or via the the managed interrupts. But this is a non-issue, as pointed
out in my reply to 1.
<span class=quote>
&gt; 4) HART hotplug breaks because kernel irq-subsystem can't</span>
<span class=quote>&gt;     migrate the IRQs (both MSIs and Wired) targeting HART X</span>
<span class=quote>&gt;     to another HART Y when the HART X goes down.</span>

Yes, we might end up in scenarios where we can't move to a certain
target cpu, but I wouldn't expect that to be a common scenario.
<span class=quote>
&gt; The idea of treating per-HART MSIs as separate IRQs has</span>
<span class=quote>&gt; been discussed in the past.</span>

Aha! I tried to look for it in lore, but didn't find any. Could you
point me to those discussions?
<span class=quote>
&gt; Also, the current approach is very similar to the ARM GICv3 driver</span>
<span class=quote>&gt; where ITS LPIs across CPUs are treated as single IRQ.</span>

I'm not familiar with the GIC. Is the GICv3 design similar to IMSIC? I
had the impression that the GIC had a more advanced interrupt routing
mechanism, than what IMSIC exposes. I think x86 APIC takes the 1-1
approach (the folks on the To: list definitely knows! ;-)).

My concern is interrupts become a scarce resource with this
implementation, but maybe my view is incorrect. I've seen bare-metal
x86 systems (no VMs) with ~200 cores, and ~2000 interrupts, but maybe
that is considered "a lot of interrupts".

As long as we don't get into scenarios where we're running out of
interrupts, due to the software design.


Björn
</pre>
</div>
<a name=25563128></a>
<div class=comment>
<div class=meta>
 <span><a href="https://patchwork.kernel.org/project/linux-riscv/list/?submitter=204452">Anup Patel</a></span>
 <span class=pull-right>Oct. 20, 2023, 11 a.m. UTC | <a href=https://patchwork.kernel.org/comment/25563128/>#4</a></span>
</div>
<pre class=content>On Fri, Oct 20, 2023 at 2:17 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:
<span class=quote>&gt;</span>
<span class=quote>&gt; Thanks for the quick reply!</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; &gt; On Thu, Oct 19, 2023 at 7:13 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; Hi Anup,</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; The RISC-V AIA specification is ratified as-per the RISC-V international</span>
<span class=quote>&gt; &gt;&gt; &gt; process. The latest ratified AIA specifcation can be found at:</span>
<span class=quote>&gt; &gt;&gt; &gt; https://github.com/riscv/riscv-aia/releases/download/1.0/riscv-interrupts-1.0.pdf</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; At a high-level, the AIA specification adds three things:</span>
<span class=quote>&gt; &gt;&gt; &gt; 1) AIA CSRs</span>
<span class=quote>&gt; &gt;&gt; &gt;    - Improved local interrupt support</span>
<span class=quote>&gt; &gt;&gt; &gt; 2) Incoming Message Signaled Interrupt Controller (IMSIC)</span>
<span class=quote>&gt; &gt;&gt; &gt;    - Per-HART MSI controller</span>
<span class=quote>&gt; &gt;&gt; &gt;    - Support MSI virtualization</span>
<span class=quote>&gt; &gt;&gt; &gt;    - Support IPI along with virtualization</span>
<span class=quote>&gt; &gt;&gt; &gt; 3) Advanced Platform-Level Interrupt Controller (APLIC)</span>
<span class=quote>&gt; &gt;&gt; &gt;    - Wired interrupt controller</span>
<span class=quote>&gt; &gt;&gt; &gt;    - In MSI-mode, converts wired interrupt into MSIs (i.e. MSI generator)</span>
<span class=quote>&gt; &gt;&gt; &gt;    - In Direct-mode, injects external interrupts directly into HARTs</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; Thanks for working on the AIA support! I had a look at the series, and</span>
<span class=quote>&gt; &gt;&gt; have some concerns about interrupt ID abstraction.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; A bit of background, for readers not familiar with the AIA details.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; IMSIC allows for 2047 unique MSI ("msi-irq") sources per hart, and</span>
<span class=quote>&gt; &gt;&gt; each MSI is dedicated to a certain hart. The series takes the approach</span>
<span class=quote>&gt; &gt;&gt; to say that there are, e.g., 2047 interrupts ("lnx-irq") globally.</span>
<span class=quote>&gt; &gt;&gt; Each lnx-irq consists of #harts * msi-irq -- a slice -- and in the</span>
<span class=quote>&gt; &gt;&gt; slice only *one* msi-irq is acutally used.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; This scheme makes affinity changes more robust, because the interrupt</span>
<span class=quote>&gt; &gt;&gt; sources on "other" harts are pre-allocated. On the other hand it</span>
<span class=quote>&gt; &gt;&gt; requires to propagate irq masking to other harts via IPIs (this is</span>
<span class=quote>&gt; &gt;&gt; mostly done up setup/tear down). It's also wasteful, because msi-irqs</span>
<span class=quote>&gt; &gt;&gt; are hogged, and cannot be used.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; Contemporary storage/networking drivers usually uses queues per core</span>
<span class=quote>&gt; &gt;&gt; (or a sub-set of cores). The current scheme wastes a lot of msi-irqs.</span>
<span class=quote>&gt; &gt;&gt; If we instead used a scheme where "msi-irq == lnx-irq", instead of</span>
<span class=quote>&gt; &gt;&gt; "lnq-irq = {hart 0;msi-irq x , ... hart N;msi-irq x}", there would be</span>
<span class=quote>&gt; &gt;&gt; a lot MSIs for other users. 1-1 vs 1-N. E.g., if a storage device</span>
<span class=quote>&gt; &gt;&gt; would like to use 5 queues (5 cores) on a 128 core system, the current</span>
<span class=quote>&gt; &gt;&gt; scheme would consume 5 * 128 MSIs, instead of just 5.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; On the plus side:</span>
<span class=quote>&gt; &gt;&gt; * Changing interrupts affinity will never fail, because the interrupts</span>
<span class=quote>&gt; &gt;&gt;   on each hart is pre-allocated.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; On the negative side:</span>
<span class=quote>&gt; &gt;&gt; * Wasteful interrupt usage, and a system can potientially "run out" of</span>
<span class=quote>&gt; &gt;&gt;   interrupts. Especially for many core systems.</span>
<span class=quote>&gt; &gt;&gt; * Interrupt masking need to proagate to harts via IPIs (there's no</span>
<span class=quote>&gt; &gt;&gt;   broadcast csr in IMSIC), and a more complex locking scheme IMSIC</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; Summary:</span>
<span class=quote>&gt; &gt;&gt; The current series caps the number of global interrupts to maximum</span>
<span class=quote>&gt; &gt;&gt; 2047 MSIs for all cores (whole system). A better scheme, IMO, would be</span>
<span class=quote>&gt; &gt;&gt; to expose 2047 * #harts unique MSIs.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; I think this could simplify/remove(?) the locking as well.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; Exposing 2047 * #harts unique MSIs has multiple issues:</span>
<span class=quote>&gt; &gt; 1) The irq_set_affinity() does not work for MSIs because each</span>
<span class=quote>&gt; &gt;      IRQ is not tied to a particular HART. This means we can't</span>
<span class=quote>&gt; &gt;      balance the IRQ processing load among HARTs.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Yes, you can balance. In your code, each *active* MSI is still</span>
<span class=quote>&gt; bound/active to a specific hard together with the affinity mask. In an</span>
<span class=quote>&gt; 1-1 model you would still need to track the affinity mask, but the</span>
<span class=quote>&gt; irq_set_affinity() would be different. It would try to allocate a new</span>
<span class=quote>&gt; MSI from the target CPU, and then switch to having that MSI active.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; That's what x86 does AFAIU, which is also constrained by the # of</span>
<span class=quote>&gt; available MSIs.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; The downside, as I pointed out, is that the set affinity action can</span>
<span class=quote>&gt; fail for a certain target CPU.</span>

Yes, irq_set_affinity() can fail for the suggested approach plus for
RISC-V AIA, one HART does not have access to other HARTs
MSI enable/disable bits so the approach will also involve IPI.
<span class=quote>
&gt;</span>
<span class=quote>&gt; &gt; 2) All wired IRQs for APLIC MSI-mode will also target a</span>
<span class=quote>&gt; &gt;     fixed HART hence irq_set_affinity() won't work for wired</span>
<span class=quote>&gt; &gt;     IRQs as well.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; I'm not following here. Why would APLIC put a constraint here? I had a</span>
<span class=quote>&gt; look at the specs, and I didn't see anything supporting the current</span>
<span class=quote>&gt; scheme explicitly.</span>

Lets say the number of APLIC wired interrupts  are greater than the
number of per-CPU IMSIC IDs. In this case, if all wired interrupts are
moved to a particular CPU then irq_set_affinity() will fail for some of
the wired interrupts.
<span class=quote>
&gt;</span>
<span class=quote>&gt; &gt; 3) Contemporary storage/networking drivers which use per-core</span>
<span class=quote>&gt; &gt;      queues use irq_set_affinity() on queue IRQs to balance</span>
<span class=quote>&gt; &gt;      across cores but this will fail.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Or via the the managed interrupts. But this is a non-issue, as pointed</span>
<span class=quote>&gt; out in my reply to 1.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; &gt; 4) HART hotplug breaks because kernel irq-subsystem can't</span>
<span class=quote>&gt; &gt;     migrate the IRQs (both MSIs and Wired) targeting HART X</span>
<span class=quote>&gt; &gt;     to another HART Y when the HART X goes down.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Yes, we might end up in scenarios where we can't move to a certain</span>
<span class=quote>&gt; target cpu, but I wouldn't expect that to be a common scenario.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; &gt; The idea of treating per-HART MSIs as separate IRQs has</span>
<span class=quote>&gt; &gt; been discussed in the past.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Aha! I tried to look for it in lore, but didn't find any. Could you</span>
<span class=quote>&gt; point me to those discussions?</span>

This was done 2 years back in the AIA TG meeting when we were
doing the PoC for AIA spec.
<span class=quote>
&gt;</span>
<span class=quote>&gt; &gt; Also, the current approach is very similar to the ARM GICv3 driver</span>
<span class=quote>&gt; &gt; where ITS LPIs across CPUs are treated as single IRQ.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; I'm not familiar with the GIC. Is the GICv3 design similar to IMSIC? I</span>
<span class=quote>&gt; had the impression that the GIC had a more advanced interrupt routing</span>
<span class=quote>&gt; mechanism, than what IMSIC exposes. I think x86 APIC takes the 1-1</span>
<span class=quote>&gt; approach (the folks on the To: list definitely knows! ;-)).</span>

GIC has a per-CPU redistributor which handles LPIs. The MSIs are
taken by GIC ITS and forwarded as LPI to the redistributor of a CPU.

The GIC driver treats LPI numbering space as global and not per-CPU.
Also, the limit on maximum number of LPIs is quite high because LPI
INTID can be 32-bit wide.
<span class=quote>
&gt;</span>
<span class=quote>&gt; My concern is interrupts become a scarce resource with this</span>
<span class=quote>&gt; implementation, but maybe my view is incorrect. I've seen bare-metal</span>
<span class=quote>&gt; x86 systems (no VMs) with ~200 cores, and ~2000 interrupts, but maybe</span>
<span class=quote>&gt; that is considered "a lot of interrupts".</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; As long as we don't get into scenarios where we're running out of</span>
<span class=quote>&gt; interrupts, due to the software design.</span>
<span class=quote>&gt;</span>

The current approach is simpler and ensures irq_set_affinity
always works. The limit of max 2047 IDs is sufficient for many
systems (if not all).

When we encounter a system requiring a large number of MSIs,
we can either:
1) Extend the AIA spec to support greater than 2047 IDs
2) Re-think the approach in the IMSIC driver

The choice between #1 and #2 above depends on the
guarantees we want for irq_set_affinity().

Regards,
Anup
</pre>
</div>
<a name=25563429></a>
<div class=comment>
<div class=meta>
 <span><a href="https://patchwork.kernel.org/project/linux-riscv/list/?submitter=198739">Björn Töpel</a></span>
 <span class=pull-right>Oct. 20, 2023, 2:40 p.m. UTC | <a href=https://patchwork.kernel.org/comment/25563429/>#5</a></span>
</div>
<pre class=content>Anup Patel &lt;apatel@ventanamicro.com&gt; writes:
<span class=quote>
&gt; On Fri, Oct 20, 2023 at 2:17 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Thanks for the quick reply!</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt; On Thu, Oct 19, 2023 at 7:13 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; Hi Anup,</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; The RISC-V AIA specification is ratified as-per the RISC-V international</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; process. The latest ratified AIA specifcation can be found at:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; https://github.com/riscv/riscv-aia/releases/download/1.0/riscv-interrupts-1.0.pdf</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; At a high-level, the AIA specification adds three things:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; 1) AIA CSRs</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;    - Improved local interrupt support</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; 2) Incoming Message Signaled Interrupt Controller (IMSIC)</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;    - Per-HART MSI controller</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;    - Support MSI virtualization</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;    - Support IPI along with virtualization</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; 3) Advanced Platform-Level Interrupt Controller (APLIC)</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;    - Wired interrupt controller</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;    - In MSI-mode, converts wired interrupt into MSIs (i.e. MSI generator)</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;    - In Direct-mode, injects external interrupts directly into HARTs</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; Thanks for working on the AIA support! I had a look at the series, and</span>
<span class=quote>&gt;&gt; &gt;&gt; have some concerns about interrupt ID abstraction.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; A bit of background, for readers not familiar with the AIA details.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; IMSIC allows for 2047 unique MSI ("msi-irq") sources per hart, and</span>
<span class=quote>&gt;&gt; &gt;&gt; each MSI is dedicated to a certain hart. The series takes the approach</span>
<span class=quote>&gt;&gt; &gt;&gt; to say that there are, e.g., 2047 interrupts ("lnx-irq") globally.</span>
<span class=quote>&gt;&gt; &gt;&gt; Each lnx-irq consists of #harts * msi-irq -- a slice -- and in the</span>
<span class=quote>&gt;&gt; &gt;&gt; slice only *one* msi-irq is acutally used.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; This scheme makes affinity changes more robust, because the interrupt</span>
<span class=quote>&gt;&gt; &gt;&gt; sources on "other" harts are pre-allocated. On the other hand it</span>
<span class=quote>&gt;&gt; &gt;&gt; requires to propagate irq masking to other harts via IPIs (this is</span>
<span class=quote>&gt;&gt; &gt;&gt; mostly done up setup/tear down). It's also wasteful, because msi-irqs</span>
<span class=quote>&gt;&gt; &gt;&gt; are hogged, and cannot be used.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; Contemporary storage/networking drivers usually uses queues per core</span>
<span class=quote>&gt;&gt; &gt;&gt; (or a sub-set of cores). The current scheme wastes a lot of msi-irqs.</span>
<span class=quote>&gt;&gt; &gt;&gt; If we instead used a scheme where "msi-irq == lnx-irq", instead of</span>
<span class=quote>&gt;&gt; &gt;&gt; "lnq-irq = {hart 0;msi-irq x , ... hart N;msi-irq x}", there would be</span>
<span class=quote>&gt;&gt; &gt;&gt; a lot MSIs for other users. 1-1 vs 1-N. E.g., if a storage device</span>
<span class=quote>&gt;&gt; &gt;&gt; would like to use 5 queues (5 cores) on a 128 core system, the current</span>
<span class=quote>&gt;&gt; &gt;&gt; scheme would consume 5 * 128 MSIs, instead of just 5.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; On the plus side:</span>
<span class=quote>&gt;&gt; &gt;&gt; * Changing interrupts affinity will never fail, because the interrupts</span>
<span class=quote>&gt;&gt; &gt;&gt;   on each hart is pre-allocated.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; On the negative side:</span>
<span class=quote>&gt;&gt; &gt;&gt; * Wasteful interrupt usage, and a system can potientially "run out" of</span>
<span class=quote>&gt;&gt; &gt;&gt;   interrupts. Especially for many core systems.</span>
<span class=quote>&gt;&gt; &gt;&gt; * Interrupt masking need to proagate to harts via IPIs (there's no</span>
<span class=quote>&gt;&gt; &gt;&gt;   broadcast csr in IMSIC), and a more complex locking scheme IMSIC</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; Summary:</span>
<span class=quote>&gt;&gt; &gt;&gt; The current series caps the number of global interrupts to maximum</span>
<span class=quote>&gt;&gt; &gt;&gt; 2047 MSIs for all cores (whole system). A better scheme, IMO, would be</span>
<span class=quote>&gt;&gt; &gt;&gt; to expose 2047 * #harts unique MSIs.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; I think this could simplify/remove(?) the locking as well.</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; Exposing 2047 * #harts unique MSIs has multiple issues:</span>
<span class=quote>&gt;&gt; &gt; 1) The irq_set_affinity() does not work for MSIs because each</span>
<span class=quote>&gt;&gt; &gt;      IRQ is not tied to a particular HART. This means we can't</span>
<span class=quote>&gt;&gt; &gt;      balance the IRQ processing load among HARTs.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Yes, you can balance. In your code, each *active* MSI is still</span>
<span class=quote>&gt;&gt; bound/active to a specific hard together with the affinity mask. In an</span>
<span class=quote>&gt;&gt; 1-1 model you would still need to track the affinity mask, but the</span>
<span class=quote>&gt;&gt; irq_set_affinity() would be different. It would try to allocate a new</span>
<span class=quote>&gt;&gt; MSI from the target CPU, and then switch to having that MSI active.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; That's what x86 does AFAIU, which is also constrained by the # of</span>
<span class=quote>&gt;&gt; available MSIs.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; The downside, as I pointed out, is that the set affinity action can</span>
<span class=quote>&gt;&gt; fail for a certain target CPU.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Yes, irq_set_affinity() can fail for the suggested approach plus for</span>
<span class=quote>&gt; RISC-V AIA, one HART does not have access to other HARTs</span>
<span class=quote>&gt; MSI enable/disable bits so the approach will also involve IPI.</span>

Correct, but the current series does a broadcast to all cores, where the
1-1 approach is at most an IPI to a single core.

128+c machines are getting more common, and you have devices that you
bring up/down on a per-core basis. Broadcasting IPIs to all cores, when
dealing with a per-core activity is a pretty noisy neighbor.

This could be fixed in the existing 1-n approach, by not require to sync
the cores that are not handling the MSI in question. "Lazy disable"
<span class=quote>
&gt;&gt; &gt; 2) All wired IRQs for APLIC MSI-mode will also target a</span>
<span class=quote>&gt;&gt; &gt;     fixed HART hence irq_set_affinity() won't work for wired</span>
<span class=quote>&gt;&gt; &gt;     IRQs as well.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; I'm not following here. Why would APLIC put a constraint here? I had a</span>
<span class=quote>&gt;&gt; look at the specs, and I didn't see anything supporting the current</span>
<span class=quote>&gt;&gt; scheme explicitly.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Lets say the number of APLIC wired interrupts  are greater than the</span>
<span class=quote>&gt; number of per-CPU IMSIC IDs. In this case, if all wired interrupts are</span>
<span class=quote>&gt; moved to a particular CPU then irq_set_affinity() will fail for some of</span>
<span class=quote>&gt; the wired interrupts.</span>

Right, it's the case of "full remote CPU" again. Thanks for clearing
that up.
<span class=quote>
&gt;&gt; &gt; The idea of treating per-HART MSIs as separate IRQs has</span>
<span class=quote>&gt;&gt; &gt; been discussed in the past.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Aha! I tried to look for it in lore, but didn't find any. Could you</span>
<span class=quote>&gt;&gt; point me to those discussions?</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; This was done 2 years back in the AIA TG meeting when we were</span>
<span class=quote>&gt; doing the PoC for AIA spec.</span>

Ah, too bad. Thanks regardless.
<span class=quote>
&gt;&gt; My concern is interrupts become a scarce resource with this</span>
<span class=quote>&gt;&gt; implementation, but maybe my view is incorrect. I've seen bare-metal</span>
<span class=quote>&gt;&gt; x86 systems (no VMs) with ~200 cores, and ~2000 interrupts, but maybe</span>
<span class=quote>&gt;&gt; that is considered "a lot of interrupts".</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; As long as we don't get into scenarios where we're running out of</span>
<span class=quote>&gt;&gt; interrupts, due to the software design.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; The current approach is simpler and ensures irq_set_affinity</span>
<span class=quote>&gt; always works. The limit of max 2047 IDs is sufficient for many</span>
<span class=quote>&gt; systems (if not all).</span>

Let me give you another view. On a 128c system each core has ~16 unique
interrupts for disposal. E.g. the Intel E800 NIC has more than 2048
network queue pairs for each PF.
<span class=quote>
&gt; When we encounter a system requiring a large number of MSIs,</span>
<span class=quote>&gt; we can either:</span>
<span class=quote>&gt; 1) Extend the AIA spec to support greater than 2047 IDs</span>
<span class=quote>&gt; 2) Re-think the approach in the IMSIC driver</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; The choice between #1 and #2 above depends on the</span>
<span class=quote>&gt; guarantees we want for irq_set_affinity().</span>

The irq_set_affinity() behavior is better with this series, but I think
the other downsides: number of available interrupt sources, and IPI
broadcast are worse.


Björn
</pre>
</div>
<a name=25563535></a>
<div class=comment>
<div class=meta>
 <span><a href="https://patchwork.kernel.org/project/linux-riscv/list/?submitter=204452">Anup Patel</a></span>
 <span class=pull-right>Oct. 20, 2023, 3:34 p.m. UTC | <a href=https://patchwork.kernel.org/comment/25563535/>#6</a></span>
</div>
<pre class=content>On Fri, Oct 20, 2023 at 8:10 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:
<span class=quote>&gt;</span>
<span class=quote>&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; &gt; On Fri, Oct 20, 2023 at 2:17 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; Thanks for the quick reply!</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; On Thu, Oct 19, 2023 at 7:13 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; Hi Anup,</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; The RISC-V AIA specification is ratified as-per the RISC-V international</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; process. The latest ratified AIA specifcation can be found at:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; https://github.com/riscv/riscv-aia/releases/download/1.0/riscv-interrupts-1.0.pdf</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; At a high-level, the AIA specification adds three things:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; 1) AIA CSRs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;    - Improved local interrupt support</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; 2) Incoming Message Signaled Interrupt Controller (IMSIC)</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;    - Per-HART MSI controller</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;    - Support MSI virtualization</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;    - Support IPI along with virtualization</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; 3) Advanced Platform-Level Interrupt Controller (APLIC)</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;    - Wired interrupt controller</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;    - In MSI-mode, converts wired interrupt into MSIs (i.e. MSI generator)</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;    - In Direct-mode, injects external interrupts directly into HARTs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; Thanks for working on the AIA support! I had a look at the series, and</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; have some concerns about interrupt ID abstraction.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; A bit of background, for readers not familiar with the AIA details.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; IMSIC allows for 2047 unique MSI ("msi-irq") sources per hart, and</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; each MSI is dedicated to a certain hart. The series takes the approach</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; to say that there are, e.g., 2047 interrupts ("lnx-irq") globally.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; Each lnx-irq consists of #harts * msi-irq -- a slice -- and in the</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; slice only *one* msi-irq is acutally used.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; This scheme makes affinity changes more robust, because the interrupt</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; sources on "other" harts are pre-allocated. On the other hand it</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; requires to propagate irq masking to other harts via IPIs (this is</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; mostly done up setup/tear down). It's also wasteful, because msi-irqs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; are hogged, and cannot be used.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; Contemporary storage/networking drivers usually uses queues per core</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; (or a sub-set of cores). The current scheme wastes a lot of msi-irqs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; If we instead used a scheme where "msi-irq == lnx-irq", instead of</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; "lnq-irq = {hart 0;msi-irq x , ... hart N;msi-irq x}", there would be</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; a lot MSIs for other users. 1-1 vs 1-N. E.g., if a storage device</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; would like to use 5 queues (5 cores) on a 128 core system, the current</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; scheme would consume 5 * 128 MSIs, instead of just 5.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; On the plus side:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; * Changing interrupts affinity will never fail, because the interrupts</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;   on each hart is pre-allocated.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; On the negative side:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; * Wasteful interrupt usage, and a system can potientially "run out" of</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;   interrupts. Especially for many core systems.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; * Interrupt masking need to proagate to harts via IPIs (there's no</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;   broadcast csr in IMSIC), and a more complex locking scheme IMSIC</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; Summary:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; The current series caps the number of global interrupts to maximum</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; 2047 MSIs for all cores (whole system). A better scheme, IMO, would be</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; to expose 2047 * #harts unique MSIs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; I think this could simplify/remove(?) the locking as well.</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; Exposing 2047 * #harts unique MSIs has multiple issues:</span>
<span class=quote>&gt; &gt;&gt; &gt; 1) The irq_set_affinity() does not work for MSIs because each</span>
<span class=quote>&gt; &gt;&gt; &gt;      IRQ is not tied to a particular HART. This means we can't</span>
<span class=quote>&gt; &gt;&gt; &gt;      balance the IRQ processing load among HARTs.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; Yes, you can balance. In your code, each *active* MSI is still</span>
<span class=quote>&gt; &gt;&gt; bound/active to a specific hard together with the affinity mask. In an</span>
<span class=quote>&gt; &gt;&gt; 1-1 model you would still need to track the affinity mask, but the</span>
<span class=quote>&gt; &gt;&gt; irq_set_affinity() would be different. It would try to allocate a new</span>
<span class=quote>&gt; &gt;&gt; MSI from the target CPU, and then switch to having that MSI active.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; That's what x86 does AFAIU, which is also constrained by the # of</span>
<span class=quote>&gt; &gt;&gt; available MSIs.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; The downside, as I pointed out, is that the set affinity action can</span>
<span class=quote>&gt; &gt;&gt; fail for a certain target CPU.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; Yes, irq_set_affinity() can fail for the suggested approach plus for</span>
<span class=quote>&gt; &gt; RISC-V AIA, one HART does not have access to other HARTs</span>
<span class=quote>&gt; &gt; MSI enable/disable bits so the approach will also involve IPI.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Correct, but the current series does a broadcast to all cores, where the</span>
<span class=quote>&gt; 1-1 approach is at most an IPI to a single core.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; 128+c machines are getting more common, and you have devices that you</span>
<span class=quote>&gt; bring up/down on a per-core basis. Broadcasting IPIs to all cores, when</span>
<span class=quote>&gt; dealing with a per-core activity is a pretty noisy neighbor.</span>

Broadcast IPI in the current approach is only done upon MSI mask/unmask
operation. It is not done upon set_affinity() of interrupt handling.
<span class=quote>
&gt;</span>
<span class=quote>&gt; This could be fixed in the existing 1-n approach, by not require to sync</span>
<span class=quote>&gt; the cores that are not handling the MSI in question. "Lazy disable"</span>

Incorrect. The approach you are suggesting involves an IPI upon every
irq_set_affinity(). This is because a HART can only enable it's own
MSI ID so when an IRQ is moved to from HART A to HART B with
a different ID X on HART B then we will need an IPI in irq_set_affinit()
to enable ID X on HART B.
<span class=quote>
&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; 2) All wired IRQs for APLIC MSI-mode will also target a</span>
<span class=quote>&gt; &gt;&gt; &gt;     fixed HART hence irq_set_affinity() won't work for wired</span>
<span class=quote>&gt; &gt;&gt; &gt;     IRQs as well.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; I'm not following here. Why would APLIC put a constraint here? I had a</span>
<span class=quote>&gt; &gt;&gt; look at the specs, and I didn't see anything supporting the current</span>
<span class=quote>&gt; &gt;&gt; scheme explicitly.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; Lets say the number of APLIC wired interrupts  are greater than the</span>
<span class=quote>&gt; &gt; number of per-CPU IMSIC IDs. In this case, if all wired interrupts are</span>
<span class=quote>&gt; &gt; moved to a particular CPU then irq_set_affinity() will fail for some of</span>
<span class=quote>&gt; &gt; the wired interrupts.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Right, it's the case of "full remote CPU" again. Thanks for clearing</span>
<span class=quote>&gt; that up.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; The idea of treating per-HART MSIs as separate IRQs has</span>
<span class=quote>&gt; &gt;&gt; &gt; been discussed in the past.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; Aha! I tried to look for it in lore, but didn't find any. Could you</span>
<span class=quote>&gt; &gt;&gt; point me to those discussions?</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; This was done 2 years back in the AIA TG meeting when we were</span>
<span class=quote>&gt; &gt; doing the PoC for AIA spec.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Ah, too bad. Thanks regardless.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; &gt;&gt; My concern is interrupts become a scarce resource with this</span>
<span class=quote>&gt; &gt;&gt; implementation, but maybe my view is incorrect. I've seen bare-metal</span>
<span class=quote>&gt; &gt;&gt; x86 systems (no VMs) with ~200 cores, and ~2000 interrupts, but maybe</span>
<span class=quote>&gt; &gt;&gt; that is considered "a lot of interrupts".</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; As long as we don't get into scenarios where we're running out of</span>
<span class=quote>&gt; &gt;&gt; interrupts, due to the software design.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; The current approach is simpler and ensures irq_set_affinity</span>
<span class=quote>&gt; &gt; always works. The limit of max 2047 IDs is sufficient for many</span>
<span class=quote>&gt; &gt; systems (if not all).</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Let me give you another view. On a 128c system each core has ~16 unique</span>
<span class=quote>&gt; interrupts for disposal. E.g. the Intel E800 NIC has more than 2048</span>
<span class=quote>&gt; network queue pairs for each PF.</span>

Clearly, this example is a hypothetical and represents a poorly
designed platform.

Having just 16 IDs per-Core is a very poor design choice. In fact, the
Server SoC spec mandates a minimum 255 IDs.

Regarding NICs which support a large number of queues, the driver
will typically enable only one queue per-core and set the affinity to
separate cores. We have user-space data plane applications based
on DPDK which are capable of using a large number of NIC queues
but these applications are polling based and don't use MSIs.
<span class=quote>
&gt;</span>
<span class=quote>&gt; &gt; When we encounter a system requiring a large number of MSIs,</span>
<span class=quote>&gt; &gt; we can either:</span>
<span class=quote>&gt; &gt; 1) Extend the AIA spec to support greater than 2047 IDs</span>
<span class=quote>&gt; &gt; 2) Re-think the approach in the IMSIC driver</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; The choice between #1 and #2 above depends on the</span>
<span class=quote>&gt; &gt; guarantees we want for irq_set_affinity().</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; The irq_set_affinity() behavior is better with this series, but I think</span>
<span class=quote>&gt; the other downsides: number of available interrupt sources, and IPI</span>
<span class=quote>&gt; broadcast are worse.</span>

The IPI overhead in the approach you are suggesting will be
even bad compared to the IPI overhead of the current approach
because we will end-up doing IPI upon every irq_set_affinity()
in the suggested approach compared to doing IPI upon every
mask/unmask in the current approach.

The biggest advantage of the current approach is a reliable
irq_set_affinity() which is a very valuable thing to have.

ARM systems easily support a large number of LPIs per-core.
For example, GIC-700 supports 56000 LPIs per-core.
(Refer, https://developer.arm.com/documentation/101516/0300/About-the-GIC-700/Features)

In the RISC-V world, we can easily define a small fast track
extension based on S*csrind extension which can allow a
large number of IMSIC IDs per-core.

Instead of addressing problems on a hypothetical system,
I suggest we go ahead with the current approach and deal
with a system having MSI over-subscription when such a
system shows up.

Regards,
Anup
</pre>
</div>
<a name=25563661></a>
<div class=comment>
<div class=meta>
 <span><a href="https://patchwork.kernel.org/project/linux-riscv/list/?submitter=198739">Björn Töpel</a></span>
 <span class=pull-right>Oct. 20, 2023, 4:36 p.m. UTC | <a href=https://patchwork.kernel.org/comment/25563661/>#7</a></span>
</div>
<pre class=content>Anup Patel &lt;apatel@ventanamicro.com&gt; writes:
<span class=quote>
&gt; On Fri, Oct 20, 2023 at 8:10 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt; On Fri, Oct 20, 2023 at 2:17 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; Thanks for the quick reply!</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; On Thu, Oct 19, 2023 at 7:13 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; Hi Anup,</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt; The RISC-V AIA specification is ratified as-per the RISC-V international</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt; process. The latest ratified AIA specifcation can be found at:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt; https://github.com/riscv/riscv-aia/releases/download/1.0/riscv-interrupts-1.0.pdf</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt; At a high-level, the AIA specification adds three things:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt; 1) AIA CSRs</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Improved local interrupt support</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt; 2) Incoming Message Signaled Interrupt Controller (IMSIC)</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Per-HART MSI controller</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Support MSI virtualization</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Support IPI along with virtualization</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt; 3) Advanced Platform-Level Interrupt Controller (APLIC)</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Wired interrupt controller</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;    - In MSI-mode, converts wired interrupt into MSIs (i.e. MSI generator)</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;    - In Direct-mode, injects external interrupts directly into HARTs</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; Thanks for working on the AIA support! I had a look at the series, and</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; have some concerns about interrupt ID abstraction.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; A bit of background, for readers not familiar with the AIA details.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; IMSIC allows for 2047 unique MSI ("msi-irq") sources per hart, and</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; each MSI is dedicated to a certain hart. The series takes the approach</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; to say that there are, e.g., 2047 interrupts ("lnx-irq") globally.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; Each lnx-irq consists of #harts * msi-irq -- a slice -- and in the</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; slice only *one* msi-irq is acutally used.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; This scheme makes affinity changes more robust, because the interrupt</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; sources on "other" harts are pre-allocated. On the other hand it</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; requires to propagate irq masking to other harts via IPIs (this is</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; mostly done up setup/tear down). It's also wasteful, because msi-irqs</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; are hogged, and cannot be used.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; Contemporary storage/networking drivers usually uses queues per core</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; (or a sub-set of cores). The current scheme wastes a lot of msi-irqs.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; If we instead used a scheme where "msi-irq == lnx-irq", instead of</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; "lnq-irq = {hart 0;msi-irq x , ... hart N;msi-irq x}", there would be</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; a lot MSIs for other users. 1-1 vs 1-N. E.g., if a storage device</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; would like to use 5 queues (5 cores) on a 128 core system, the current</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; scheme would consume 5 * 128 MSIs, instead of just 5.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; On the plus side:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; * Changing interrupts affinity will never fail, because the interrupts</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;   on each hart is pre-allocated.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; On the negative side:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; * Wasteful interrupt usage, and a system can potientially "run out" of</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;   interrupts. Especially for many core systems.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; * Interrupt masking need to proagate to harts via IPIs (there's no</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;   broadcast csr in IMSIC), and a more complex locking scheme IMSIC</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; Summary:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; The current series caps the number of global interrupts to maximum</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; 2047 MSIs for all cores (whole system). A better scheme, IMO, would be</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; to expose 2047 * #harts unique MSIs.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; I think this could simplify/remove(?) the locking as well.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; Exposing 2047 * #harts unique MSIs has multiple issues:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; 1) The irq_set_affinity() does not work for MSIs because each</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;      IRQ is not tied to a particular HART. This means we can't</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;      balance the IRQ processing load among HARTs.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; Yes, you can balance. In your code, each *active* MSI is still</span>
<span class=quote>&gt;&gt; &gt;&gt; bound/active to a specific hard together with the affinity mask. In an</span>
<span class=quote>&gt;&gt; &gt;&gt; 1-1 model you would still need to track the affinity mask, but the</span>
<span class=quote>&gt;&gt; &gt;&gt; irq_set_affinity() would be different. It would try to allocate a new</span>
<span class=quote>&gt;&gt; &gt;&gt; MSI from the target CPU, and then switch to having that MSI active.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; That's what x86 does AFAIU, which is also constrained by the # of</span>
<span class=quote>&gt;&gt; &gt;&gt; available MSIs.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; The downside, as I pointed out, is that the set affinity action can</span>
<span class=quote>&gt;&gt; &gt;&gt; fail for a certain target CPU.</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; Yes, irq_set_affinity() can fail for the suggested approach plus for</span>
<span class=quote>&gt;&gt; &gt; RISC-V AIA, one HART does not have access to other HARTs</span>
<span class=quote>&gt;&gt; &gt; MSI enable/disable bits so the approach will also involve IPI.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Correct, but the current series does a broadcast to all cores, where the</span>
<span class=quote>&gt;&gt; 1-1 approach is at most an IPI to a single core.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; 128+c machines are getting more common, and you have devices that you</span>
<span class=quote>&gt;&gt; bring up/down on a per-core basis. Broadcasting IPIs to all cores, when</span>
<span class=quote>&gt;&gt; dealing with a per-core activity is a pretty noisy neighbor.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Broadcast IPI in the current approach is only done upon MSI mask/unmask</span>
<span class=quote>&gt; operation. It is not done upon set_affinity() of interrupt handling.</span>

I'm aware. We're on the same page here.
<span class=quote>
&gt;&gt;</span>
<span class=quote>&gt;&gt; This could be fixed in the existing 1-n approach, by not require to sync</span>
<span class=quote>&gt;&gt; the cores that are not handling the MSI in question. "Lazy disable"</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Incorrect. The approach you are suggesting involves an IPI upon every</span>
<span class=quote>&gt; irq_set_affinity(). This is because a HART can only enable it's own</span>
<span class=quote>&gt; MSI ID so when an IRQ is moved to from HART A to HART B with</span>
<span class=quote>&gt; a different ID X on HART B then we will need an IPI in irq_set_affinit()</span>
<span class=quote>&gt; to enable ID X on HART B.</span>

Yes, the 1-1 approach will require an IPI to one target cpu on affinity
changes, and similar on mask/unmask.

The 1-n approach, require no-IPI on affinity changes (nice!), but IPI
broadcast to all cores on mask/unmask (not so nice).
<span class=quote>
&gt;&gt; &gt;&gt; My concern is interrupts become a scarce resource with this</span>
<span class=quote>&gt;&gt; &gt;&gt; implementation, but maybe my view is incorrect. I've seen bare-metal</span>
<span class=quote>&gt;&gt; &gt;&gt; x86 systems (no VMs) with ~200 cores, and ~2000 interrupts, but maybe</span>
<span class=quote>&gt;&gt; &gt;&gt; that is considered "a lot of interrupts".</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; As long as we don't get into scenarios where we're running out of</span>
<span class=quote>&gt;&gt; &gt;&gt; interrupts, due to the software design.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; The current approach is simpler and ensures irq_set_affinity</span>
<span class=quote>&gt;&gt; &gt; always works. The limit of max 2047 IDs is sufficient for many</span>
<span class=quote>&gt;&gt; &gt; systems (if not all).</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Let me give you another view. On a 128c system each core has ~16 unique</span>
<span class=quote>&gt;&gt; interrupts for disposal. E.g. the Intel E800 NIC has more than 2048</span>
<span class=quote>&gt;&gt; network queue pairs for each PF.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Clearly, this example is a hypothetical and represents a poorly</span>
<span class=quote>&gt; designed platform.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Having just 16 IDs per-Core is a very poor design choice. In fact, the</span>
<span class=quote>&gt; Server SoC spec mandates a minimum 255 IDs.</span>

You are misreading. A 128c system with 2047 MSIs per-core, will only
have 16 *per-core unique* (2047/128) interrupts with the current series.

I'm not saying that each IMSIC has 16 IDs, I'm saying that in a 128c
system with the maximum amount of MSIs possible in the spec, you'll end
up with 16 *unique* interrupts per core.
<span class=quote>
&gt; Regarding NICs which support a large number of queues, the driver</span>
<span class=quote>&gt; will typically enable only one queue per-core and set the affinity to</span>
<span class=quote>&gt; separate cores. We have user-space data plane applications based</span>
<span class=quote>&gt; on DPDK which are capable of using a large number of NIC queues</span>
<span class=quote>&gt; but these applications are polling based and don't use MSIs.</span>

That's one sample point, and clearly not the only one. There are *many*
different usage models. Just because you *assign* MSI, doesn't mean they
are firing all the time.

I can show you a couple of networking setups where this is clearly not
enough. Each core has a large number of QoS queues, and each queue would
very much like to have a dedicated MSI.
<span class=quote>
&gt;&gt; &gt; When we encounter a system requiring a large number of MSIs,</span>
<span class=quote>&gt;&gt; &gt; we can either:</span>
<span class=quote>&gt;&gt; &gt; 1) Extend the AIA spec to support greater than 2047 IDs</span>
<span class=quote>&gt;&gt; &gt; 2) Re-think the approach in the IMSIC driver</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; The choice between #1 and #2 above depends on the</span>
<span class=quote>&gt;&gt; &gt; guarantees we want for irq_set_affinity().</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; The irq_set_affinity() behavior is better with this series, but I think</span>
<span class=quote>&gt;&gt; the other downsides: number of available interrupt sources, and IPI</span>
<span class=quote>&gt;&gt; broadcast are worse.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; The IPI overhead in the approach you are suggesting will be</span>
<span class=quote>&gt; even bad compared to the IPI overhead of the current approach</span>
<span class=quote>&gt; because we will end-up doing IPI upon every irq_set_affinity()</span>
<span class=quote>&gt; in the suggested approach compared to doing IPI upon every</span>
<span class=quote>&gt; mask/unmask in the current approach.</span>

Again, very workload dependent.

This series does IPI broadcast on masking/unmasking, which means that
cores that don't care get interrupted because, say, a network queue-pair
is setup on another core.

Some workloads never change the irq affinity.

I'm just pointing out that there are pro/cons with both variants.
<span class=quote>
&gt; The biggest advantage of the current approach is a reliable</span>
<span class=quote>&gt; irq_set_affinity() which is a very valuable thing to have.</span>

...and I'm arguing that we're paying a big price for that.
<span class=quote>
&gt; ARM systems easily support a large number of LPIs per-core.</span>
<span class=quote>&gt; For example, GIC-700 supports 56000 LPIs per-core.</span>
<span class=quote>&gt; (Refer, https://developer.arm.com/documentation/101516/0300/About-the-GIC-700/Features)</span>

Yeah, but this is not the GIC. This is something that looks more like
the x86 world. We'll be stuck with a lot of implementations with AIA 1.0
spec, and many cores.
<span class=quote>
&gt; In the RISC-V world, we can easily define a small fast track</span>
<span class=quote>&gt; extension based on S*csrind extension which can allow a</span>
<span class=quote>&gt; large number of IMSIC IDs per-core.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Instead of addressing problems on a hypothetical system,</span>
<span class=quote>&gt; I suggest we go ahead with the current approach and deal</span>
<span class=quote>&gt; with a system having MSI over-subscription when such a</span>
<span class=quote>&gt; system shows up.</span>

I've pointed out my concerns. We're not agreeing, but hey, I'm just one
sample point here! I'll leave it here for others to chime in!

Still much appreciate all the hard work on the series!


Have a nice weekend,
Björn
</pre>
</div>
<a name=25563715></a>
<div class=comment>
<div class=meta>
 <span><a href="https://patchwork.kernel.org/project/linux-riscv/list/?submitter=204452">Anup Patel</a></span>
 <span class=pull-right>Oct. 20, 2023, 5:13 p.m. UTC | <a href=https://patchwork.kernel.org/comment/25563715/>#8</a></span>
</div>
<pre class=content>On Fri, Oct 20, 2023 at 10:07 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:
<span class=quote>&gt;</span>
<span class=quote>&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; &gt; On Fri, Oct 20, 2023 at 8:10 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; On Fri, Oct 20, 2023 at 2:17 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; Thanks for the quick reply!</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; On Thu, Oct 19, 2023 at 7:13 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; Hi Anup,</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; The RISC-V AIA specification is ratified as-per the RISC-V international</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; process. The latest ratified AIA specifcation can be found at:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; https://github.com/riscv/riscv-aia/releases/download/1.0/riscv-interrupts-1.0.pdf</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; At a high-level, the AIA specification adds three things:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 1) AIA CSRs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Improved local interrupt support</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 2) Incoming Message Signaled Interrupt Controller (IMSIC)</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Per-HART MSI controller</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Support MSI virtualization</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Support IPI along with virtualization</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 3) Advanced Platform-Level Interrupt Controller (APLIC)</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Wired interrupt controller</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - In MSI-mode, converts wired interrupt into MSIs (i.e. MSI generator)</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - In Direct-mode, injects external interrupts directly into HARTs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; Thanks for working on the AIA support! I had a look at the series, and</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; have some concerns about interrupt ID abstraction.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; A bit of background, for readers not familiar with the AIA details.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; IMSIC allows for 2047 unique MSI ("msi-irq") sources per hart, and</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; each MSI is dedicated to a certain hart. The series takes the approach</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; to say that there are, e.g., 2047 interrupts ("lnx-irq") globally.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; Each lnx-irq consists of #harts * msi-irq -- a slice -- and in the</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; slice only *one* msi-irq is acutally used.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; This scheme makes affinity changes more robust, because the interrupt</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; sources on "other" harts are pre-allocated. On the other hand it</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; requires to propagate irq masking to other harts via IPIs (this is</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; mostly done up setup/tear down). It's also wasteful, because msi-irqs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; are hogged, and cannot be used.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; Contemporary storage/networking drivers usually uses queues per core</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; (or a sub-set of cores). The current scheme wastes a lot of msi-irqs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; If we instead used a scheme where "msi-irq == lnx-irq", instead of</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; "lnq-irq = {hart 0;msi-irq x , ... hart N;msi-irq x}", there would be</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; a lot MSIs for other users. 1-1 vs 1-N. E.g., if a storage device</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; would like to use 5 queues (5 cores) on a 128 core system, the current</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; scheme would consume 5 * 128 MSIs, instead of just 5.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; On the plus side:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Changing interrupts affinity will never fail, because the interrupts</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;   on each hart is pre-allocated.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; On the negative side:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Wasteful interrupt usage, and a system can potientially "run out" of</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;   interrupts. Especially for many core systems.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Interrupt masking need to proagate to harts via IPIs (there's no</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;   broadcast csr in IMSIC), and a more complex locking scheme IMSIC</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; Summary:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; The current series caps the number of global interrupts to maximum</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; 2047 MSIs for all cores (whole system). A better scheme, IMO, would be</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; to expose 2047 * #harts unique MSIs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; I think this could simplify/remove(?) the locking as well.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; Exposing 2047 * #harts unique MSIs has multiple issues:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; 1) The irq_set_affinity() does not work for MSIs because each</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;      IRQ is not tied to a particular HART. This means we can't</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;      balance the IRQ processing load among HARTs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; Yes, you can balance. In your code, each *active* MSI is still</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; bound/active to a specific hard together with the affinity mask. In an</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; 1-1 model you would still need to track the affinity mask, but the</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; irq_set_affinity() would be different. It would try to allocate a new</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; MSI from the target CPU, and then switch to having that MSI active.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; That's what x86 does AFAIU, which is also constrained by the # of</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; available MSIs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; The downside, as I pointed out, is that the set affinity action can</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; fail for a certain target CPU.</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; Yes, irq_set_affinity() can fail for the suggested approach plus for</span>
<span class=quote>&gt; &gt;&gt; &gt; RISC-V AIA, one HART does not have access to other HARTs</span>
<span class=quote>&gt; &gt;&gt; &gt; MSI enable/disable bits so the approach will also involve IPI.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; Correct, but the current series does a broadcast to all cores, where the</span>
<span class=quote>&gt; &gt;&gt; 1-1 approach is at most an IPI to a single core.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; 128+c machines are getting more common, and you have devices that you</span>
<span class=quote>&gt; &gt;&gt; bring up/down on a per-core basis. Broadcasting IPIs to all cores, when</span>
<span class=quote>&gt; &gt;&gt; dealing with a per-core activity is a pretty noisy neighbor.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; Broadcast IPI in the current approach is only done upon MSI mask/unmask</span>
<span class=quote>&gt; &gt; operation. It is not done upon set_affinity() of interrupt handling.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; I'm aware. We're on the same page here.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; This could be fixed in the existing 1-n approach, by not require to sync</span>
<span class=quote>&gt; &gt;&gt; the cores that are not handling the MSI in question. "Lazy disable"</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; Incorrect. The approach you are suggesting involves an IPI upon every</span>
<span class=quote>&gt; &gt; irq_set_affinity(). This is because a HART can only enable it's own</span>
<span class=quote>&gt; &gt; MSI ID so when an IRQ is moved to from HART A to HART B with</span>
<span class=quote>&gt; &gt; a different ID X on HART B then we will need an IPI in irq_set_affinit()</span>
<span class=quote>&gt; &gt; to enable ID X on HART B.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Yes, the 1-1 approach will require an IPI to one target cpu on affinity</span>
<span class=quote>&gt; changes, and similar on mask/unmask.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; The 1-n approach, require no-IPI on affinity changes (nice!), but IPI</span>
<span class=quote>&gt; broadcast to all cores on mask/unmask (not so nice).</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; My concern is interrupts become a scarce resource with this</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; implementation, but maybe my view is incorrect. I've seen bare-metal</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; x86 systems (no VMs) with ~200 cores, and ~2000 interrupts, but maybe</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; that is considered "a lot of interrupts".</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; As long as we don't get into scenarios where we're running out of</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; interrupts, due to the software design.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; The current approach is simpler and ensures irq_set_affinity</span>
<span class=quote>&gt; &gt;&gt; &gt; always works. The limit of max 2047 IDs is sufficient for many</span>
<span class=quote>&gt; &gt;&gt; &gt; systems (if not all).</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; Let me give you another view. On a 128c system each core has ~16 unique</span>
<span class=quote>&gt; &gt;&gt; interrupts for disposal. E.g. the Intel E800 NIC has more than 2048</span>
<span class=quote>&gt; &gt;&gt; network queue pairs for each PF.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; Clearly, this example is a hypothetical and represents a poorly</span>
<span class=quote>&gt; &gt; designed platform.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; Having just 16 IDs per-Core is a very poor design choice. In fact, the</span>
<span class=quote>&gt; &gt; Server SoC spec mandates a minimum 255 IDs.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; You are misreading. A 128c system with 2047 MSIs per-core, will only</span>
<span class=quote>&gt; have 16 *per-core unique* (2047/128) interrupts with the current series.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; I'm not saying that each IMSIC has 16 IDs, I'm saying that in a 128c</span>
<span class=quote>&gt; system with the maximum amount of MSIs possible in the spec, you'll end</span>
<span class=quote>&gt; up with 16 *unique* interrupts per core.</span>

-ENOPARSE

I don't see how this applies to the current approach because we treat
MSI ID space as global across cores so if a system has 2047 MSIs
per-core then we have 2047 MSIs across all cores.
<span class=quote>
&gt;</span>
<span class=quote>&gt; &gt; Regarding NICs which support a large number of queues, the driver</span>
<span class=quote>&gt; &gt; will typically enable only one queue per-core and set the affinity to</span>
<span class=quote>&gt; &gt; separate cores. We have user-space data plane applications based</span>
<span class=quote>&gt; &gt; on DPDK which are capable of using a large number of NIC queues</span>
<span class=quote>&gt; &gt; but these applications are polling based and don't use MSIs.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; That's one sample point, and clearly not the only one. There are *many*</span>
<span class=quote>&gt; different usage models. Just because you *assign* MSI, doesn't mean they</span>
<span class=quote>&gt; are firing all the time.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; I can show you a couple of networking setups where this is clearly not</span>
<span class=quote>&gt; enough. Each core has a large number of QoS queues, and each queue would</span>
<span class=quote>&gt; very much like to have a dedicated MSI.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; When we encounter a system requiring a large number of MSIs,</span>
<span class=quote>&gt; &gt;&gt; &gt; we can either:</span>
<span class=quote>&gt; &gt;&gt; &gt; 1) Extend the AIA spec to support greater than 2047 IDs</span>
<span class=quote>&gt; &gt;&gt; &gt; 2) Re-think the approach in the IMSIC driver</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; The choice between #1 and #2 above depends on the</span>
<span class=quote>&gt; &gt;&gt; &gt; guarantees we want for irq_set_affinity().</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; The irq_set_affinity() behavior is better with this series, but I think</span>
<span class=quote>&gt; &gt;&gt; the other downsides: number of available interrupt sources, and IPI</span>
<span class=quote>&gt; &gt;&gt; broadcast are worse.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; The IPI overhead in the approach you are suggesting will be</span>
<span class=quote>&gt; &gt; even bad compared to the IPI overhead of the current approach</span>
<span class=quote>&gt; &gt; because we will end-up doing IPI upon every irq_set_affinity()</span>
<span class=quote>&gt; &gt; in the suggested approach compared to doing IPI upon every</span>
<span class=quote>&gt; &gt; mask/unmask in the current approach.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Again, very workload dependent.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; This series does IPI broadcast on masking/unmasking, which means that</span>
<span class=quote>&gt; cores that don't care get interrupted because, say, a network queue-pair</span>
<span class=quote>&gt; is setup on another core.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Some workloads never change the irq affinity.</span>

There are various events which irq affinity such as irq balance,
CPU hotplug, system suspend, etc.

Also, the 1-1 approach does IPI upon set_affinity, mask and
unmask whereas the 1-n approach does IPI only upon mask
and unmask.
<span class=quote>
&gt;</span>
<span class=quote>&gt; I'm just pointing out that there are pro/cons with both variants.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; &gt; The biggest advantage of the current approach is a reliable</span>
<span class=quote>&gt; &gt; irq_set_affinity() which is a very valuable thing to have.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; ...and I'm arguing that we're paying a big price for that.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; &gt; ARM systems easily support a large number of LPIs per-core.</span>
<span class=quote>&gt; &gt; For example, GIC-700 supports 56000 LPIs per-core.</span>
<span class=quote>&gt; &gt; (Refer, https://developer.arm.com/documentation/101516/0300/About-the-GIC-700/Features)</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Yeah, but this is not the GIC. This is something that looks more like</span>
<span class=quote>&gt; the x86 world. We'll be stuck with a lot of implementations with AIA 1.0</span>
<span class=quote>&gt; spec, and many cores.</span>

Well, RISC-V AIA is neigher ARM GIG not x86 APIC. All I am saying
is that there are systems with large number per-core interrupt IDs
for handling MSIs.
<span class=quote>
&gt;</span>
<span class=quote>&gt; &gt; In the RISC-V world, we can easily define a small fast track</span>
<span class=quote>&gt; &gt; extension based on S*csrind extension which can allow a</span>
<span class=quote>&gt; &gt; large number of IMSIC IDs per-core.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; Instead of addressing problems on a hypothetical system,</span>
<span class=quote>&gt; &gt; I suggest we go ahead with the current approach and deal</span>
<span class=quote>&gt; &gt; with a system having MSI over-subscription when such a</span>
<span class=quote>&gt; &gt; system shows up.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; I've pointed out my concerns. We're not agreeing, but hey, I'm just one</span>
<span class=quote>&gt; sample point here! I'll leave it here for others to chime in!</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Still much appreciate all the hard work on the series!</span>

Thanks, we have disagreements on this topic but this is
certainly a good discussion.
<span class=quote>
&gt;</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Have a nice weekend,</span>

Regards,
Anup
</pre>
</div>
<a name=25563877></a>
<div class=comment>
<div class=meta>
 <span><a href="https://patchwork.kernel.org/project/linux-riscv/list/?submitter=198739">Björn Töpel</a></span>
 <span class=pull-right>Oct. 20, 2023, 7:45 p.m. UTC | <a href=https://patchwork.kernel.org/comment/25563877/>#9</a></span>
</div>
<pre class=content>Anup Patel &lt;apatel@ventanamicro.com&gt; writes:
<span class=quote>
&gt; On Fri, Oct 20, 2023 at 10:07 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt; On Fri, Oct 20, 2023 at 8:10 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; On Fri, Oct 20, 2023 at 2:17 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; Thanks for the quick reply!</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt; On Thu, Oct 19, 2023 at 7:13 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Hi Anup,</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; The RISC-V AIA specification is ratified as-per the RISC-V international</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; process. The latest ratified AIA specifcation can be found at:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; https://github.com/riscv/riscv-aia/releases/download/1.0/riscv-interrupts-1.0.pdf</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; At a high-level, the AIA specification adds three things:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 1) AIA CSRs</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Improved local interrupt support</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 2) Incoming Message Signaled Interrupt Controller (IMSIC)</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Per-HART MSI controller</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Support MSI virtualization</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Support IPI along with virtualization</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 3) Advanced Platform-Level Interrupt Controller (APLIC)</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Wired interrupt controller</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - In MSI-mode, converts wired interrupt into MSIs (i.e. MSI generator)</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - In Direct-mode, injects external interrupts directly into HARTs</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Thanks for working on the AIA support! I had a look at the series, and</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; have some concerns about interrupt ID abstraction.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; A bit of background, for readers not familiar with the AIA details.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; IMSIC allows for 2047 unique MSI ("msi-irq") sources per hart, and</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; each MSI is dedicated to a certain hart. The series takes the approach</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; to say that there are, e.g., 2047 interrupts ("lnx-irq") globally.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Each lnx-irq consists of #harts * msi-irq -- a slice -- and in the</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; slice only *one* msi-irq is acutally used.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; This scheme makes affinity changes more robust, because the interrupt</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; sources on "other" harts are pre-allocated. On the other hand it</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; requires to propagate irq masking to other harts via IPIs (this is</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; mostly done up setup/tear down). It's also wasteful, because msi-irqs</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; are hogged, and cannot be used.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Contemporary storage/networking drivers usually uses queues per core</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; (or a sub-set of cores). The current scheme wastes a lot of msi-irqs.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; If we instead used a scheme where "msi-irq == lnx-irq", instead of</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; "lnq-irq = {hart 0;msi-irq x , ... hart N;msi-irq x}", there would be</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; a lot MSIs for other users. 1-1 vs 1-N. E.g., if a storage device</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; would like to use 5 queues (5 cores) on a 128 core system, the current</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; scheme would consume 5 * 128 MSIs, instead of just 5.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; On the plus side:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Changing interrupts affinity will never fail, because the interrupts</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   on each hart is pre-allocated.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; On the negative side:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Wasteful interrupt usage, and a system can potientially "run out" of</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   interrupts. Especially for many core systems.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Interrupt masking need to proagate to harts via IPIs (there's no</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   broadcast csr in IMSIC), and a more complex locking scheme IMSIC</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Summary:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; The current series caps the number of global interrupts to maximum</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 2047 MSIs for all cores (whole system). A better scheme, IMO, would be</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; to expose 2047 * #harts unique MSIs.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; I think this could simplify/remove(?) the locking as well.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt; Exposing 2047 * #harts unique MSIs has multiple issues:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt; 1) The irq_set_affinity() does not work for MSIs because each</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;      IRQ is not tied to a particular HART. This means we can't</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; &gt;      balance the IRQ processing load among HARTs.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; Yes, you can balance. In your code, each *active* MSI is still</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; bound/active to a specific hard together with the affinity mask. In an</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; 1-1 model you would still need to track the affinity mask, but the</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; irq_set_affinity() would be different. It would try to allocate a new</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; MSI from the target CPU, and then switch to having that MSI active.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; That's what x86 does AFAIU, which is also constrained by the # of</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; available MSIs.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; The downside, as I pointed out, is that the set affinity action can</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; fail for a certain target CPU.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; Yes, irq_set_affinity() can fail for the suggested approach plus for</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; RISC-V AIA, one HART does not have access to other HARTs</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; MSI enable/disable bits so the approach will also involve IPI.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; Correct, but the current series does a broadcast to all cores, where the</span>
<span class=quote>&gt;&gt; &gt;&gt; 1-1 approach is at most an IPI to a single core.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; 128+c machines are getting more common, and you have devices that you</span>
<span class=quote>&gt;&gt; &gt;&gt; bring up/down on a per-core basis. Broadcasting IPIs to all cores, when</span>
<span class=quote>&gt;&gt; &gt;&gt; dealing with a per-core activity is a pretty noisy neighbor.</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; Broadcast IPI in the current approach is only done upon MSI mask/unmask</span>
<span class=quote>&gt;&gt; &gt; operation. It is not done upon set_affinity() of interrupt handling.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; I'm aware. We're on the same page here.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; This could be fixed in the existing 1-n approach, by not require to sync</span>
<span class=quote>&gt;&gt; &gt;&gt; the cores that are not handling the MSI in question. "Lazy disable"</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; Incorrect. The approach you are suggesting involves an IPI upon every</span>
<span class=quote>&gt;&gt; &gt; irq_set_affinity(). This is because a HART can only enable it's own</span>
<span class=quote>&gt;&gt; &gt; MSI ID so when an IRQ is moved to from HART A to HART B with</span>
<span class=quote>&gt;&gt; &gt; a different ID X on HART B then we will need an IPI in irq_set_affinit()</span>
<span class=quote>&gt;&gt; &gt; to enable ID X on HART B.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Yes, the 1-1 approach will require an IPI to one target cpu on affinity</span>
<span class=quote>&gt;&gt; changes, and similar on mask/unmask.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; The 1-n approach, require no-IPI on affinity changes (nice!), but IPI</span>
<span class=quote>&gt;&gt; broadcast to all cores on mask/unmask (not so nice).</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; My concern is interrupts become a scarce resource with this</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; implementation, but maybe my view is incorrect. I've seen bare-metal</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; x86 systems (no VMs) with ~200 cores, and ~2000 interrupts, but maybe</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; that is considered "a lot of interrupts".</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; As long as we don't get into scenarios where we're running out of</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; interrupts, due to the software design.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; The current approach is simpler and ensures irq_set_affinity</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; always works. The limit of max 2047 IDs is sufficient for many</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; systems (if not all).</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; Let me give you another view. On a 128c system each core has ~16 unique</span>
<span class=quote>&gt;&gt; &gt;&gt; interrupts for disposal. E.g. the Intel E800 NIC has more than 2048</span>
<span class=quote>&gt;&gt; &gt;&gt; network queue pairs for each PF.</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; Clearly, this example is a hypothetical and represents a poorly</span>
<span class=quote>&gt;&gt; &gt; designed platform.</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; Having just 16 IDs per-Core is a very poor design choice. In fact, the</span>
<span class=quote>&gt;&gt; &gt; Server SoC spec mandates a minimum 255 IDs.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; You are misreading. A 128c system with 2047 MSIs per-core, will only</span>
<span class=quote>&gt;&gt; have 16 *per-core unique* (2047/128) interrupts with the current series.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; I'm not saying that each IMSIC has 16 IDs, I'm saying that in a 128c</span>
<span class=quote>&gt;&gt; system with the maximum amount of MSIs possible in the spec, you'll end</span>
<span class=quote>&gt;&gt; up with 16 *unique* interrupts per core.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; -ENOPARSE</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; I don't see how this applies to the current approach because we treat</span>
<span class=quote>&gt; MSI ID space as global across cores so if a system has 2047 MSIs</span>
<span class=quote>&gt; per-core then we have 2047 MSIs across all cores.</span>

Ok, I'll try again! :-)

Let's assume that each core in the 128c system has some per-core
resources, say a two NIC queue pairs, and a storage queue pair. This
will consume, e.g., 2*2 + 2 (6) MSI sources from the global namespace.

If each core does this it'll be 6*128 MSI sources of the global
namespace.

The maximum number of "privates" MSI sources a core can utilize is 16.

I'm trying (it's does seem to go that well ;-)) to point out that it's
only 16 unique sources per core. For, say, a 256 core system it would be
8. 2047 MSI sources in a system is not much.

Say that I want to spin up 24 NIC queues with one MSI each on each core
on my 128c system. That's not possible with this series, while with an
1-1 system it wouldn't be an issue.

Clearer, or still weird?
<span class=quote>
&gt;</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt; Regarding NICs which support a large number of queues, the driver</span>
<span class=quote>&gt;&gt; &gt; will typically enable only one queue per-core and set the affinity to</span>
<span class=quote>&gt;&gt; &gt; separate cores. We have user-space data plane applications based</span>
<span class=quote>&gt;&gt; &gt; on DPDK which are capable of using a large number of NIC queues</span>
<span class=quote>&gt;&gt; &gt; but these applications are polling based and don't use MSIs.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; That's one sample point, and clearly not the only one. There are *many*</span>
<span class=quote>&gt;&gt; different usage models. Just because you *assign* MSI, doesn't mean they</span>
<span class=quote>&gt;&gt; are firing all the time.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; I can show you a couple of networking setups where this is clearly not</span>
<span class=quote>&gt;&gt; enough. Each core has a large number of QoS queues, and each queue would</span>
<span class=quote>&gt;&gt; very much like to have a dedicated MSI.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; When we encounter a system requiring a large number of MSIs,</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; we can either:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; 1) Extend the AIA spec to support greater than 2047 IDs</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; 2) Re-think the approach in the IMSIC driver</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; The choice between #1 and #2 above depends on the</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; guarantees we want for irq_set_affinity().</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; The irq_set_affinity() behavior is better with this series, but I think</span>
<span class=quote>&gt;&gt; &gt;&gt; the other downsides: number of available interrupt sources, and IPI</span>
<span class=quote>&gt;&gt; &gt;&gt; broadcast are worse.</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; The IPI overhead in the approach you are suggesting will be</span>
<span class=quote>&gt;&gt; &gt; even bad compared to the IPI overhead of the current approach</span>
<span class=quote>&gt;&gt; &gt; because we will end-up doing IPI upon every irq_set_affinity()</span>
<span class=quote>&gt;&gt; &gt; in the suggested approach compared to doing IPI upon every</span>
<span class=quote>&gt;&gt; &gt; mask/unmask in the current approach.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Again, very workload dependent.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; This series does IPI broadcast on masking/unmasking, which means that</span>
<span class=quote>&gt;&gt; cores that don't care get interrupted because, say, a network queue-pair</span>
<span class=quote>&gt;&gt; is setup on another core.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Some workloads never change the irq affinity.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; There are various events which irq affinity such as irq balance,</span>
<span class=quote>&gt; CPU hotplug, system suspend, etc.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Also, the 1-1 approach does IPI upon set_affinity, mask and</span>
<span class=quote>&gt; unmask whereas the 1-n approach does IPI only upon mask</span>
<span class=quote>&gt; and unmask.</span>

An important distinction; When you say IPI on mask/unmask it is a
broadcast IPI to *all* cores, which is pretty instrusive.

The 1-1 variant does an IPI to a *one* target core.
<span class=quote>
&gt;&gt; I'm just pointing out that there are pro/cons with both variants.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt; The biggest advantage of the current approach is a reliable</span>
<span class=quote>&gt;&gt; &gt; irq_set_affinity() which is a very valuable thing to have.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; ...and I'm arguing that we're paying a big price for that.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt; ARM systems easily support a large number of LPIs per-core.</span>
<span class=quote>&gt;&gt; &gt; For example, GIC-700 supports 56000 LPIs per-core.</span>
<span class=quote>&gt;&gt; &gt; (Refer, https://developer.arm.com/documentation/101516/0300/About-the-GIC-700/Features)</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Yeah, but this is not the GIC. This is something that looks more like</span>
<span class=quote>&gt;&gt; the x86 world. We'll be stuck with a lot of implementations with AIA 1.0</span>
<span class=quote>&gt;&gt; spec, and many cores.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Well, RISC-V AIA is neigher ARM GIG not x86 APIC. All I am saying</span>
<span class=quote>&gt; is that there are systems with large number per-core interrupt IDs</span>
<span class=quote>&gt; for handling MSIs.</span>

Yes, and while that is nice, it's not what IMSIC is.


Now, back to the weekend for real! ;-) (https://xkcd.com/386/)
Björn
</pre>
</div>
<a name=25565097></a>
<div class=comment>
<div class=meta>
 <span><a href="https://patchwork.kernel.org/project/linux-riscv/list/?submitter=198739">Björn Töpel</a></span>
 <span class=pull-right>Oct. 23, 2023, 7:02 a.m. UTC | <a href=https://patchwork.kernel.org/comment/25565097/>#10</a></span>
</div>
<pre class=content>Björn Töpel &lt;bjorn@kernel.org&gt; writes:
<span class=quote>
&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;</span>
<span class=quote>&gt;&gt; On Fri, Oct 20, 2023 at 10:07 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt; On Fri, Oct 20, 2023 at 8:10 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt; On Fri, Oct 20, 2023 at 2:17 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; Thanks for the quick reply!</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; On Thu, Oct 19, 2023 at 7:13 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Hi Anup,</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; The RISC-V AIA specification is ratified as-per the RISC-V international</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; process. The latest ratified AIA specifcation can be found at:</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; https://github.com/riscv/riscv-aia/releases/download/1.0/riscv-interrupts-1.0.pdf</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; At a high-level, the AIA specification adds three things:</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 1) AIA CSRs</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Improved local interrupt support</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 2) Incoming Message Signaled Interrupt Controller (IMSIC)</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Per-HART MSI controller</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Support MSI virtualization</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Support IPI along with virtualization</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 3) Advanced Platform-Level Interrupt Controller (APLIC)</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Wired interrupt controller</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - In MSI-mode, converts wired interrupt into MSIs (i.e. MSI generator)</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - In Direct-mode, injects external interrupts directly into HARTs</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Thanks for working on the AIA support! I had a look at the series, and</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; have some concerns about interrupt ID abstraction.</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; A bit of background, for readers not familiar with the AIA details.</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; IMSIC allows for 2047 unique MSI ("msi-irq") sources per hart, and</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; each MSI is dedicated to a certain hart. The series takes the approach</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; to say that there are, e.g., 2047 interrupts ("lnx-irq") globally.</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Each lnx-irq consists of #harts * msi-irq -- a slice -- and in the</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; slice only *one* msi-irq is acutally used.</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; This scheme makes affinity changes more robust, because the interrupt</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; sources on "other" harts are pre-allocated. On the other hand it</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; requires to propagate irq masking to other harts via IPIs (this is</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; mostly done up setup/tear down). It's also wasteful, because msi-irqs</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; are hogged, and cannot be used.</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Contemporary storage/networking drivers usually uses queues per core</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; (or a sub-set of cores). The current scheme wastes a lot of msi-irqs.</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; If we instead used a scheme where "msi-irq == lnx-irq", instead of</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; "lnq-irq = {hart 0;msi-irq x , ... hart N;msi-irq x}", there would be</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; a lot MSIs for other users. 1-1 vs 1-N. E.g., if a storage device</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; would like to use 5 queues (5 cores) on a 128 core system, the current</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; scheme would consume 5 * 128 MSIs, instead of just 5.</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; On the plus side:</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Changing interrupts affinity will never fail, because the interrupts</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   on each hart is pre-allocated.</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; On the negative side:</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Wasteful interrupt usage, and a system can potientially "run out" of</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   interrupts. Especially for many core systems.</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Interrupt masking need to proagate to harts via IPIs (there's no</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   broadcast csr in IMSIC), and a more complex locking scheme IMSIC</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Summary:</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; The current series caps the number of global interrupts to maximum</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 2047 MSIs for all cores (whole system). A better scheme, IMO, would be</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; to expose 2047 * #harts unique MSIs.</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; I think this could simplify/remove(?) the locking as well.</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; Exposing 2047 * #harts unique MSIs has multiple issues:</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; 1) The irq_set_affinity() does not work for MSIs because each</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;      IRQ is not tied to a particular HART. This means we can't</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;      balance the IRQ processing load among HARTs.</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; Yes, you can balance. In your code, each *active* MSI is still</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; bound/active to a specific hard together with the affinity mask. In an</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; 1-1 model you would still need to track the affinity mask, but the</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; irq_set_affinity() would be different. It would try to allocate a new</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; MSI from the target CPU, and then switch to having that MSI active.</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; That's what x86 does AFAIU, which is also constrained by the # of</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; available MSIs.</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; The downside, as I pointed out, is that the set affinity action can</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; fail for a certain target CPU.</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt; Yes, irq_set_affinity() can fail for the suggested approach plus for</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt; RISC-V AIA, one HART does not have access to other HARTs</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt; MSI enable/disable bits so the approach will also involve IPI.</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; Correct, but the current series does a broadcast to all cores, where the</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; 1-1 approach is at most an IPI to a single core.</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; 128+c machines are getting more common, and you have devices that you</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; bring up/down on a per-core basis. Broadcasting IPIs to all cores, when</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; dealing with a per-core activity is a pretty noisy neighbor.</span>
<span class=quote>&gt;&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt;&gt; &gt; Broadcast IPI in the current approach is only done upon MSI mask/unmask</span>
<span class=quote>&gt;&gt;&gt; &gt; operation. It is not done upon set_affinity() of interrupt handling.</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; I'm aware. We're on the same page here.</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; This could be fixed in the existing 1-n approach, by not require to sync</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; the cores that are not handling the MSI in question. "Lazy disable"</span>
<span class=quote>&gt;&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt;&gt; &gt; Incorrect. The approach you are suggesting involves an IPI upon every</span>
<span class=quote>&gt;&gt;&gt; &gt; irq_set_affinity(). This is because a HART can only enable it's own</span>
<span class=quote>&gt;&gt;&gt; &gt; MSI ID so when an IRQ is moved to from HART A to HART B with</span>
<span class=quote>&gt;&gt;&gt; &gt; a different ID X on HART B then we will need an IPI in irq_set_affinit()</span>
<span class=quote>&gt;&gt;&gt; &gt; to enable ID X on HART B.</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; Yes, the 1-1 approach will require an IPI to one target cpu on affinity</span>
<span class=quote>&gt;&gt;&gt; changes, and similar on mask/unmask.</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; The 1-n approach, require no-IPI on affinity changes (nice!), but IPI</span>
<span class=quote>&gt;&gt;&gt; broadcast to all cores on mask/unmask (not so nice).</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; My concern is interrupts become a scarce resource with this</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; implementation, but maybe my view is incorrect. I've seen bare-metal</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; x86 systems (no VMs) with ~200 cores, and ~2000 interrupts, but maybe</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; that is considered "a lot of interrupts".</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; As long as we don't get into scenarios where we're running out of</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt; interrupts, due to the software design.</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt; The current approach is simpler and ensures irq_set_affinity</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt; always works. The limit of max 2047 IDs is sufficient for many</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt; systems (if not all).</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; Let me give you another view. On a 128c system each core has ~16 unique</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; interrupts for disposal. E.g. the Intel E800 NIC has more than 2048</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; network queue pairs for each PF.</span>
<span class=quote>&gt;&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt;&gt; &gt; Clearly, this example is a hypothetical and represents a poorly</span>
<span class=quote>&gt;&gt;&gt; &gt; designed platform.</span>
<span class=quote>&gt;&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt;&gt; &gt; Having just 16 IDs per-Core is a very poor design choice. In fact, the</span>
<span class=quote>&gt;&gt;&gt; &gt; Server SoC spec mandates a minimum 255 IDs.</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; You are misreading. A 128c system with 2047 MSIs per-core, will only</span>
<span class=quote>&gt;&gt;&gt; have 16 *per-core unique* (2047/128) interrupts with the current series.</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; I'm not saying that each IMSIC has 16 IDs, I'm saying that in a 128c</span>
<span class=quote>&gt;&gt;&gt; system with the maximum amount of MSIs possible in the spec, you'll end</span>
<span class=quote>&gt;&gt;&gt; up with 16 *unique* interrupts per core.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; -ENOPARSE</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; I don't see how this applies to the current approach because we treat</span>
<span class=quote>&gt;&gt; MSI ID space as global across cores so if a system has 2047 MSIs</span>
<span class=quote>&gt;&gt; per-core then we have 2047 MSIs across all cores.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Ok, I'll try again! :-)</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Let's assume that each core in the 128c system has some per-core</span>
<span class=quote>&gt; resources, say a two NIC queue pairs, and a storage queue pair. This</span>
<span class=quote>&gt; will consume, e.g., 2*2 + 2 (6) MSI sources from the global namespace.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; If each core does this it'll be 6*128 MSI sources of the global</span>
<span class=quote>&gt; namespace.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; The maximum number of "privates" MSI sources a core can utilize is 16.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; I'm trying (it's does seem to go that well ;-)) to point out that it's</span>
<span class=quote>&gt; only 16 unique sources per core. For, say, a 256 core system it would be</span>
<span class=quote>&gt; 8. 2047 MSI sources in a system is not much.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Say that I want to spin up 24 NIC queues with one MSI each on each core</span>
<span class=quote>&gt; on my 128c system. That's not possible with this series, while with an</span>
<span class=quote>&gt; 1-1 system it wouldn't be an issue.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Clearer, or still weird?</span>
<span class=quote>&gt;</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt; Regarding NICs which support a large number of queues, the driver</span>
<span class=quote>&gt;&gt;&gt; &gt; will typically enable only one queue per-core and set the affinity to</span>
<span class=quote>&gt;&gt;&gt; &gt; separate cores. We have user-space data plane applications based</span>
<span class=quote>&gt;&gt;&gt; &gt; on DPDK which are capable of using a large number of NIC queues</span>
<span class=quote>&gt;&gt;&gt; &gt; but these applications are polling based and don't use MSIs.</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; That's one sample point, and clearly not the only one. There are *many*</span>
<span class=quote>&gt;&gt;&gt; different usage models. Just because you *assign* MSI, doesn't mean they</span>
<span class=quote>&gt;&gt;&gt; are firing all the time.</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; I can show you a couple of networking setups where this is clearly not</span>
<span class=quote>&gt;&gt;&gt; enough. Each core has a large number of QoS queues, and each queue would</span>
<span class=quote>&gt;&gt;&gt; very much like to have a dedicated MSI.</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt; When we encounter a system requiring a large number of MSIs,</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt; we can either:</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt; 1) Extend the AIA spec to support greater than 2047 IDs</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt; 2) Re-think the approach in the IMSIC driver</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt; The choice between #1 and #2 above depends on the</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; &gt; guarantees we want for irq_set_affinity().</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; The irq_set_affinity() behavior is better with this series, but I think</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; the other downsides: number of available interrupt sources, and IPI</span>
<span class=quote>&gt;&gt;&gt; &gt;&gt; broadcast are worse.</span>
<span class=quote>&gt;&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt;&gt; &gt; The IPI overhead in the approach you are suggesting will be</span>
<span class=quote>&gt;&gt;&gt; &gt; even bad compared to the IPI overhead of the current approach</span>
<span class=quote>&gt;&gt;&gt; &gt; because we will end-up doing IPI upon every irq_set_affinity()</span>
<span class=quote>&gt;&gt;&gt; &gt; in the suggested approach compared to doing IPI upon every</span>
<span class=quote>&gt;&gt;&gt; &gt; mask/unmask in the current approach.</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; Again, very workload dependent.</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; This series does IPI broadcast on masking/unmasking, which means that</span>
<span class=quote>&gt;&gt;&gt; cores that don't care get interrupted because, say, a network queue-pair</span>
<span class=quote>&gt;&gt;&gt; is setup on another core.</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; Some workloads never change the irq affinity.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; There are various events which irq affinity such as irq balance,</span>
<span class=quote>&gt;&gt; CPU hotplug, system suspend, etc.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Also, the 1-1 approach does IPI upon set_affinity, mask and</span>
<span class=quote>&gt;&gt; unmask whereas the 1-n approach does IPI only upon mask</span>
<span class=quote>&gt;&gt; and unmask.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; An important distinction; When you say IPI on mask/unmask it is a</span>
<span class=quote>&gt; broadcast IPI to *all* cores, which is pretty instrusive.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; The 1-1 variant does an IPI to a *one* target core.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt;&gt;&gt; I'm just pointing out that there are pro/cons with both variants.</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt; The biggest advantage of the current approach is a reliable</span>
<span class=quote>&gt;&gt;&gt; &gt; irq_set_affinity() which is a very valuable thing to have.</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; ...and I'm arguing that we're paying a big price for that.</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; &gt; ARM systems easily support a large number of LPIs per-core.</span>
<span class=quote>&gt;&gt;&gt; &gt; For example, GIC-700 supports 56000 LPIs per-core.</span>
<span class=quote>&gt;&gt;&gt; &gt; (Refer, https://developer.arm.com/documentation/101516/0300/About-the-GIC-700/Features)</span>
<span class=quote>&gt;&gt;&gt;</span>
<span class=quote>&gt;&gt;&gt; Yeah, but this is not the GIC. This is something that looks more like</span>
<span class=quote>&gt;&gt;&gt; the x86 world. We'll be stuck with a lot of implementations with AIA 1.0</span>
<span class=quote>&gt;&gt;&gt; spec, and many cores.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Well, RISC-V AIA is neigher ARM GIG not x86 APIC. All I am saying</span>
<span class=quote>&gt;&gt; is that there are systems with large number per-core interrupt IDs</span>
<span class=quote>&gt;&gt; for handling MSIs.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Yes, and while that is nice, it's not what IMSIC is.</span>

Some follow-ups, after thinking more about it more over the weekend.

* Do one really need an IPI for irq_set_affinity() for the 1-1 model?
  Why touch the enable/disable bits when moving interrupts?
  
* In my book the IMSIC looks very much like the x86 LAPIC, which also
  has few interrupts (IMSIC &lt;2048, LAPIC 256). The IRQ matrix allocator
  [1], and a scheme similar to LAPIC [2] would be a good fit. This is
  the 1-1 model, but more sophisticated than what I've been describing
  (e.g. properly handling mangaged/regular irqs). As a bonus we would
  get the IRQ matrix debugfs/tracepoint support.
  
  
Björn
</pre>
</div>
<a name=25565238></a>
<div class=comment>
<div class=meta>
 <span><a href="https://patchwork.kernel.org/project/linux-riscv/list/?submitter=204452">Anup Patel</a></span>
 <span class=pull-right>Oct. 23, 2023, 8:34 a.m. UTC | <a href=https://patchwork.kernel.org/comment/25565238/>#11</a></span>
</div>
<pre class=content>On Mon, Oct 23, 2023 at 12:32 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:
<span class=quote>&gt;</span>
<span class=quote>&gt; Björn Töpel &lt;bjorn@kernel.org&gt; writes:</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; &gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; On Fri, Oct 20, 2023 at 10:07 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; On Fri, Oct 20, 2023 at 8:10 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt; On Fri, Oct 20, 2023 at 2:17 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thanks for the quick reply!</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; On Thu, Oct 19, 2023 at 7:13 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Hi Anup,</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; The RISC-V AIA specification is ratified as-per the RISC-V international</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; process. The latest ratified AIA specifcation can be found at:</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; https://github.com/riscv/riscv-aia/releases/download/1.0/riscv-interrupts-1.0.pdf</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; At a high-level, the AIA specification adds three things:</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 1) AIA CSRs</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Improved local interrupt support</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 2) Incoming Message Signaled Interrupt Controller (IMSIC)</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Per-HART MSI controller</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Support MSI virtualization</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Support IPI along with virtualization</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 3) Advanced Platform-Level Interrupt Controller (APLIC)</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Wired interrupt controller</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - In MSI-mode, converts wired interrupt into MSIs (i.e. MSI generator)</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - In Direct-mode, injects external interrupts directly into HARTs</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Thanks for working on the AIA support! I had a look at the series, and</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; have some concerns about interrupt ID abstraction.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; A bit of background, for readers not familiar with the AIA details.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; IMSIC allows for 2047 unique MSI ("msi-irq") sources per hart, and</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; each MSI is dedicated to a certain hart. The series takes the approach</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; to say that there are, e.g., 2047 interrupts ("lnx-irq") globally.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Each lnx-irq consists of #harts * msi-irq -- a slice -- and in the</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; slice only *one* msi-irq is acutally used.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; This scheme makes affinity changes more robust, because the interrupt</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; sources on "other" harts are pre-allocated. On the other hand it</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; requires to propagate irq masking to other harts via IPIs (this is</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; mostly done up setup/tear down). It's also wasteful, because msi-irqs</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; are hogged, and cannot be used.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Contemporary storage/networking drivers usually uses queues per core</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; (or a sub-set of cores). The current scheme wastes a lot of msi-irqs.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; If we instead used a scheme where "msi-irq == lnx-irq", instead of</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; "lnq-irq = {hart 0;msi-irq x , ... hart N;msi-irq x}", there would be</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; a lot MSIs for other users. 1-1 vs 1-N. E.g., if a storage device</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; would like to use 5 queues (5 cores) on a 128 core system, the current</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; scheme would consume 5 * 128 MSIs, instead of just 5.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; On the plus side:</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Changing interrupts affinity will never fail, because the interrupts</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   on each hart is pre-allocated.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; On the negative side:</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Wasteful interrupt usage, and a system can potientially "run out" of</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   interrupts. Especially for many core systems.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Interrupt masking need to proagate to harts via IPIs (there's no</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   broadcast csr in IMSIC), and a more complex locking scheme IMSIC</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Summary:</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; The current series caps the number of global interrupts to maximum</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 2047 MSIs for all cores (whole system). A better scheme, IMO, would be</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; to expose 2047 * #harts unique MSIs.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; I think this could simplify/remove(?) the locking as well.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; Exposing 2047 * #harts unique MSIs has multiple issues:</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; 1) The irq_set_affinity() does not work for MSIs because each</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;      IRQ is not tied to a particular HART. This means we can't</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;      balance the IRQ processing load among HARTs.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Yes, you can balance. In your code, each *active* MSI is still</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; bound/active to a specific hard together with the affinity mask. In an</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; 1-1 model you would still need to track the affinity mask, but the</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; irq_set_affinity() would be different. It would try to allocate a new</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; MSI from the target CPU, and then switch to having that MSI active.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; That's what x86 does AFAIU, which is also constrained by the # of</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; available MSIs.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; The downside, as I pointed out, is that the set affinity action can</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; fail for a certain target CPU.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt; Yes, irq_set_affinity() can fail for the suggested approach plus for</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt; RISC-V AIA, one HART does not have access to other HARTs</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt; MSI enable/disable bits so the approach will also involve IPI.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; Correct, but the current series does a broadcast to all cores, where the</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; 1-1 approach is at most an IPI to a single core.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; 128+c machines are getting more common, and you have devices that you</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; bring up/down on a per-core basis. Broadcasting IPIs to all cores, when</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; dealing with a per-core activity is a pretty noisy neighbor.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; Broadcast IPI in the current approach is only done upon MSI mask/unmask</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; operation. It is not done upon set_affinity() of interrupt handling.</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; I'm aware. We're on the same page here.</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; This could be fixed in the existing 1-n approach, by not require to sync</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; the cores that are not handling the MSI in question. "Lazy disable"</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; Incorrect. The approach you are suggesting involves an IPI upon every</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; irq_set_affinity(). This is because a HART can only enable it's own</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; MSI ID so when an IRQ is moved to from HART A to HART B with</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; a different ID X on HART B then we will need an IPI in irq_set_affinit()</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; to enable ID X on HART B.</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; Yes, the 1-1 approach will require an IPI to one target cpu on affinity</span>
<span class=quote>&gt; &gt;&gt;&gt; changes, and similar on mask/unmask.</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; The 1-n approach, require no-IPI on affinity changes (nice!), but IPI</span>
<span class=quote>&gt; &gt;&gt;&gt; broadcast to all cores on mask/unmask (not so nice).</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; My concern is interrupts become a scarce resource with this</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; implementation, but maybe my view is incorrect. I've seen bare-metal</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; x86 systems (no VMs) with ~200 cores, and ~2000 interrupts, but maybe</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; that is considered "a lot of interrupts".</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; As long as we don't get into scenarios where we're running out of</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; interrupts, due to the software design.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt; The current approach is simpler and ensures irq_set_affinity</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt; always works. The limit of max 2047 IDs is sufficient for many</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt; systems (if not all).</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; Let me give you another view. On a 128c system each core has ~16 unique</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; interrupts for disposal. E.g. the Intel E800 NIC has more than 2048</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; network queue pairs for each PF.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; Clearly, this example is a hypothetical and represents a poorly</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; designed platform.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; Having just 16 IDs per-Core is a very poor design choice. In fact, the</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; Server SoC spec mandates a minimum 255 IDs.</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; You are misreading. A 128c system with 2047 MSIs per-core, will only</span>
<span class=quote>&gt; &gt;&gt;&gt; have 16 *per-core unique* (2047/128) interrupts with the current series.</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; I'm not saying that each IMSIC has 16 IDs, I'm saying that in a 128c</span>
<span class=quote>&gt; &gt;&gt;&gt; system with the maximum amount of MSIs possible in the spec, you'll end</span>
<span class=quote>&gt; &gt;&gt;&gt; up with 16 *unique* interrupts per core.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; -ENOPARSE</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; I don't see how this applies to the current approach because we treat</span>
<span class=quote>&gt; &gt;&gt; MSI ID space as global across cores so if a system has 2047 MSIs</span>
<span class=quote>&gt; &gt;&gt; per-core then we have 2047 MSIs across all cores.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; Ok, I'll try again! :-)</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; Let's assume that each core in the 128c system has some per-core</span>
<span class=quote>&gt; &gt; resources, say a two NIC queue pairs, and a storage queue pair. This</span>
<span class=quote>&gt; &gt; will consume, e.g., 2*2 + 2 (6) MSI sources from the global namespace.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; If each core does this it'll be 6*128 MSI sources of the global</span>
<span class=quote>&gt; &gt; namespace.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; The maximum number of "privates" MSI sources a core can utilize is 16.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; I'm trying (it's does seem to go that well ;-)) to point out that it's</span>
<span class=quote>&gt; &gt; only 16 unique sources per core. For, say, a 256 core system it would be</span>
<span class=quote>&gt; &gt; 8. 2047 MSI sources in a system is not much.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; Say that I want to spin up 24 NIC queues with one MSI each on each core</span>
<span class=quote>&gt; &gt; on my 128c system. That's not possible with this series, while with an</span>
<span class=quote>&gt; &gt; 1-1 system it wouldn't be an issue.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; Clearer, or still weird?</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; Regarding NICs which support a large number of queues, the driver</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; will typically enable only one queue per-core and set the affinity to</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; separate cores. We have user-space data plane applications based</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; on DPDK which are capable of using a large number of NIC queues</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; but these applications are polling based and don't use MSIs.</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; That's one sample point, and clearly not the only one. There are *many*</span>
<span class=quote>&gt; &gt;&gt;&gt; different usage models. Just because you *assign* MSI, doesn't mean they</span>
<span class=quote>&gt; &gt;&gt;&gt; are firing all the time.</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; I can show you a couple of networking setups where this is clearly not</span>
<span class=quote>&gt; &gt;&gt;&gt; enough. Each core has a large number of QoS queues, and each queue would</span>
<span class=quote>&gt; &gt;&gt;&gt; very much like to have a dedicated MSI.</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt; When we encounter a system requiring a large number of MSIs,</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt; we can either:</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt; 1) Extend the AIA spec to support greater than 2047 IDs</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt; 2) Re-think the approach in the IMSIC driver</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt; The choice between #1 and #2 above depends on the</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; &gt; guarantees we want for irq_set_affinity().</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; The irq_set_affinity() behavior is better with this series, but I think</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; the other downsides: number of available interrupt sources, and IPI</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;&gt; broadcast are worse.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; The IPI overhead in the approach you are suggesting will be</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; even bad compared to the IPI overhead of the current approach</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; because we will end-up doing IPI upon every irq_set_affinity()</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; in the suggested approach compared to doing IPI upon every</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; mask/unmask in the current approach.</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; Again, very workload dependent.</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; This series does IPI broadcast on masking/unmasking, which means that</span>
<span class=quote>&gt; &gt;&gt;&gt; cores that don't care get interrupted because, say, a network queue-pair</span>
<span class=quote>&gt; &gt;&gt;&gt; is setup on another core.</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; Some workloads never change the irq affinity.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; There are various events which irq affinity such as irq balance,</span>
<span class=quote>&gt; &gt;&gt; CPU hotplug, system suspend, etc.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; Also, the 1-1 approach does IPI upon set_affinity, mask and</span>
<span class=quote>&gt; &gt;&gt; unmask whereas the 1-n approach does IPI only upon mask</span>
<span class=quote>&gt; &gt;&gt; and unmask.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; An important distinction; When you say IPI on mask/unmask it is a</span>
<span class=quote>&gt; &gt; broadcast IPI to *all* cores, which is pretty instrusive.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; The 1-1 variant does an IPI to a *one* target core.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; I'm just pointing out that there are pro/cons with both variants.</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; The biggest advantage of the current approach is a reliable</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; irq_set_affinity() which is a very valuable thing to have.</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; ...and I'm arguing that we're paying a big price for that.</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; ARM systems easily support a large number of LPIs per-core.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; For example, GIC-700 supports 56000 LPIs per-core.</span>
<span class=quote>&gt; &gt;&gt;&gt; &gt; (Refer, https://developer.arm.com/documentation/101516/0300/About-the-GIC-700/Features)</span>
<span class=quote>&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt;&gt; Yeah, but this is not the GIC. This is something that looks more like</span>
<span class=quote>&gt; &gt;&gt;&gt; the x86 world. We'll be stuck with a lot of implementations with AIA 1.0</span>
<span class=quote>&gt; &gt;&gt;&gt; spec, and many cores.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; Well, RISC-V AIA is neigher ARM GIG not x86 APIC. All I am saying</span>
<span class=quote>&gt; &gt;&gt; is that there are systems with large number per-core interrupt IDs</span>
<span class=quote>&gt; &gt;&gt; for handling MSIs.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; Yes, and while that is nice, it's not what IMSIC is.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Some follow-ups, after thinking more about it more over the weekend.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; * Do one really need an IPI for irq_set_affinity() for the 1-1 model?</span>
<span class=quote>&gt;   Why touch the enable/disable bits when moving interrupts?</span>

In the 1-1 model, the ID on the current HART and target HART upon
irq_set_affinity will be different so we can't leave the unused ID on
current HART enabled because it can lead to spurious interrupts
when the ID on current HART is re-used for some other device.

There is also a possibility of receiving an interrupt while the ID was
moved to a new target HART in-which case we have to detect and
re-trigger interrupt on the new target HART. In fact, x86 APLIC does
an IPI to take care of this case.
<span class=quote>
&gt;</span>
<span class=quote>&gt; * In my book the IMSIC looks very much like the x86 LAPIC, which also</span>
<span class=quote>&gt;   has few interrupts (IMSIC &lt;2048, LAPIC 256). The IRQ matrix allocator</span>
<span class=quote>&gt;   [1], and a scheme similar to LAPIC [2] would be a good fit. This is</span>
<span class=quote>&gt;   the 1-1 model, but more sophisticated than what I've been describing</span>
<span class=quote>&gt;   (e.g. properly handling mangaged/regular irqs). As a bonus we would</span>
<span class=quote>&gt;   get the IRQ matrix debugfs/tracepoint support.</span>
<span class=quote>&gt;</span>

Yes, I have been evaluating the 1-1 model for the past few days. I also
have a working implementation with a simple per-CPU bitmap based
allocator which handles both legacy MSI (block of 1,2,4,8,16, or 32 IDs)
and MSI-X.

The irq matrix allocator needs to be improved for handling legacy MSI
so initially I will post a v11 series which works for me and converging
with irq matrix allocator can be future work.

Regards,
Anup
</pre>
</div>
<a name=25565925></a>
<div class=comment>
<div class=meta>
 <span><a href="https://patchwork.kernel.org/project/linux-riscv/list/?submitter=198739">Björn Töpel</a></span>
 <span class=pull-right>Oct. 23, 2023, 2:07 p.m. UTC | <a href=https://patchwork.kernel.org/comment/25565925/>#12</a></span>
</div>
<pre class=content>Anup Patel &lt;apatel@ventanamicro.com&gt; writes:
<span class=quote>
&gt; On Mon, Oct 23, 2023 at 12:32 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Björn Töpel &lt;bjorn@kernel.org&gt; writes:</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; On Fri, Oct 20, 2023 at 10:07 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; On Fri, Oct 20, 2023 at 8:10 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; On Fri, Oct 20, 2023 at 2:17 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thanks for the quick reply!</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; On Thu, Oct 19, 2023 at 7:13 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Hi Anup,</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; The RISC-V AIA specification is ratified as-per the RISC-V international</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; process. The latest ratified AIA specifcation can be found at:</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; https://github.com/riscv/riscv-aia/releases/download/1.0/riscv-interrupts-1.0.pdf</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; At a high-level, the AIA specification adds three things:</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 1) AIA CSRs</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Improved local interrupt support</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 2) Incoming Message Signaled Interrupt Controller (IMSIC)</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Per-HART MSI controller</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Support MSI virtualization</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Support IPI along with virtualization</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 3) Advanced Platform-Level Interrupt Controller (APLIC)</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Wired interrupt controller</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - In MSI-mode, converts wired interrupt into MSIs (i.e. MSI generator)</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - In Direct-mode, injects external interrupts directly into HARTs</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Thanks for working on the AIA support! I had a look at the series, and</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; have some concerns about interrupt ID abstraction.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; A bit of background, for readers not familiar with the AIA details.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; IMSIC allows for 2047 unique MSI ("msi-irq") sources per hart, and</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; each MSI is dedicated to a certain hart. The series takes the approach</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; to say that there are, e.g., 2047 interrupts ("lnx-irq") globally.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Each lnx-irq consists of #harts * msi-irq -- a slice -- and in the</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; slice only *one* msi-irq is acutally used.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; This scheme makes affinity changes more robust, because the interrupt</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; sources on "other" harts are pre-allocated. On the other hand it</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; requires to propagate irq masking to other harts via IPIs (this is</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; mostly done up setup/tear down). It's also wasteful, because msi-irqs</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; are hogged, and cannot be used.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Contemporary storage/networking drivers usually uses queues per core</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; (or a sub-set of cores). The current scheme wastes a lot of msi-irqs.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; If we instead used a scheme where "msi-irq == lnx-irq", instead of</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; "lnq-irq = {hart 0;msi-irq x , ... hart N;msi-irq x}", there would be</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; a lot MSIs for other users. 1-1 vs 1-N. E.g., if a storage device</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; would like to use 5 queues (5 cores) on a 128 core system, the current</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; scheme would consume 5 * 128 MSIs, instead of just 5.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; On the plus side:</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Changing interrupts affinity will never fail, because the interrupts</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   on each hart is pre-allocated.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; On the negative side:</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Wasteful interrupt usage, and a system can potientially "run out" of</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   interrupts. Especially for many core systems.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Interrupt masking need to proagate to harts via IPIs (there's no</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   broadcast csr in IMSIC), and a more complex locking scheme IMSIC</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Summary:</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; The current series caps the number of global interrupts to maximum</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 2047 MSIs for all cores (whole system). A better scheme, IMO, would be</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; to expose 2047 * #harts unique MSIs.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; I think this could simplify/remove(?) the locking as well.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; Exposing 2047 * #harts unique MSIs has multiple issues:</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; 1) The irq_set_affinity() does not work for MSIs because each</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;      IRQ is not tied to a particular HART. This means we can't</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;      balance the IRQ processing load among HARTs.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Yes, you can balance. In your code, each *active* MSI is still</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; bound/active to a specific hard together with the affinity mask. In an</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; 1-1 model you would still need to track the affinity mask, but the</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; irq_set_affinity() would be different. It would try to allocate a new</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; MSI from the target CPU, and then switch to having that MSI active.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; That's what x86 does AFAIU, which is also constrained by the # of</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; available MSIs.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; The downside, as I pointed out, is that the set affinity action can</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; fail for a certain target CPU.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Yes, irq_set_affinity() can fail for the suggested approach plus for</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; RISC-V AIA, one HART does not have access to other HARTs</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; MSI enable/disable bits so the approach will also involve IPI.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; Correct, but the current series does a broadcast to all cores, where the</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; 1-1 approach is at most an IPI to a single core.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; 128+c machines are getting more common, and you have devices that you</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; bring up/down on a per-core basis. Broadcasting IPIs to all cores, when</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; dealing with a per-core activity is a pretty noisy neighbor.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; Broadcast IPI in the current approach is only done upon MSI mask/unmask</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; operation. It is not done upon set_affinity() of interrupt handling.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; I'm aware. We're on the same page here.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; This could be fixed in the existing 1-n approach, by not require to sync</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; the cores that are not handling the MSI in question. "Lazy disable"</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; Incorrect. The approach you are suggesting involves an IPI upon every</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; irq_set_affinity(). This is because a HART can only enable it's own</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; MSI ID so when an IRQ is moved to from HART A to HART B with</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; a different ID X on HART B then we will need an IPI in irq_set_affinit()</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; to enable ID X on HART B.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; Yes, the 1-1 approach will require an IPI to one target cpu on affinity</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; changes, and similar on mask/unmask.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; The 1-n approach, require no-IPI on affinity changes (nice!), but IPI</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; broadcast to all cores on mask/unmask (not so nice).</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; My concern is interrupts become a scarce resource with this</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; implementation, but maybe my view is incorrect. I've seen bare-metal</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; x86 systems (no VMs) with ~200 cores, and ~2000 interrupts, but maybe</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; that is considered "a lot of interrupts".</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; As long as we don't get into scenarios where we're running out of</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; interrupts, due to the software design.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; The current approach is simpler and ensures irq_set_affinity</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; always works. The limit of max 2047 IDs is sufficient for many</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; systems (if not all).</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; Let me give you another view. On a 128c system each core has ~16 unique</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; interrupts for disposal. E.g. the Intel E800 NIC has more than 2048</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; network queue pairs for each PF.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; Clearly, this example is a hypothetical and represents a poorly</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; designed platform.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; Having just 16 IDs per-Core is a very poor design choice. In fact, the</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; Server SoC spec mandates a minimum 255 IDs.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; You are misreading. A 128c system with 2047 MSIs per-core, will only</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; have 16 *per-core unique* (2047/128) interrupts with the current series.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; I'm not saying that each IMSIC has 16 IDs, I'm saying that in a 128c</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; system with the maximum amount of MSIs possible in the spec, you'll end</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; up with 16 *unique* interrupts per core.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; -ENOPARSE</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; I don't see how this applies to the current approach because we treat</span>
<span class=quote>&gt;&gt; &gt;&gt; MSI ID space as global across cores so if a system has 2047 MSIs</span>
<span class=quote>&gt;&gt; &gt;&gt; per-core then we have 2047 MSIs across all cores.</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; Ok, I'll try again! :-)</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; Let's assume that each core in the 128c system has some per-core</span>
<span class=quote>&gt;&gt; &gt; resources, say a two NIC queue pairs, and a storage queue pair. This</span>
<span class=quote>&gt;&gt; &gt; will consume, e.g., 2*2 + 2 (6) MSI sources from the global namespace.</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; If each core does this it'll be 6*128 MSI sources of the global</span>
<span class=quote>&gt;&gt; &gt; namespace.</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; The maximum number of "privates" MSI sources a core can utilize is 16.</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; I'm trying (it's does seem to go that well ;-)) to point out that it's</span>
<span class=quote>&gt;&gt; &gt; only 16 unique sources per core. For, say, a 256 core system it would be</span>
<span class=quote>&gt;&gt; &gt; 8. 2047 MSI sources in a system is not much.</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; Say that I want to spin up 24 NIC queues with one MSI each on each core</span>
<span class=quote>&gt;&gt; &gt; on my 128c system. That's not possible with this series, while with an</span>
<span class=quote>&gt;&gt; &gt; 1-1 system it wouldn't be an issue.</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; Clearer, or still weird?</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; Regarding NICs which support a large number of queues, the driver</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; will typically enable only one queue per-core and set the affinity to</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; separate cores. We have user-space data plane applications based</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; on DPDK which are capable of using a large number of NIC queues</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; but these applications are polling based and don't use MSIs.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; That's one sample point, and clearly not the only one. There are *many*</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; different usage models. Just because you *assign* MSI, doesn't mean they</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; are firing all the time.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; I can show you a couple of networking setups where this is clearly not</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; enough. Each core has a large number of QoS queues, and each queue would</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; very much like to have a dedicated MSI.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; When we encounter a system requiring a large number of MSIs,</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; we can either:</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; 1) Extend the AIA spec to support greater than 2047 IDs</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; 2) Re-think the approach in the IMSIC driver</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; The choice between #1 and #2 above depends on the</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; guarantees we want for irq_set_affinity().</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; The irq_set_affinity() behavior is better with this series, but I think</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; the other downsides: number of available interrupt sources, and IPI</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;&gt; broadcast are worse.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; The IPI overhead in the approach you are suggesting will be</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; even bad compared to the IPI overhead of the current approach</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; because we will end-up doing IPI upon every irq_set_affinity()</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; in the suggested approach compared to doing IPI upon every</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; mask/unmask in the current approach.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; Again, very workload dependent.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; This series does IPI broadcast on masking/unmasking, which means that</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; cores that don't care get interrupted because, say, a network queue-pair</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; is setup on another core.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; Some workloads never change the irq affinity.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; There are various events which irq affinity such as irq balance,</span>
<span class=quote>&gt;&gt; &gt;&gt; CPU hotplug, system suspend, etc.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; Also, the 1-1 approach does IPI upon set_affinity, mask and</span>
<span class=quote>&gt;&gt; &gt;&gt; unmask whereas the 1-n approach does IPI only upon mask</span>
<span class=quote>&gt;&gt; &gt;&gt; and unmask.</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; An important distinction; When you say IPI on mask/unmask it is a</span>
<span class=quote>&gt;&gt; &gt; broadcast IPI to *all* cores, which is pretty instrusive.</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; The 1-1 variant does an IPI to a *one* target core.</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; I'm just pointing out that there are pro/cons with both variants.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; The biggest advantage of the current approach is a reliable</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; irq_set_affinity() which is a very valuable thing to have.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; ...and I'm arguing that we're paying a big price for that.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; ARM systems easily support a large number of LPIs per-core.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; For example, GIC-700 supports 56000 LPIs per-core.</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; &gt; (Refer, https://developer.arm.com/documentation/101516/0300/About-the-GIC-700/Features)</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; Yeah, but this is not the GIC. This is something that looks more like</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; the x86 world. We'll be stuck with a lot of implementations with AIA 1.0</span>
<span class=quote>&gt;&gt; &gt;&gt;&gt; spec, and many cores.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; Well, RISC-V AIA is neigher ARM GIG not x86 APIC. All I am saying</span>
<span class=quote>&gt;&gt; &gt;&gt; is that there are systems with large number per-core interrupt IDs</span>
<span class=quote>&gt;&gt; &gt;&gt; for handling MSIs.</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; Yes, and while that is nice, it's not what IMSIC is.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Some follow-ups, after thinking more about it more over the weekend.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; * Do one really need an IPI for irq_set_affinity() for the 1-1 model?</span>
<span class=quote>&gt;&gt;   Why touch the enable/disable bits when moving interrupts?</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; In the 1-1 model, the ID on the current HART and target HART upon</span>
<span class=quote>&gt; irq_set_affinity will be different so we can't leave the unused ID on</span>
<span class=quote>&gt; current HART enabled because it can lead to spurious interrupts</span>
<span class=quote>&gt; when the ID on current HART is re-used for some other device.</span>

Hmm, is this really an actual problem, or a theoretical one? The
implementation need to track what's in-use, so can we ever get into this
situation?

Somewhat related; I had a similar question for imsic_pci_{un,}mask_irq()
-- why not only do the the default mask operation (only
pci_msi_{un,}mask_irq()), but instead propagate to the IMSIC
mask/unmask?
<span class=quote>
&gt; There is also a possibility of receiving an interrupt while the ID was</span>
<span class=quote>&gt; moved to a new target HART in-which case we have to detect and</span>
<span class=quote>&gt; re-trigger interrupt on the new target HART. In fact, x86 APLIC does</span>
<span class=quote>&gt; an IPI to take care of this case.</span>

This case I get, and the implementation can track that both are in use.
It's the spurious one that I'm dubious of (don't get).
<span class=quote>
&gt;&gt;</span>
<span class=quote>&gt;&gt; * In my book the IMSIC looks very much like the x86 LAPIC, which also</span>
<span class=quote>&gt;&gt;   has few interrupts (IMSIC &lt;2048, LAPIC 256). The IRQ matrix allocator</span>
<span class=quote>&gt;&gt;   [1], and a scheme similar to LAPIC [2] would be a good fit. This is</span>
<span class=quote>&gt;&gt;   the 1-1 model, but more sophisticated than what I've been describing</span>
<span class=quote>&gt;&gt;   (e.g. properly handling mangaged/regular irqs). As a bonus we would</span>
<span class=quote>&gt;&gt;   get the IRQ matrix debugfs/tracepoint support.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Yes, I have been evaluating the 1-1 model for the past few days. I also</span>
<span class=quote>&gt; have a working implementation with a simple per-CPU bitmap based</span>
<span class=quote>&gt; allocator which handles both legacy MSI (block of 1,2,4,8,16, or 32 IDs)</span>
<span class=quote>&gt; and MSI-X.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; The irq matrix allocator needs to be improved for handling legacy MSI</span>
<span class=quote>&gt; so initially I will post a v11 series which works for me and converging</span>
<span class=quote>&gt; with irq matrix allocator can be future work.</span>

What's missing/needs to be improved for legacy MSI (legacy MSI ==
!MSI-X, right?) in the matrix allocator?


Björn
</pre>
</div>
<a name=25565994></a>
<div class=comment>
<div class=meta>
 <span><a href="https://patchwork.kernel.org/project/linux-riscv/list/?submitter=204452">Anup Patel</a></span>
 <span class=pull-right>Oct. 23, 2023, 2:41 p.m. UTC | <a href=https://patchwork.kernel.org/comment/25565994/>#13</a></span>
</div>
<pre class=content>On Mon, Oct 23, 2023 at 7:37 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:
<span class=quote>&gt;</span>
<span class=quote>&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; &gt; On Mon, Oct 23, 2023 at 12:32 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; Björn Töpel &lt;bjorn@kernel.org&gt; writes:</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; On Fri, Oct 20, 2023 at 10:07 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; On Fri, Oct 20, 2023 at 8:10 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; On Fri, Oct 20, 2023 at 2:17 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thanks for the quick reply!</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; On Thu, Oct 19, 2023 at 7:13 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Hi Anup,</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; The RISC-V AIA specification is ratified as-per the RISC-V international</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; process. The latest ratified AIA specifcation can be found at:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; https://github.com/riscv/riscv-aia/releases/download/1.0/riscv-interrupts-1.0.pdf</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; At a high-level, the AIA specification adds three things:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 1) AIA CSRs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Improved local interrupt support</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 2) Incoming Message Signaled Interrupt Controller (IMSIC)</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Per-HART MSI controller</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Support MSI virtualization</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Support IPI along with virtualization</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 3) Advanced Platform-Level Interrupt Controller (APLIC)</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Wired interrupt controller</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - In MSI-mode, converts wired interrupt into MSIs (i.e. MSI generator)</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - In Direct-mode, injects external interrupts directly into HARTs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Thanks for working on the AIA support! I had a look at the series, and</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; have some concerns about interrupt ID abstraction.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; A bit of background, for readers not familiar with the AIA details.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; IMSIC allows for 2047 unique MSI ("msi-irq") sources per hart, and</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; each MSI is dedicated to a certain hart. The series takes the approach</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; to say that there are, e.g., 2047 interrupts ("lnx-irq") globally.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Each lnx-irq consists of #harts * msi-irq -- a slice -- and in the</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; slice only *one* msi-irq is acutally used.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; This scheme makes affinity changes more robust, because the interrupt</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; sources on "other" harts are pre-allocated. On the other hand it</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; requires to propagate irq masking to other harts via IPIs (this is</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; mostly done up setup/tear down). It's also wasteful, because msi-irqs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; are hogged, and cannot be used.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Contemporary storage/networking drivers usually uses queues per core</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; (or a sub-set of cores). The current scheme wastes a lot of msi-irqs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; If we instead used a scheme where "msi-irq == lnx-irq", instead of</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; "lnq-irq = {hart 0;msi-irq x , ... hart N;msi-irq x}", there would be</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; a lot MSIs for other users. 1-1 vs 1-N. E.g., if a storage device</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; would like to use 5 queues (5 cores) on a 128 core system, the current</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; scheme would consume 5 * 128 MSIs, instead of just 5.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; On the plus side:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Changing interrupts affinity will never fail, because the interrupts</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   on each hart is pre-allocated.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; On the negative side:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Wasteful interrupt usage, and a system can potientially "run out" of</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   interrupts. Especially for many core systems.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Interrupt masking need to proagate to harts via IPIs (there's no</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   broadcast csr in IMSIC), and a more complex locking scheme IMSIC</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Summary:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; The current series caps the number of global interrupts to maximum</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 2047 MSIs for all cores (whole system). A better scheme, IMO, would be</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; to expose 2047 * #harts unique MSIs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; I think this could simplify/remove(?) the locking as well.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; Exposing 2047 * #harts unique MSIs has multiple issues:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; 1) The irq_set_affinity() does not work for MSIs because each</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;      IRQ is not tied to a particular HART. This means we can't</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;      balance the IRQ processing load among HARTs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Yes, you can balance. In your code, each *active* MSI is still</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; bound/active to a specific hard together with the affinity mask. In an</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; 1-1 model you would still need to track the affinity mask, but the</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; irq_set_affinity() would be different. It would try to allocate a new</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; MSI from the target CPU, and then switch to having that MSI active.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; That's what x86 does AFAIU, which is also constrained by the # of</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; available MSIs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; The downside, as I pointed out, is that the set affinity action can</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; fail for a certain target CPU.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Yes, irq_set_affinity() can fail for the suggested approach plus for</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; RISC-V AIA, one HART does not have access to other HARTs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; MSI enable/disable bits so the approach will also involve IPI.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Correct, but the current series does a broadcast to all cores, where the</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; 1-1 approach is at most an IPI to a single core.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; 128+c machines are getting more common, and you have devices that you</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; bring up/down on a per-core basis. Broadcasting IPIs to all cores, when</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; dealing with a per-core activity is a pretty noisy neighbor.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; Broadcast IPI in the current approach is only done upon MSI mask/unmask</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; operation. It is not done upon set_affinity() of interrupt handling.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; I'm aware. We're on the same page here.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; This could be fixed in the existing 1-n approach, by not require to sync</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; the cores that are not handling the MSI in question. "Lazy disable"</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; Incorrect. The approach you are suggesting involves an IPI upon every</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; irq_set_affinity(). This is because a HART can only enable it's own</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; MSI ID so when an IRQ is moved to from HART A to HART B with</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; a different ID X on HART B then we will need an IPI in irq_set_affinit()</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; to enable ID X on HART B.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; Yes, the 1-1 approach will require an IPI to one target cpu on affinity</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; changes, and similar on mask/unmask.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; The 1-n approach, require no-IPI on affinity changes (nice!), but IPI</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; broadcast to all cores on mask/unmask (not so nice).</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; My concern is interrupts become a scarce resource with this</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; implementation, but maybe my view is incorrect. I've seen bare-metal</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; x86 systems (no VMs) with ~200 cores, and ~2000 interrupts, but maybe</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; that is considered "a lot of interrupts".</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; As long as we don't get into scenarios where we're running out of</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; interrupts, due to the software design.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; The current approach is simpler and ensures irq_set_affinity</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; always works. The limit of max 2047 IDs is sufficient for many</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; systems (if not all).</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Let me give you another view. On a 128c system each core has ~16 unique</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; interrupts for disposal. E.g. the Intel E800 NIC has more than 2048</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; network queue pairs for each PF.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; Clearly, this example is a hypothetical and represents a poorly</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; designed platform.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; Having just 16 IDs per-Core is a very poor design choice. In fact, the</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; Server SoC spec mandates a minimum 255 IDs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; You are misreading. A 128c system with 2047 MSIs per-core, will only</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; have 16 *per-core unique* (2047/128) interrupts with the current series.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; I'm not saying that each IMSIC has 16 IDs, I'm saying that in a 128c</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; system with the maximum amount of MSIs possible in the spec, you'll end</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; up with 16 *unique* interrupts per core.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; -ENOPARSE</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; I don't see how this applies to the current approach because we treat</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; MSI ID space as global across cores so if a system has 2047 MSIs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; per-core then we have 2047 MSIs across all cores.</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; Ok, I'll try again! :-)</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; Let's assume that each core in the 128c system has some per-core</span>
<span class=quote>&gt; &gt;&gt; &gt; resources, say a two NIC queue pairs, and a storage queue pair. This</span>
<span class=quote>&gt; &gt;&gt; &gt; will consume, e.g., 2*2 + 2 (6) MSI sources from the global namespace.</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; If each core does this it'll be 6*128 MSI sources of the global</span>
<span class=quote>&gt; &gt;&gt; &gt; namespace.</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; The maximum number of "privates" MSI sources a core can utilize is 16.</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; I'm trying (it's does seem to go that well ;-)) to point out that it's</span>
<span class=quote>&gt; &gt;&gt; &gt; only 16 unique sources per core. For, say, a 256 core system it would be</span>
<span class=quote>&gt; &gt;&gt; &gt; 8. 2047 MSI sources in a system is not much.</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; Say that I want to spin up 24 NIC queues with one MSI each on each core</span>
<span class=quote>&gt; &gt;&gt; &gt; on my 128c system. That's not possible with this series, while with an</span>
<span class=quote>&gt; &gt;&gt; &gt; 1-1 system it wouldn't be an issue.</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; Clearer, or still weird?</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; Regarding NICs which support a large number of queues, the driver</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; will typically enable only one queue per-core and set the affinity to</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; separate cores. We have user-space data plane applications based</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; on DPDK which are capable of using a large number of NIC queues</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; but these applications are polling based and don't use MSIs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; That's one sample point, and clearly not the only one. There are *many*</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; different usage models. Just because you *assign* MSI, doesn't mean they</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; are firing all the time.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; I can show you a couple of networking setups where this is clearly not</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; enough. Each core has a large number of QoS queues, and each queue would</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; very much like to have a dedicated MSI.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; When we encounter a system requiring a large number of MSIs,</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; we can either:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; 1) Extend the AIA spec to support greater than 2047 IDs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; 2) Re-think the approach in the IMSIC driver</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; The choice between #1 and #2 above depends on the</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; guarantees we want for irq_set_affinity().</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; The irq_set_affinity() behavior is better with this series, but I think</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; the other downsides: number of available interrupt sources, and IPI</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; broadcast are worse.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; The IPI overhead in the approach you are suggesting will be</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; even bad compared to the IPI overhead of the current approach</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; because we will end-up doing IPI upon every irq_set_affinity()</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; in the suggested approach compared to doing IPI upon every</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; mask/unmask in the current approach.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; Again, very workload dependent.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; This series does IPI broadcast on masking/unmasking, which means that</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; cores that don't care get interrupted because, say, a network queue-pair</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; is setup on another core.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; Some workloads never change the irq affinity.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; There are various events which irq affinity such as irq balance,</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; CPU hotplug, system suspend, etc.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; Also, the 1-1 approach does IPI upon set_affinity, mask and</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; unmask whereas the 1-n approach does IPI only upon mask</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; and unmask.</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; An important distinction; When you say IPI on mask/unmask it is a</span>
<span class=quote>&gt; &gt;&gt; &gt; broadcast IPI to *all* cores, which is pretty instrusive.</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; The 1-1 variant does an IPI to a *one* target core.</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; I'm just pointing out that there are pro/cons with both variants.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; The biggest advantage of the current approach is a reliable</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; irq_set_affinity() which is a very valuable thing to have.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; ...and I'm arguing that we're paying a big price for that.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; ARM systems easily support a large number of LPIs per-core.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; For example, GIC-700 supports 56000 LPIs per-core.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; &gt; (Refer, https://developer.arm.com/documentation/101516/0300/About-the-GIC-700/Features)</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; Yeah, but this is not the GIC. This is something that looks more like</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; the x86 world. We'll be stuck with a lot of implementations with AIA 1.0</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;&gt; spec, and many cores.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; Well, RISC-V AIA is neigher ARM GIG not x86 APIC. All I am saying</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; is that there are systems with large number per-core interrupt IDs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; for handling MSIs.</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; Yes, and while that is nice, it's not what IMSIC is.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; Some follow-ups, after thinking more about it more over the weekend.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; * Do one really need an IPI for irq_set_affinity() for the 1-1 model?</span>
<span class=quote>&gt; &gt;&gt;   Why touch the enable/disable bits when moving interrupts?</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; In the 1-1 model, the ID on the current HART and target HART upon</span>
<span class=quote>&gt; &gt; irq_set_affinity will be different so we can't leave the unused ID on</span>
<span class=quote>&gt; &gt; current HART enabled because it can lead to spurious interrupts</span>
<span class=quote>&gt; &gt; when the ID on current HART is re-used for some other device.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Hmm, is this really an actual problem, or a theoretical one? The</span>
<span class=quote>&gt; implementation need to track what's in-use, so can we ever get into this</span>
<span class=quote>&gt; situation?</span>

As of now, it is theoretical but it is certainly possible to hit this issue.
<span class=quote>
&gt;</span>
<span class=quote>&gt; Somewhat related; I had a similar question for imsic_pci_{un,}mask_irq()</span>
<span class=quote>&gt; -- why not only do the the default mask operation (only</span>
<span class=quote>&gt; pci_msi_{un,}mask_irq()), but instead propagate to the IMSIC</span>
<span class=quote>&gt; mask/unmask?</span>

We have hierarchical IMSIC PCI irq domain whoes parent irq domain
is IMSIC base domain. Unfortunately pci_msi_[un]mask_irq() don't
work for hierarchical irq domain.
<span class=quote>
&gt;</span>
<span class=quote>&gt; &gt; There is also a possibility of receiving an interrupt while the ID was</span>
<span class=quote>&gt; &gt; moved to a new target HART in-which case we have to detect and</span>
<span class=quote>&gt; &gt; re-trigger interrupt on the new target HART. In fact, x86 APLIC does</span>
<span class=quote>&gt; &gt; an IPI to take care of this case.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; This case I get, and the implementation can track that both are in use.</span>
<span class=quote>&gt; It's the spurious one that I'm dubious of (don't get).</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; * In my book the IMSIC looks very much like the x86 LAPIC, which also</span>
<span class=quote>&gt; &gt;&gt;   has few interrupts (IMSIC &lt;2048, LAPIC 256). The IRQ matrix allocator</span>
<span class=quote>&gt; &gt;&gt;   [1], and a scheme similar to LAPIC [2] would be a good fit. This is</span>
<span class=quote>&gt; &gt;&gt;   the 1-1 model, but more sophisticated than what I've been describing</span>
<span class=quote>&gt; &gt;&gt;   (e.g. properly handling mangaged/regular irqs). As a bonus we would</span>
<span class=quote>&gt; &gt;&gt;   get the IRQ matrix debugfs/tracepoint support.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; Yes, I have been evaluating the 1-1 model for the past few days. I also</span>
<span class=quote>&gt; &gt; have a working implementation with a simple per-CPU bitmap based</span>
<span class=quote>&gt; &gt; allocator which handles both legacy MSI (block of 1,2,4,8,16, or 32 IDs)</span>
<span class=quote>&gt; &gt; and MSI-X.</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; The irq matrix allocator needs to be improved for handling legacy MSI</span>
<span class=quote>&gt; &gt; so initially I will post a v11 series which works for me and converging</span>
<span class=quote>&gt; &gt; with irq matrix allocator can be future work.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; What's missing/needs to be improved for legacy MSI (legacy MSI ==</span>
<span class=quote>&gt; !MSI-X, right?) in the matrix allocator?</span>

For legacy MSI, a block IDs needs to be contiguous and the number
of IDs can be a power of 2.

For a short difference between MSI vs MSI-X, refer:
https://en.wikipedia.org/wiki/Message_Signaled_Interrupts

Regards,
Anup
</pre>
</div>
<a name=25566109></a>
<div class=comment>
<div class=meta>
 <span><a href="https://patchwork.kernel.org/project/linux-riscv/list/?submitter=198739">Björn Töpel</a></span>
 <span class=pull-right>Oct. 23, 2023, 3:45 p.m. UTC | <a href=https://patchwork.kernel.org/comment/25566109/>#14</a></span>
</div>
<pre class=content>Anup Patel &lt;apatel@ventanamicro.com&gt; writes:
<span class=quote>
&gt; On Mon, Oct 23, 2023 at 7:37 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt; On Mon, Oct 23, 2023 at 12:32 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; Björn Töpel &lt;bjorn@kernel.org&gt; writes:</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; On Fri, Oct 20, 2023 at 10:07 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; On Fri, Oct 20, 2023 at 8:10 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; On Fri, Oct 20, 2023 at 2:17 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thanks for the quick reply!</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; On Thu, Oct 19, 2023 at 7:13 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Hi Anup,</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; The RISC-V AIA specification is ratified as-per the RISC-V international</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; process. The latest ratified AIA specifcation can be found at:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; https://github.com/riscv/riscv-aia/releases/download/1.0/riscv-interrupts-1.0.pdf</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; At a high-level, the AIA specification adds three things:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 1) AIA CSRs</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Improved local interrupt support</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 2) Incoming Message Signaled Interrupt Controller (IMSIC)</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Per-HART MSI controller</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Support MSI virtualization</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Support IPI along with virtualization</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 3) Advanced Platform-Level Interrupt Controller (APLIC)</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Wired interrupt controller</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - In MSI-mode, converts wired interrupt into MSIs (i.e. MSI generator)</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - In Direct-mode, injects external interrupts directly into HARTs</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Thanks for working on the AIA support! I had a look at the series, and</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; have some concerns about interrupt ID abstraction.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; A bit of background, for readers not familiar with the AIA details.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; IMSIC allows for 2047 unique MSI ("msi-irq") sources per hart, and</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; each MSI is dedicated to a certain hart. The series takes the approach</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; to say that there are, e.g., 2047 interrupts ("lnx-irq") globally.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Each lnx-irq consists of #harts * msi-irq -- a slice -- and in the</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; slice only *one* msi-irq is acutally used.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; This scheme makes affinity changes more robust, because the interrupt</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; sources on "other" harts are pre-allocated. On the other hand it</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; requires to propagate irq masking to other harts via IPIs (this is</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; mostly done up setup/tear down). It's also wasteful, because msi-irqs</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; are hogged, and cannot be used.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Contemporary storage/networking drivers usually uses queues per core</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; (or a sub-set of cores). The current scheme wastes a lot of msi-irqs.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; If we instead used a scheme where "msi-irq == lnx-irq", instead of</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; "lnq-irq = {hart 0;msi-irq x , ... hart N;msi-irq x}", there would be</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; a lot MSIs for other users. 1-1 vs 1-N. E.g., if a storage device</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; would like to use 5 queues (5 cores) on a 128 core system, the current</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; scheme would consume 5 * 128 MSIs, instead of just 5.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; On the plus side:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Changing interrupts affinity will never fail, because the interrupts</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   on each hart is pre-allocated.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; On the negative side:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Wasteful interrupt usage, and a system can potientially "run out" of</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   interrupts. Especially for many core systems.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Interrupt masking need to proagate to harts via IPIs (there's no</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   broadcast csr in IMSIC), and a more complex locking scheme IMSIC</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Summary:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; The current series caps the number of global interrupts to maximum</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 2047 MSIs for all cores (whole system). A better scheme, IMO, would be</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; to expose 2047 * #harts unique MSIs.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; I think this could simplify/remove(?) the locking as well.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; Exposing 2047 * #harts unique MSIs has multiple issues:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; 1) The irq_set_affinity() does not work for MSIs because each</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;      IRQ is not tied to a particular HART. This means we can't</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;      balance the IRQ processing load among HARTs.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Yes, you can balance. In your code, each *active* MSI is still</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; bound/active to a specific hard together with the affinity mask. In an</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; 1-1 model you would still need to track the affinity mask, but the</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; irq_set_affinity() would be different. It would try to allocate a new</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; MSI from the target CPU, and then switch to having that MSI active.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; That's what x86 does AFAIU, which is also constrained by the # of</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; available MSIs.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; The downside, as I pointed out, is that the set affinity action can</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; fail for a certain target CPU.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Yes, irq_set_affinity() can fail for the suggested approach plus for</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; RISC-V AIA, one HART does not have access to other HARTs</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; MSI enable/disable bits so the approach will also involve IPI.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Correct, but the current series does a broadcast to all cores, where the</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; 1-1 approach is at most an IPI to a single core.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; 128+c machines are getting more common, and you have devices that you</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; bring up/down on a per-core basis. Broadcasting IPIs to all cores, when</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; dealing with a per-core activity is a pretty noisy neighbor.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Broadcast IPI in the current approach is only done upon MSI mask/unmask</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; operation. It is not done upon set_affinity() of interrupt handling.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; I'm aware. We're on the same page here.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; This could be fixed in the existing 1-n approach, by not require to sync</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; the cores that are not handling the MSI in question. "Lazy disable"</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Incorrect. The approach you are suggesting involves an IPI upon every</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; irq_set_affinity(). This is because a HART can only enable it's own</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; MSI ID so when an IRQ is moved to from HART A to HART B with</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; a different ID X on HART B then we will need an IPI in irq_set_affinit()</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; to enable ID X on HART B.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; Yes, the 1-1 approach will require an IPI to one target cpu on affinity</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; changes, and similar on mask/unmask.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; The 1-n approach, require no-IPI on affinity changes (nice!), but IPI</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; broadcast to all cores on mask/unmask (not so nice).</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; My concern is interrupts become a scarce resource with this</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; implementation, but maybe my view is incorrect. I've seen bare-metal</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; x86 systems (no VMs) with ~200 cores, and ~2000 interrupts, but maybe</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; that is considered "a lot of interrupts".</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; As long as we don't get into scenarios where we're running out of</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; interrupts, due to the software design.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; The current approach is simpler and ensures irq_set_affinity</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; always works. The limit of max 2047 IDs is sufficient for many</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; systems (if not all).</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Let me give you another view. On a 128c system each core has ~16 unique</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; interrupts for disposal. E.g. the Intel E800 NIC has more than 2048</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; network queue pairs for each PF.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Clearly, this example is a hypothetical and represents a poorly</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; designed platform.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Having just 16 IDs per-Core is a very poor design choice. In fact, the</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Server SoC spec mandates a minimum 255 IDs.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; You are misreading. A 128c system with 2047 MSIs per-core, will only</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; have 16 *per-core unique* (2047/128) interrupts with the current series.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; I'm not saying that each IMSIC has 16 IDs, I'm saying that in a 128c</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; system with the maximum amount of MSIs possible in the spec, you'll end</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; up with 16 *unique* interrupts per core.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; -ENOPARSE</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; I don't see how this applies to the current approach because we treat</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; MSI ID space as global across cores so if a system has 2047 MSIs</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; per-core then we have 2047 MSIs across all cores.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; Ok, I'll try again! :-)</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; Let's assume that each core in the 128c system has some per-core</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; resources, say a two NIC queue pairs, and a storage queue pair. This</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; will consume, e.g., 2*2 + 2 (6) MSI sources from the global namespace.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; If each core does this it'll be 6*128 MSI sources of the global</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; namespace.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; The maximum number of "privates" MSI sources a core can utilize is 16.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; I'm trying (it's does seem to go that well ;-)) to point out that it's</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; only 16 unique sources per core. For, say, a 256 core system it would be</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; 8. 2047 MSI sources in a system is not much.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; Say that I want to spin up 24 NIC queues with one MSI each on each core</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; on my 128c system. That's not possible with this series, while with an</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; 1-1 system it wouldn't be an issue.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; Clearer, or still weird?</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Regarding NICs which support a large number of queues, the driver</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; will typically enable only one queue per-core and set the affinity to</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; separate cores. We have user-space data plane applications based</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; on DPDK which are capable of using a large number of NIC queues</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; but these applications are polling based and don't use MSIs.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; That's one sample point, and clearly not the only one. There are *many*</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; different usage models. Just because you *assign* MSI, doesn't mean they</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; are firing all the time.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; I can show you a couple of networking setups where this is clearly not</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; enough. Each core has a large number of QoS queues, and each queue would</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; very much like to have a dedicated MSI.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; When we encounter a system requiring a large number of MSIs,</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; we can either:</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; 1) Extend the AIA spec to support greater than 2047 IDs</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; 2) Re-think the approach in the IMSIC driver</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; The choice between #1 and #2 above depends on the</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; guarantees we want for irq_set_affinity().</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; The irq_set_affinity() behavior is better with this series, but I think</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; the other downsides: number of available interrupt sources, and IPI</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; broadcast are worse.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; The IPI overhead in the approach you are suggesting will be</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; even bad compared to the IPI overhead of the current approach</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; because we will end-up doing IPI upon every irq_set_affinity()</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; in the suggested approach compared to doing IPI upon every</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; mask/unmask in the current approach.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; Again, very workload dependent.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; This series does IPI broadcast on masking/unmasking, which means that</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; cores that don't care get interrupted because, say, a network queue-pair</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; is setup on another core.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; Some workloads never change the irq affinity.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; There are various events which irq affinity such as irq balance,</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; CPU hotplug, system suspend, etc.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; Also, the 1-1 approach does IPI upon set_affinity, mask and</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; unmask whereas the 1-n approach does IPI only upon mask</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; and unmask.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; An important distinction; When you say IPI on mask/unmask it is a</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; broadcast IPI to *all* cores, which is pretty instrusive.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; The 1-1 variant does an IPI to a *one* target core.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; I'm just pointing out that there are pro/cons with both variants.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; The biggest advantage of the current approach is a reliable</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; irq_set_affinity() which is a very valuable thing to have.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; ...and I'm arguing that we're paying a big price for that.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; ARM systems easily support a large number of LPIs per-core.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; For example, GIC-700 supports 56000 LPIs per-core.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; (Refer, https://developer.arm.com/documentation/101516/0300/About-the-GIC-700/Features)</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; Yeah, but this is not the GIC. This is something that looks more like</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; the x86 world. We'll be stuck with a lot of implementations with AIA 1.0</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;&gt; spec, and many cores.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; Well, RISC-V AIA is neigher ARM GIG not x86 APIC. All I am saying</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; is that there are systems with large number per-core interrupt IDs</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;&gt; for handling MSIs.</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; &gt; Yes, and while that is nice, it's not what IMSIC is.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; Some follow-ups, after thinking more about it more over the weekend.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; * Do one really need an IPI for irq_set_affinity() for the 1-1 model?</span>
<span class=quote>&gt;&gt; &gt;&gt;   Why touch the enable/disable bits when moving interrupts?</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; In the 1-1 model, the ID on the current HART and target HART upon</span>
<span class=quote>&gt;&gt; &gt; irq_set_affinity will be different so we can't leave the unused ID on</span>
<span class=quote>&gt;&gt; &gt; current HART enabled because it can lead to spurious interrupts</span>
<span class=quote>&gt;&gt; &gt; when the ID on current HART is re-used for some other device.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; Hmm, is this really an actual problem, or a theoretical one? The</span>
<span class=quote>&gt;&gt; implementation need to track what's in-use, so can we ever get into this</span>
<span class=quote>&gt;&gt; situation?</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; As of now, it is theoretical but it is certainly possible to hit this issue.</span>

Sorry for being slow here, Anup, but could you give an example how this
could happen? For me it sounds like this could only be caused by a
broken (buggy) implementation?
<span class=quote>
&gt;&gt; Somewhat related; I had a similar question for imsic_pci_{un,}mask_irq()</span>
<span class=quote>&gt;&gt; -- why not only do the the default mask operation (only</span>
<span class=quote>&gt;&gt; pci_msi_{un,}mask_irq()), but instead propagate to the IMSIC</span>
<span class=quote>&gt;&gt; mask/unmask?</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; We have hierarchical IMSIC PCI irq domain whoes parent irq domain</span>
<span class=quote>&gt; is IMSIC base domain. Unfortunately pci_msi_[un]mask_irq() don't</span>
<span class=quote>&gt; work for hierarchical irq domain.</span>

Ok! Thanks for the explanation!
<span class=quote>
&gt;&gt; &gt; There is also a possibility of receiving an interrupt while the ID was</span>
<span class=quote>&gt;&gt; &gt; moved to a new target HART in-which case we have to detect and</span>
<span class=quote>&gt;&gt; &gt; re-trigger interrupt on the new target HART. In fact, x86 APLIC does</span>
<span class=quote>&gt;&gt; &gt; an IPI to take care of this case.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; This case I get, and the implementation can track that both are in use.</span>
<span class=quote>&gt;&gt; It's the spurious one that I'm dubious of (don't get).</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;&gt; * In my book the IMSIC looks very much like the x86 LAPIC, which also</span>
<span class=quote>&gt;&gt; &gt;&gt;   has few interrupts (IMSIC &lt;2048, LAPIC 256). The IRQ matrix allocator</span>
<span class=quote>&gt;&gt; &gt;&gt;   [1], and a scheme similar to LAPIC [2] would be a good fit. This is</span>
<span class=quote>&gt;&gt; &gt;&gt;   the 1-1 model, but more sophisticated than what I've been describing</span>
<span class=quote>&gt;&gt; &gt;&gt;   (e.g. properly handling mangaged/regular irqs). As a bonus we would</span>
<span class=quote>&gt;&gt; &gt;&gt;   get the IRQ matrix debugfs/tracepoint support.</span>
<span class=quote>&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; Yes, I have been evaluating the 1-1 model for the past few days. I also</span>
<span class=quote>&gt;&gt; &gt; have a working implementation with a simple per-CPU bitmap based</span>
<span class=quote>&gt;&gt; &gt; allocator which handles both legacy MSI (block of 1,2,4,8,16, or 32 IDs)</span>
<span class=quote>&gt;&gt; &gt; and MSI-X.</span>
<span class=quote>&gt;&gt; &gt;</span>
<span class=quote>&gt;&gt; &gt; The irq matrix allocator needs to be improved for handling legacy MSI</span>
<span class=quote>&gt;&gt; &gt; so initially I will post a v11 series which works for me and converging</span>
<span class=quote>&gt;&gt; &gt; with irq matrix allocator can be future work.</span>
<span class=quote>&gt;&gt;</span>
<span class=quote>&gt;&gt; What's missing/needs to be improved for legacy MSI (legacy MSI ==</span>
<span class=quote>&gt;&gt; !MSI-X, right?) in the matrix allocator?</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; For legacy MSI, a block IDs needs to be contiguous and the number</span>
<span class=quote>&gt; of IDs can be a power of 2.</span>

Oh, so this is not supported by the matrix allocator?


Björn
</pre>
</div>
<a name=25566302></a>
<div class=comment>
<div class=meta>
 <span><a href="https://patchwork.kernel.org/project/linux-riscv/list/?submitter=204452">Anup Patel</a></span>
 <span class=pull-right>Oct. 23, 2023, 5:25 p.m. UTC | <a href=https://patchwork.kernel.org/comment/25566302/>#15</a></span>
</div>
<pre class=content>On Mon, Oct 23, 2023 at 9:15 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:
<span class=quote>&gt;</span>
<span class=quote>&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; &gt; On Mon, Oct 23, 2023 at 7:37 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; On Mon, Oct 23, 2023 at 12:32 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; Björn Töpel &lt;bjorn@kernel.org&gt; writes:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; On Fri, Oct 20, 2023 at 10:07 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; On Fri, Oct 20, 2023 at 8:10 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; On Fri, Oct 20, 2023 at 2:17 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thanks for the quick reply!</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; On Thu, Oct 19, 2023 at 7:13 PM Björn Töpel &lt;bjorn@kernel.org&gt; wrote:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Hi Anup,</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Anup Patel &lt;apatel@ventanamicro.com&gt; writes:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; The RISC-V AIA specification is ratified as-per the RISC-V international</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; process. The latest ratified AIA specifcation can be found at:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; https://github.com/riscv/riscv-aia/releases/download/1.0/riscv-interrupts-1.0.pdf</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; At a high-level, the AIA specification adds three things:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 1) AIA CSRs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Improved local interrupt support</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 2) Incoming Message Signaled Interrupt Controller (IMSIC)</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Per-HART MSI controller</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Support MSI virtualization</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Support IPI along with virtualization</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; 3) Advanced Platform-Level Interrupt Controller (APLIC)</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - Wired interrupt controller</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - In MSI-mode, converts wired interrupt into MSIs (i.e. MSI generator)</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;    - In Direct-mode, injects external interrupts directly into HARTs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Thanks for working on the AIA support! I had a look at the series, and</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; have some concerns about interrupt ID abstraction.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; A bit of background, for readers not familiar with the AIA details.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; IMSIC allows for 2047 unique MSI ("msi-irq") sources per hart, and</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; each MSI is dedicated to a certain hart. The series takes the approach</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; to say that there are, e.g., 2047 interrupts ("lnx-irq") globally.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Each lnx-irq consists of #harts * msi-irq -- a slice -- and in the</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; slice only *one* msi-irq is acutally used.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; This scheme makes affinity changes more robust, because the interrupt</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; sources on "other" harts are pre-allocated. On the other hand it</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; requires to propagate irq masking to other harts via IPIs (this is</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; mostly done up setup/tear down). It's also wasteful, because msi-irqs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; are hogged, and cannot be used.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Contemporary storage/networking drivers usually uses queues per core</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; (or a sub-set of cores). The current scheme wastes a lot of msi-irqs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; If we instead used a scheme where "msi-irq == lnx-irq", instead of</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; "lnq-irq = {hart 0;msi-irq x , ... hart N;msi-irq x}", there would be</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; a lot MSIs for other users. 1-1 vs 1-N. E.g., if a storage device</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; would like to use 5 queues (5 cores) on a 128 core system, the current</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; scheme would consume 5 * 128 MSIs, instead of just 5.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; On the plus side:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Changing interrupts affinity will never fail, because the interrupts</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   on each hart is pre-allocated.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; On the negative side:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Wasteful interrupt usage, and a system can potientially "run out" of</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   interrupts. Especially for many core systems.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; * Interrupt masking need to proagate to harts via IPIs (there's no</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;   broadcast csr in IMSIC), and a more complex locking scheme IMSIC</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Summary:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; The current series caps the number of global interrupts to maximum</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 2047 MSIs for all cores (whole system). A better scheme, IMO, would be</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; to expose 2047 * #harts unique MSIs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; I think this could simplify/remove(?) the locking as well.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; Exposing 2047 * #harts unique MSIs has multiple issues:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; 1) The irq_set_affinity() does not work for MSIs because each</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;      IRQ is not tied to a particular HART. This means we can't</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;      balance the IRQ processing load among HARTs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Yes, you can balance. In your code, each *active* MSI is still</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; bound/active to a specific hard together with the affinity mask. In an</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; 1-1 model you would still need to track the affinity mask, but the</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; irq_set_affinity() would be different. It would try to allocate a new</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; MSI from the target CPU, and then switch to having that MSI active.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; That's what x86 does AFAIU, which is also constrained by the # of</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; available MSIs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; The downside, as I pointed out, is that the set affinity action can</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; fail for a certain target CPU.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Yes, irq_set_affinity() can fail for the suggested approach plus for</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; RISC-V AIA, one HART does not have access to other HARTs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; MSI enable/disable bits so the approach will also involve IPI.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Correct, but the current series does a broadcast to all cores, where the</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; 1-1 approach is at most an IPI to a single core.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; 128+c machines are getting more common, and you have devices that you</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; bring up/down on a per-core basis. Broadcasting IPIs to all cores, when</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; dealing with a per-core activity is a pretty noisy neighbor.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Broadcast IPI in the current approach is only done upon MSI mask/unmask</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; operation. It is not done upon set_affinity() of interrupt handling.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; I'm aware. We're on the same page here.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; This could be fixed in the existing 1-n approach, by not require to sync</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; the cores that are not handling the MSI in question. "Lazy disable"</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Incorrect. The approach you are suggesting involves an IPI upon every</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; irq_set_affinity(). This is because a HART can only enable it's own</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; MSI ID so when an IRQ is moved to from HART A to HART B with</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; a different ID X on HART B then we will need an IPI in irq_set_affinit()</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; to enable ID X on HART B.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Yes, the 1-1 approach will require an IPI to one target cpu on affinity</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; changes, and similar on mask/unmask.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; The 1-n approach, require no-IPI on affinity changes (nice!), but IPI</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; broadcast to all cores on mask/unmask (not so nice).</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; My concern is interrupts become a scarce resource with this</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; implementation, but maybe my view is incorrect. I've seen bare-metal</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; x86 systems (no VMs) with ~200 cores, and ~2000 interrupts, but maybe</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; that is considered "a lot of interrupts".</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; As long as we don't get into scenarios where we're running out of</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; interrupts, due to the software design.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; The current approach is simpler and ensures irq_set_affinity</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; always works. The limit of max 2047 IDs is sufficient for many</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; systems (if not all).</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Let me give you another view. On a 128c system each core has ~16 unique</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; interrupts for disposal. E.g. the Intel E800 NIC has more than 2048</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; network queue pairs for each PF.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Clearly, this example is a hypothetical and represents a poorly</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; designed platform.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Having just 16 IDs per-Core is a very poor design choice. In fact, the</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Server SoC spec mandates a minimum 255 IDs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; You are misreading. A 128c system with 2047 MSIs per-core, will only</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; have 16 *per-core unique* (2047/128) interrupts with the current series.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; I'm not saying that each IMSIC has 16 IDs, I'm saying that in a 128c</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; system with the maximum amount of MSIs possible in the spec, you'll end</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; up with 16 *unique* interrupts per core.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; -ENOPARSE</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; I don't see how this applies to the current approach because we treat</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; MSI ID space as global across cores so if a system has 2047 MSIs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; per-core then we have 2047 MSIs across all cores.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; Ok, I'll try again! :-)</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; Let's assume that each core in the 128c system has some per-core</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; resources, say a two NIC queue pairs, and a storage queue pair. This</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; will consume, e.g., 2*2 + 2 (6) MSI sources from the global namespace.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; If each core does this it'll be 6*128 MSI sources of the global</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; namespace.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; The maximum number of "privates" MSI sources a core can utilize is 16.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; I'm trying (it's does seem to go that well ;-)) to point out that it's</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; only 16 unique sources per core. For, say, a 256 core system it would be</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; 8. 2047 MSI sources in a system is not much.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; Say that I want to spin up 24 NIC queues with one MSI each on each core</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; on my 128c system. That's not possible with this series, while with an</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; 1-1 system it wouldn't be an issue.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; Clearer, or still weird?</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Regarding NICs which support a large number of queues, the driver</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; will typically enable only one queue per-core and set the affinity to</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; separate cores. We have user-space data plane applications based</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; on DPDK which are capable of using a large number of NIC queues</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; but these applications are polling based and don't use MSIs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; That's one sample point, and clearly not the only one. There are *many*</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; different usage models. Just because you *assign* MSI, doesn't mean they</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; are firing all the time.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; I can show you a couple of networking setups where this is clearly not</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; enough. Each core has a large number of QoS queues, and each queue would</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; very much like to have a dedicated MSI.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; When we encounter a system requiring a large number of MSIs,</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; we can either:</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; 1) Extend the AIA spec to support greater than 2047 IDs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; 2) Re-think the approach in the IMSIC driver</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; The choice between #1 and #2 above depends on the</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; guarantees we want for irq_set_affinity().</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; The irq_set_affinity() behavior is better with this series, but I think</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; the other downsides: number of available interrupt sources, and IPI</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; broadcast are worse.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; The IPI overhead in the approach you are suggesting will be</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; even bad compared to the IPI overhead of the current approach</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; because we will end-up doing IPI upon every irq_set_affinity()</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; in the suggested approach compared to doing IPI upon every</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; mask/unmask in the current approach.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Again, very workload dependent.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; This series does IPI broadcast on masking/unmasking, which means that</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; cores that don't care get interrupted because, say, a network queue-pair</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; is setup on another core.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Some workloads never change the irq affinity.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; There are various events which irq affinity such as irq balance,</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; CPU hotplug, system suspend, etc.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; Also, the 1-1 approach does IPI upon set_affinity, mask and</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; unmask whereas the 1-n approach does IPI only upon mask</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; and unmask.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; An important distinction; When you say IPI on mask/unmask it is a</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; broadcast IPI to *all* cores, which is pretty instrusive.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; The 1-1 variant does an IPI to a *one* target core.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; I'm just pointing out that there are pro/cons with both variants.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; The biggest advantage of the current approach is a reliable</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; irq_set_affinity() which is a very valuable thing to have.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; ...and I'm arguing that we're paying a big price for that.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; ARM systems easily support a large number of LPIs per-core.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; For example, GIC-700 supports 56000 LPIs per-core.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; (Refer, https://developer.arm.com/documentation/101516/0300/About-the-GIC-700/Features)</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Yeah, but this is not the GIC. This is something that looks more like</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; the x86 world. We'll be stuck with a lot of implementations with AIA 1.0</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; spec, and many cores.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; Well, RISC-V AIA is neigher ARM GIG not x86 APIC. All I am saying</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; is that there are systems with large number per-core interrupt IDs</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;&gt; for handling MSIs.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; &gt; Yes, and while that is nice, it's not what IMSIC is.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; Some follow-ups, after thinking more about it more over the weekend.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; * Do one really need an IPI for irq_set_affinity() for the 1-1 model?</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;   Why touch the enable/disable bits when moving interrupts?</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; In the 1-1 model, the ID on the current HART and target HART upon</span>
<span class=quote>&gt; &gt;&gt; &gt; irq_set_affinity will be different so we can't leave the unused ID on</span>
<span class=quote>&gt; &gt;&gt; &gt; current HART enabled because it can lead to spurious interrupts</span>
<span class=quote>&gt; &gt;&gt; &gt; when the ID on current HART is re-used for some other device.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; Hmm, is this really an actual problem, or a theoretical one? The</span>
<span class=quote>&gt; &gt;&gt; implementation need to track what's in-use, so can we ever get into this</span>
<span class=quote>&gt; &gt;&gt; situation?</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; As of now, it is theoretical but it is certainly possible to hit this issue.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Sorry for being slow here, Anup, but could you give an example how this</span>
<span class=quote>&gt; could happen? For me it sounds like this could only be caused by a</span>
<span class=quote>&gt; broken (buggy) implementation?</span>

Let me re-state the problem: When ID X on HART A is moved to ID Y
on HART B. Now the movement might have be initiated by some
some HART C doing irq_set_affinity(). Once the movement is done
the ID X on HART A should be disabled because if not disabled then
some device (possibly buggy) can have HART A to take MSI with
ID X but this MSI won't map to any Linux IRQ since the it is already
moved to ID Y on HART B.
<span class=quote>
&gt;</span>
<span class=quote>&gt; &gt;&gt; Somewhat related; I had a similar question for imsic_pci_{un,}mask_irq()</span>
<span class=quote>&gt; &gt;&gt; -- why not only do the the default mask operation (only</span>
<span class=quote>&gt; &gt;&gt; pci_msi_{un,}mask_irq()), but instead propagate to the IMSIC</span>
<span class=quote>&gt; &gt;&gt; mask/unmask?</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; We have hierarchical IMSIC PCI irq domain whoes parent irq domain</span>
<span class=quote>&gt; &gt; is IMSIC base domain. Unfortunately pci_msi_[un]mask_irq() don't</span>
<span class=quote>&gt; &gt; work for hierarchical irq domain.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Ok! Thanks for the explanation!</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; There is also a possibility of receiving an interrupt while the ID was</span>
<span class=quote>&gt; &gt;&gt; &gt; moved to a new target HART in-which case we have to detect and</span>
<span class=quote>&gt; &gt;&gt; &gt; re-trigger interrupt on the new target HART. In fact, x86 APLIC does</span>
<span class=quote>&gt; &gt;&gt; &gt; an IPI to take care of this case.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; This case I get, and the implementation can track that both are in use.</span>
<span class=quote>&gt; &gt;&gt; It's the spurious one that I'm dubious of (don't get).</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt; * In my book the IMSIC looks very much like the x86 LAPIC, which also</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;   has few interrupts (IMSIC &lt;2048, LAPIC 256). The IRQ matrix allocator</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;   [1], and a scheme similar to LAPIC [2] would be a good fit. This is</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;   the 1-1 model, but more sophisticated than what I've been describing</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;   (e.g. properly handling mangaged/regular irqs). As a bonus we would</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;   get the IRQ matrix debugfs/tracepoint support.</span>
<span class=quote>&gt; &gt;&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; Yes, I have been evaluating the 1-1 model for the past few days. I also</span>
<span class=quote>&gt; &gt;&gt; &gt; have a working implementation with a simple per-CPU bitmap based</span>
<span class=quote>&gt; &gt;&gt; &gt; allocator which handles both legacy MSI (block of 1,2,4,8,16, or 32 IDs)</span>
<span class=quote>&gt; &gt;&gt; &gt; and MSI-X.</span>
<span class=quote>&gt; &gt;&gt; &gt;</span>
<span class=quote>&gt; &gt;&gt; &gt; The irq matrix allocator needs to be improved for handling legacy MSI</span>
<span class=quote>&gt; &gt;&gt; &gt; so initially I will post a v11 series which works for me and converging</span>
<span class=quote>&gt; &gt;&gt; &gt; with irq matrix allocator can be future work.</span>
<span class=quote>&gt; &gt;&gt;</span>
<span class=quote>&gt; &gt;&gt; What's missing/needs to be improved for legacy MSI (legacy MSI ==</span>
<span class=quote>&gt; &gt;&gt; !MSI-X, right?) in the matrix allocator?</span>
<span class=quote>&gt; &gt;</span>
<span class=quote>&gt; &gt; For legacy MSI, a block IDs needs to be contiguous and the number</span>
<span class=quote>&gt; &gt; of IDs can be a power of 2.</span>
<span class=quote>&gt;</span>
<span class=quote>&gt; Oh, so this is not supported by the matrix allocator?</span>

Yes, that's correct. IRQ matrix allocator only supports allocating
IDs suitable for MSI-X. Improving IRQ matrix allocator is a
separate effort in my opinion.

The ARM GICv3/v4 driver supports both MSI and MSI-X.

Regards,
Anup
</pre>
</div>
 </div>
 <div id=footer>
 <a href=http://jk.ozlabs.org/projects/patchwork/>patchwork</a>
 patch tracking system | version v2.2.6 | <a href=https://patchwork.kernel.org/about/>about patchwork</a>
 </div>
 
<div style=all:initial><div style=all:initial id=__hcfy__><template shadowrootmode=open><style class=sf-hidden>#root{-webkit-text-size-adjust:100%;box-sizing:border-box;font-size:14px;font-weight:400;letter-spacing:0;line-height:1.28581;text-transform:none;color:#182026;font-family:-apple-system,"BlinkMacSystemFont","Segoe UI","Roboto","Oxygen","Ubuntu","Cantarell","Open Sans","Helvetica Neue","Icons16",sans-serif;touch-action:manipulation}#root>.bp5-portal{z-index:9999999999}</style><style class=sf-hidden>#translate-panel{background-color:#f6f7f9;display:flex;flex-direction:column;padding-bottom:8px}.bp5-dark #translate-panel{background-color:#252a31}#translate-panel .fixed{flex-shrink:0;margin-bottom:10px}#translate-panel .body{flex-grow:1;overflow:auto;overscroll-behavior:contain}#translate-panel .body::-webkit-scrollbar{width:8px;background-color:rgba(0,0,0,0);-webkit-border-radius:100px}#translate-panel .body::-webkit-scrollbar:hover{background-color:rgba(0,0,0,.09)}#translate-panel .body::-webkit-scrollbar-thumb:vertical{background:rgba(0,0,0,.5);-webkit-border-radius:100px}#translate-panel .body::-webkit-scrollbar-thumb:vertical:active{background:rgba(0,0,0,.61);-webkit-border-radius:100px}#translate-panel.size-small,#translate-panel.size-small h6.bp5-heading,#translate-panel.size-small .bp5-control.bp5-large,#translate-panel.size-small textarea.bp5-input.bp5-small{font-size:14px}#translate-panel.size-small .phonetic-item,#translate-panel.size-small .quick-settings a{font-size:12px}#translate-panel.size-middle,#translate-panel.size-middle h6.bp5-heading,#translate-panel.size-middle .bp5-control.bp5-large,#translate-panel.size-middle textarea.bp5-input{font-size:18px}#translate-panel.size-middle .phonetic-item,#translate-panel.size-middle .quick-settings a{font-size:14px}#translate-panel.size-large,#translate-panel.size-large h6.bp5-heading,#translate-panel.size-large .bp5-control.bp5-large,#translate-panel.size-large textarea.bp5-input.bp5-large{font-size:22px}#translate-panel.size-large .source,#translate-panel.size-large .phonetic-item,#translate-panel.size-large .quick-settings a{font-size:18px}#translate-panel .bp5-button.bp5-small,#translate-panel .bp5-small .bp5-button{min-height:20px;min-width:20px}#translate-panel .header{display:flex;align-items:center;padding:4px 6px 4px 10px;border-bottom:1px solid #d1d1d1}.bp5-dark #translate-panel .header{border-bottom-color:rgba(17,20,24,.4)}#translate-panel .header .drag-block{min-width:5px;flex-shrink:0;flex-grow:1;align-self:stretch}#translate-panel .header .left{flex-shrink:0;display:flex}#translate-panel .header .right{flex-shrink:0;display:flex;align-items:center}#translate-panel .header .right .bp5-icon-arrow-right{flex-shrink:0;margin:0 5px}#translate-panel .header .right>.bp5-button{flex-shrink:0;margin:0 1px}#translate-panel .header .right>.bp5-button:last-child{margin-right:0}#translate-panel .quick-settings{padding:4px 9px;margin:0 1px}#translate-panel .quick-settings>div{margin-bottom:5px}#translate-panel .quick-settings .bp5-control{margin-bottom:0}#translate-panel .query-text{position:relative;padding:10px 10px 2px 10px}#translate-panel .query-text textarea.bp5-input{min-height:44px;font-family:system-ui,-apple-system,"Segoe UI","Roboto","Ubuntu","Cantarell","Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";overscroll-behavior:contain}#translate-panel .query-text .translate-btn{position:absolute;opacity:.6}#translate-panel .query-text .translate-btn:hover{opacity:1}#translate-panel .body{padding:0 10px}#translate-panel .body .bp5-card:first-child{margin-top:1px}#translate-panel .body .bp5-card:last-child{margin-bottom:1px}#translate-panel .body .no-api{margin:20px 0}.result-block{margin:8px 0;padding:2px 5px}.result-block .bp5-button{visibility:hidden}.result-block .error .bp5-button,.result-block:hover .bp5-button{visibility:visible}.result-block .legend{display:flex;align-items:center;justify-content:space-between}.result-block .legend .legend-left{display:flex;align-items:center}.result-block .legend .api-ico,.result-block .legend .bp5-heading{flex-shrink:0;white-space:nowrap}.result-block .legend .api-ico{display:inline-block;width:14px;height:14px;background-size:contain;margin-right:3px}.result-block .legend .bp5-heading{margin-bottom:0;margin-right:10px}.result-block .legend .source{cursor:pointer;font-size:12px;display:inline-flex;align-items:center}.result-block .legend .source .source-text{overflow:hidden}.result-block .legend .source .bp5-icon{position:relative;top:-1px;margin-left:1px}.result-block .phonetic{display:flex;flex-wrap:wrap}.result-block .phonetic .phonetic-item{display:flex;align-items:center;font-size:12px}.result-block .phonetic .phonetic-item:not(:last-child){margin-right:12px}.result-block .common-result p{line-height:1.3;margin:2px 0;overflow-wrap:break-word}.result-block .common-result .dict a{text-decoration:underline}.result-block .error{font-size:12px;padding:5px 10px}.result-block .dict-pos{margin-right:5px}.external-translators{margin-bottom:3px;padding:0;display:flex;flex-wrap:wrap}.external-translators>div{margin:0 5px 5px 0}.quick-links a{margin:0 5px 5px 0}#popper-container{width:250px;max-width:100%;position:absolute;left:0;top:0;z-index:9999999998;touch-action:none;transition:opacity .2s;background-color:#f6f7f9}.bp5-dark #popper-container{background-color:#252a31}#popper-container.show{opacity:1;pointer-events:auto;-moz-user-select:auto;user-select:auto}#popper-container,#popper-container[data-popper-reference-hidden=true]{opacity:0;pointer-events:none;-moz-user-select:none;user-select:none}#popper-container .drag-block{cursor:-webkit-grab;cursor:grab}#popper-container.pin{position:fixed}#popper-container.pin .arrow{display:none}#popper-container .arrow,#popper-container .arrow::before{position:absolute;width:8px;height:8px;z-index:-1}#popper-container .arrow::before{content:"";transform:rotate(45deg);background:#f6f7f9}.bp5-dark #popper-container .arrow::before{background-color:#252a31}#popper-container .arrow{display:none}#popper-container.show[data-popper-placement]:not([data-popper-reference-hidden=true]) .arrow{display:block}#popper-container[data-popper-placement^=top] .arrow{bottom:-5px}#popper-container[data-popper-placement^=top] .arrow::before{border-right:1px solid #d1d1d1;border-bottom:1px solid #d1d1d1}#popper-container[data-popper-placement^=bottom] .arrow{top:-5px}#popper-container[data-popper-placement^=bottom] .arrow::before{border-left:1px solid #d1d1d1;border-top:1px solid #d1d1d1}#popper-container[data-popper-placement^=left] .arrow{right:-5px}#popper-container[data-popper-placement^=left] .arrow::before{border-right:1px solid #d1d1d1;border-top:1px solid #d1d1d1}#popper-container[data-popper-placement^=right] .arrow{left:-5px}#popper-container[data-popper-placement^=right] .arrow::before{border-left:1px solid #d1d1d1;border-bottom:1px solid #d1d1d1}#translate-btn{display:none;position:absolute;z-index:9999999999;left:0;top:0}#translate-btn .bp5-button{padding:2px;min-width:0;min-height:0}#translate-btn .btn-icon{width:18px;height:18px;background-image:url(moz-extension://72d6ca68-5609-1440-b1ba-15daf8cbdb2d/logo.png);background-size:contain}.bp5-dark #translate-btn .btn-icon{background-image:url(moz-extension://72d6ca68-5609-1440-b1ba-15daf8cbdb2d/logowhite36.png)}#translate-btn.show{display:block}.translate-type-selector .bp5-label{display:inline}.translate-type-selector .bp5-radio{margin-bottom:0}#ocr-container{position:fixed;z-index:99999999999999;left:0;top:0;right:0;bottom:0}#ocr-container .toolbar{display:none;position:absolute;z-index:1}#ocr-container img{max-width:100%}#app{cursor:default}.switch-pin{flex-shrink:0;cursor:pointer}.switch-pin .bp5-icon-pin{transition:transform .2s,color .2s;transform:rotate(-45deg)}.pin .switch-pin .bp5-icon-pin{transform:rotate(-70deg)}.cut-btn{margin-left:2px}.app-toaster-container{position:fixed!important;z-index:9999999999!important}.app-toaster-container .bp5-toast{min-width:auto}#web-trs-panel .app-toaster-container{padding-right:0;padding-left:0}#web-trs-panel .page-trs-form-group{margin:0 0 0 0;align-items:center}#web-trs-panel .page-trs-form-group>label{width:70px}</style><div id=root dir=ltr class=bp5-dark><div id=app class=bp5-dark><div id=translate-btn class=sf-hidden></div><div id=popper-container style=width:250px;transform:translate(0px) class=bp5-elevation-4><div id=translate-panel class=size-small><div class=fixed><div class=header><div class=left><div class=switch-pin><button type=button class="bp5-button bp5-minimal bp5-small"><span aria-hidden=true class="bp5-icon bp5-icon-pin"><svg data-icon=pin height=14 role=img viewBox="0 0 16 16" width=14><path d="M9.41.92c-.51.51-.41 1.5.15 2.56L4.34 7.54C2.8 6.48 1.45 6.05.92 6.58l3.54 3.54-3.54 4.95 4.95-3.54 3.54 3.54c.53-.53.1-1.88-.96-3.42l4.06-5.22c1.06.56 2.04.66 2.55.15L9.41.92z" fill-rule=evenodd></path></svg></span></button></div><button type=button class="bp5-button bp5-minimal bp5-small cut-btn" title=截图翻译><span aria-hidden=true class="bp5-icon bp5-icon-cut"><svg data-icon=cut height=14 role=img viewBox="0 0 16 16" width=14><path d="M13 2s.71-1.29 0-2L8.66 5.07l1.05 1.32L13 2zm.07 8c-.42 0-.82.09-1.18.26L3.31 0c-.69.71 0 2 0 2l3.68 5.02-2.77 3.24A2.996 2.996 0 000 13c0 1.66 1.34 3 3 3s3-1.34 3-3c0-.46-.11-.89-.29-1.27L8.1 8.54l2.33 3.19c-.18.39-.29.82-.29 1.27 0 1.66 1.31 3 2.93 3S16 14.66 16 13s-1.31-3-2.93-3zM3 14c-.55 0-1-.45-1-1s.45-1 1-1 1 .45 1 1-.45 1-1 1zm10.07 0c-.54 0-.98-.45-.98-1s.44-1 .98-1 .98.45.98 1-.44 1-.98 1z" fill-rule=evenodd></path></svg></span></button><button type=button class="bp5-button bp5-minimal bp5-small cut-btn" title=网页全文翻译><span aria-hidden=true class="bp5-icon bp5-icon-translate"><svg data-icon=translate height=14 role=img viewBox="0 0 16 16" width=14><path d="M15.89 14.56l-3.99-8h-.01c-.17-.33-.5-.56-.89-.56s-.72.23-.89.56h-.01L9 8.76 7.17 7.38l.23-.18C8.37 6.47 9 5.31 9 4V3h1c.55 0 1-.45 1-1s-.45-1-1-1H7c0-.55-.45-1-1-1H5c-.55 0-1 .45-1 1H1c-.55 0-1 .45-1 1s.45 1 1 1h6v1c0 .66-.32 1.25-.82 1.61l-.68.51-.68-.5C4.32 5.25 4 4.66 4 4H2c0 1.31.63 2.47 1.6 3.2l.23.17L1.4 9.2l.01.01C1.17 9.4 1 9.67 1 10c0 .55.45 1 1 1 .23 0 .42-.09.59-.21l.01.01 2.9-2.17 2.6 1.95-1.99 3.98h.01c-.07.13-.12.28-.12.44 0 .55.45 1 1 1 .39 0 .72-.23.89-.56h.01L8.62 14h4.76l.72 1.45h.01c.17.32.5.55.89.55.55 0 1-.45 1-1 0-.16-.05-.31-.11-.44zM9.62 12L11 9.24 12.38 12H9.62z" fill-rule=evenodd></path></svg></span></button><button type=button class="bp5-button bp5-minimal bp5-small cut-btn" title=音视频翻译 style=margin-right:2px><span aria-hidden=true class="bp5-icon bp5-icon-video"><svg data-icon=video height=14 role=img viewBox="0 0 16 16" width=14><path d="M15 2H1c-.55 0-1 .45-1 1v10c0 .55.45 1 1 1h14c.55 0 1-.45 1-1V3c0-.55-.45-1-1-1zM5 11V5l6 3-6 3z" fill-rule=evenodd></path></svg></span></button><button type=button title=图片翻译 class="bp5-button bp5-minimal bp5-small"><span aria-hidden=true class="bp5-icon bp5-icon-media"><svg data-icon=media height=14 role=img viewBox="0 0 16 16" width=14><path d="M11.99 6.99c.55 0 1-.45 1-1s-.45-1-1-1-1 .45-1 1 .45 1 1 1zm3-5h-14c-.55 0-1 .45-1 1v10c0 .55.45 1 1 1h14c.55 0 1-.45 1-1v-10c0-.55-.45-1-1-1zm-1 9l-5-3-1 2-3-4-3 5v-7h12v7z" fill-rule=evenodd></path></svg></span></button><button type=button title=语音翻译 class="bp5-button bp5-minimal bp5-small"><span class=bp5-icon><svg version=1.1 id=Capa_1 width=14 height=14 xmlns=http://www.w3.org/2000/svg xmlns:xlink=http://www.w3.org/1999/xlink x=0px y=0px viewBox="0 0 490.9 490.9" xml:space=preserve><g><g><path d="M245.5,322.9c53,0,96.2-43.2,96.2-96.2V96.2c0-53-43.2-96.2-96.2-96.2s-96.2,43.2-96.2,96.2v130.5 C149.3,279.8,192.5,322.9,245.5,322.9z M173.8,96.2c0-39.5,32.2-71.7,71.7-71.7s71.7,32.2,71.7,71.7v130.5 c0,39.5-32.2,71.7-71.7,71.7s-71.7-32.2-71.7-71.7V96.2z"></path><path d="M94.4,214.5c-6.8,0-12.3,5.5-12.3,12.3c0,85.9,66.7,156.6,151.1,162.8v76.7h-63.9c-6.8,0-12.3,5.5-12.3,12.3 s5.5,12.3,12.3,12.3h152.3c6.8,0,12.3-5.5,12.3-12.3s-5.5-12.3-12.3-12.3h-63.9v-76.7c84.4-6.3,151.1-76.9,151.1-162.8 c0-6.8-5.5-12.3-12.3-12.3s-12.3,5.5-12.3,12.3c0,76.6-62.3,138.9-138.9,138.9s-138.9-62.3-138.9-138.9 C106.6,220,101.2,214.5,94.4,214.5z"></path></g></g></svg></span></button></div><div class=drag-block title=按住不放可以拖动></div><div class=right><button type=button disabled title=添加到收藏夹 class="bp5-button bp5-disabled bp5-minimal bp5-small" tabindex=-1><span aria-hidden=true class="bp5-icon bp5-icon-star-empty"><svg data-icon=star-empty height=14 role=img viewBox="0 0 16 16" width=14><path d="M16 6.11l-5.53-.84L8 0 5.53 5.27 0 6.11l4 4.1L3.06 16 8 13.27 12.94 16 12 10.21l4-4.1zM4.91 13.2l.59-3.62L3 7.02l3.45-.53L8 3.2l1.55 3.29 3.45.53-2.5 2.56.59 3.62L8 11.49 4.91 13.2z" fill-rule=evenodd></path></svg></span></button><button type=button class="bp5-button bp5-minimal bp5-small settings" title=快捷设置><span aria-hidden=true class="bp5-icon bp5-icon-cog"><svg data-icon=cog height=14 role=img viewBox="0 0 16 16" width=14><path d="M15.19 6.39h-1.85c-.11-.37-.27-.71-.45-1.04l1.36-1.36c.31-.31.31-.82 0-1.13l-1.13-1.13a.803.803 0 00-1.13 0l-1.36 1.36c-.33-.17-.67-.33-1.04-.44V.79c0-.44-.36-.8-.8-.8h-1.6c-.44 0-.8.36-.8.8v1.86c-.39.12-.75.28-1.1.47l-1.3-1.3c-.3-.3-.79-.3-1.09 0L1.82 2.91c-.3.3-.3.79 0 1.09l1.3 1.3c-.2.34-.36.7-.48 1.09H.79c-.44 0-.8.36-.8.8v1.6c0 .44.36.8.8.8h1.85c.11.37.27.71.45 1.04l-1.36 1.36c-.31.31-.31.82 0 1.13l1.13 1.13c.31.31.82.31 1.13 0l1.36-1.36c.33.18.67.33 1.04.44v1.86c0 .44.36.8.8.8h1.6c.44 0 .8-.36.8-.8v-1.86c.39-.12.75-.28 1.1-.47l1.3 1.3c.3.3.79.3 1.09 0l1.09-1.09c.3-.3.3-.79 0-1.09l-1.3-1.3c.19-.35.36-.71.48-1.1h1.85c.44 0 .8-.36.8-.8v-1.6a.816.816 0 00-.81-.79zm-7.2 4.6c-1.66 0-3-1.34-3-3s1.34-3 3-3 3 1.34 3 3-1.34 3-3 3z" fill-rule=evenodd></path></svg></span></button><button type=button title=关闭(Esc) class="bp5-button bp5-minimal bp5-small"><span aria-hidden=true class="bp5-icon bp5-icon-cross"><svg data-icon=cross height=18 role=img viewBox="0 0 16 16" width=18><path d="M9.41 8l3.29-3.29c.19-.18.3-.43.3-.71a1.003 1.003 0 00-1.71-.71L8 6.59l-3.29-3.3a1.003 1.003 0 00-1.42 1.42L6.59 8 3.3 11.29c-.19.18-.3.43-.3.71a1.003 1.003 0 001.71.71L8 9.41l3.29 3.29c.18.19.43.3.71.3a1.003 1.003 0 00.71-1.71L9.41 8z" fill-rule=evenodd></path></svg></span></button></div></div><div class=bp5-collapse><div class="bp5-collapse-body sf-hidden" aria-hidden=true></div></div></div><div class=body><p>请输入需要翻译的文本。</p></div></div><div class="arrow sf-hidden"></div></div><div id=web-trs-panel></div></div></div></template></div></div><script data-template-shadow-root>(()=>{document.currentScript.remove();processNode(document);function processNode(node){node.querySelectorAll("template[shadowrootmode]").forEach(element=>{let shadowRoot = element.parentElement.shadowRoot;if (!shadowRoot) {try {shadowRoot=element.parentElement.attachShadow({mode:element.getAttribute("shadowrootmode"),delegatesFocus:element.getAttribute("shadowrootdelegatesfocus")!=null,clonable:element.getAttribute("shadowrootclonable")!=null,serializable:element.getAttribute("shadowrootserializable")!=null});shadowRoot.innerHTML=element.innerHTML;element.remove()} catch (error) {} if (shadowRoot) {processNode(shadowRoot)}}})}})()</script>